{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotting import Plotting\n",
    "from loading_preparing_data import Panda\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/jolandamalamud/phd/projects/gng_panda_antler/gng_panda/data/'\n",
    "panda = Panda(filepath)\n",
    "disp = Plotting()\n",
    "# load raw task data\n",
    "D = panda.load_data()\n",
    "# prep action choices to plot\n",
    "data = panda.extract_data(D)\n",
    "# load modelling data\n",
    "modelling = panda.load_modelling_results('modelling_results/final_results/', ['ll2b2a2epxb', 'llb'])\n",
    "#transform_params = panda.transform_parameters(modelling['emmap'])\n",
    "random_baseline_model = panda.load_modelling_results('modelling_results/final_results/', ['llb'])\n",
    "# load rct data\n",
    "dfRCT = panda.load_rctdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flat_list = []\n",
    "old_flat_list = []\n",
    "questionnaires = {'gad': 7,'phq': 9, 'beck': 21}\n",
    "for q in questionnaires:\n",
    "    y = []\n",
    "    for i in range(1,questionnaires[q]+1):\n",
    "        x = [x for x in dfRCT.columns if q + str(i) in x]\n",
    "        y.append(x)\n",
    "    flat_list = [x for xs in y for x in xs]\n",
    "    flat_list = [x for x in flat_list if 'bin' not in x]\n",
    "    old_flat_list = old_flat_list + flat_list\n",
    "    for i in flat_list:\n",
    "        if '12wk' in i: new_flat_list.append(i[:len(q)+1] + '_4')\n",
    "        elif '6wk' in i: new_flat_list.append(i[:len(q)+1] + '_3')\n",
    "        elif '2wk' in i: new_flat_list.append(i[:len(q)+1] + '_2')\n",
    "        else: new_flat_list.append(i[:4] + '_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all variables of interest\n",
    "parameter_labels = ['rew_se', 'loss_se', 'rew_LR', 'loss_LR', 'app_Pav', 'av_Pav', 'noise', 'bias', 'rbias']\n",
    "parameter_labels_transformed = []\n",
    "# for i in parameter_labels[:-2]: parameter_labels_transformed.append(i + '_trans')\n",
    "gng_columns = parameter_labels + \\\n",
    "                ['exclusion', 'goprotot', 'gopro_g2w', 'gopro_g2a', 'gopro_ng2w', \\\n",
    "                 'gopro_ng2a', 'acctot', 'acc_g2w', 'acc_g2a', 'acc_ng2w', 'acc_ng2a','iL']\n",
    "\n",
    "gng_data = np.vstack((modelling['emmap'], random_baseline_model['emmap'], modelling['excluding'], \\\n",
    "                       np.nanmean(data['a_go'],axis=(0,1)), np.nanmean(data['a_go'],axis=0), \\\n",
    "                       np.nanmean(data['a_correct'],axis=(0,1)), np.nanmean(data['a_correct'],axis=0), \\\n",
    "                       modelling['iL']))\n",
    "\n",
    "rct_columns = ['rctid', 'group', 'gad1', 'gad2log', 'gad3log', 'gad4log', 'phq1', 'phq2log', \\\n",
    "               'phq3log', 'phq4log', 'beck1', 'beck2log', 'beck3log', 'beck4log',\\\n",
    "               'site', 'cis', 'dep', 'age', 'education', 'AD_past', \\\n",
    "               'sex', 'ethnic', 'fin', 'empstat', 'marstat', 'cisscore'] + new_flat_list\n",
    "\n",
    "rct_data = dfRCT[['identifier_n', 'group', 'gadtot', 'log_gadtot_2wk', 'log_gadtot_6wk', 'log_gadtot_12wk', \\\n",
    "                  'phqtot', 'log_phqtot_2wk', 'log_phqtot_6wk', 'log_phqtot_12wk', 'becktot', \\\n",
    "                  'log_becktot_2wk','log_becktot_6wk','log_becktot_12wk','_site_n', \\\n",
    "                  'cistotal_cat', 'depr_dur_2years', 'age', 'edu3', 'antidepressantsinpast', \\\n",
    "                  'sex', 'ethnic', 'fin3', 'empstat2', 'marstat3', 'cisdepscore'] + old_flat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of all variables of interest\n",
    "df_panda = panda.create_panda_df(data, gng_columns, gng_data, rct_columns, rct_data)\n",
    "if 'gad1log' in df_panda.columns: rct_columns.append('gad1log')\n",
    "if 'phq1log' in df_panda.columns: rct_columns.append('phq1log')\n",
    "if 'beck1log' in df_panda.columns: rct_columns.append('beck1log')\n",
    "number_of_sessions = max(data['sess']).astype(int)\n",
    "number_of_subjects = len(df_panda)\n",
    "weeks = [0, 2, 6, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beck1log</th>\n",
       "      <th>phq1log</th>\n",
       "      <th>gad1log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beck1log</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791180</td>\n",
       "      <td>0.664799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phq1log</th>\n",
       "      <td>0.791180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gad1log</th>\n",
       "      <td>0.664799</td>\n",
       "      <td>0.708296</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          beck1log   phq1log   gad1log\n",
       "beck1log  1.000000  0.791180  0.664799\n",
       "phq1log   0.791180  1.000000  0.708296\n",
       "gad1log   0.664799  0.708296  1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_panda[['beck1log','phq1log', 'gad1log']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "Exclusion\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of excluded subjects at week 0:\t311.0 (0.5%)\n",
      "# of excluded subjects at week 2:\t229.0 (0.43%)\n",
      "# of excluded subjects at week 6:\t203.0 (0.42%)\n",
      "total # of excluded subjects:\t\t743.0 (46%)\n"
     ]
    }
   ],
   "source": [
    "# number of excluded subjects due to not performing the task properly\n",
    "for t in range(1,number_of_sessions+1):\n",
    "    no_data = df_panda['exclusion' + str(t)].isna()\n",
    "    ex1 = df_panda['exclusion' + str(t)] == 1\n",
    "    ex2 = df_panda['acctot' + str(t)] < 0.5\n",
    "#     df_panda.loc[~no_data, 'exclusiontot' + str(t)] = (ex1[~no_data] | ex2[~no_data]).astype(int)\n",
    "    df_panda.loc[~no_data, 'exclusiontot' + str(t)] = (ex1[~no_data]).astype(int)\n",
    "\n",
    "gng_columns.append('exclusiontot')\n",
    "\n",
    "n_subject = []\n",
    "exclusion = []\n",
    "for t in range(number_of_sessions):\n",
    "    exclusion.append(np.nansum(df_panda['exclusiontot' + str(t+1)]))\n",
    "    n_subject.append(sum(~df_panda['exclusiontot' + str(t+1)].isna()))\n",
    "    print('# of excluded subjects at week ' + str(weeks[t]) + ':\\t' + str(exclusion[t]) + \\\n",
    "      ' (' + str(round(exclusion[t]/n_subject[t],2)) +'%)')\n",
    "print('total # of excluded subjects:\\t\\t' + str(sum(exclusion)) + \\\n",
    "      ' (' + str(round(sum(exclusion)/(sum(n_subject))*100)) +'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>134</td>\n",
       "      <td>165</td>\n",
       "      <td>5.932</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>128</td>\n",
       "      <td>101</td>\n",
       "      <td>5.932</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variable  sertraline  placebo     X2      p\n",
       "0.0  exclusiontot         134      165  5.932  0.015\n",
       "1.0  exclusiontot         128      101  5.932  0.015"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 528, sertraline (N=324): 48.85%, placebo (N=329): 37.97%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>136</td>\n",
       "      <td>143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>99</td>\n",
       "      <td>104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variable  sertraline  placebo   X2    p\n",
       "0.0  exclusiontot         136      143  0.0  1.0\n",
       "1.0  exclusiontot          99      104  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 482, sertraline (N=324): 42.13%, placebo (N=329): 42.11%\n"
     ]
    }
   ],
   "source": [
    "# chi2 pearson test to compare included vs excluded in sertraline vs placebo\n",
    "for t in range(2,number_of_sessions+1):\n",
    "    tmp = panda.chi2_test(df_panda['exclusiontot' + str(t)], df_panda['group'], ['sertraline', 'placebo'])\n",
    "    tmp.insert(0, 'variable', 'exclusiontot')\n",
    "    display(tmp)\n",
    "    print('N = ' + str(sum(~df_panda['exclusiontot' + str(t)].isna() & ~df_panda['group'].isna())) + \\\n",
    "          ', sertraline (N=' + str(sum(df_panda['group']==1)) + \\\n",
    "          '): ' + str(np.round(df_panda['exclusiontot' + str(t)][df_panda['group']==1].mean()*100, 2)) + '%'\\\n",
    "          ', placebo (N=' + str(sum(df_panda['group']==0)) + \\\n",
    "          '): ' + str(np.round(df_panda['exclusiontot' + str(t)][df_panda['group']==0].mean()*100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>both_good</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>97</td>\n",
       "      <td>123</td>\n",
       "      <td>7.959</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both_bad</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>96</td>\n",
       "      <td>80</td>\n",
       "      <td>7.959</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>36</td>\n",
       "      <td>42</td>\n",
       "      <td>7.959</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>7.959</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            variable  sertraline  placebo     X2      p\n",
       "both_good  exclusion          97      123  7.959  0.047\n",
       "both_bad   exclusion          96       80  7.959  0.047\n",
       "better     exclusion          36       42  7.959  0.047\n",
       "worse      exclusion          30       18  7.959  0.047"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chi2 pearson test to compare categories of how exclusion changes in sertraline vs placebo\n",
    "tmp = {'both_bad': [1,1], 'both_good': [0,0], 'worse': [0,1], 'better': [1,0]}\n",
    "for i in tmp.keys():\n",
    "    idx = (df_panda['exclusiontot1']==tmp[i][0]) & (df_panda['exclusiontot2']==tmp[i][1])\n",
    "    df_panda.loc[idx, 'exclusion_overall'] = np.tile(i, sum(idx))\n",
    "\n",
    "tmp = panda.chi2_test(df_panda['exclusion_overall'], df_panda['group'], ['sertraline', 'placebo'])\n",
    "tmp.insert(0, 'variable', 'exclusion')\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>excluded</th>\n",
       "      <th>included</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>age</td>\n",
       "      <td>43.775</td>\n",
       "      <td>34.767</td>\n",
       "      <td>7.961</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>cisscore</td>\n",
       "      <td>10.223</td>\n",
       "      <td>10.751</td>\n",
       "      <td>-1.347</td>\n",
       "      <td>0.178</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>site</td>\n",
       "      <td>121.000</td>\n",
       "      <td>136.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402</td>\n",
       "      <td>2.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>site</td>\n",
       "      <td>71.000</td>\n",
       "      <td>56.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402</td>\n",
       "      <td>2.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>site</td>\n",
       "      <td>60.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402</td>\n",
       "      <td>2.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>site</td>\n",
       "      <td>59.000</td>\n",
       "      <td>54.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402</td>\n",
       "      <td>2.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>161.000</td>\n",
       "      <td>173.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314</td>\n",
       "      <td>2.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>91.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314</td>\n",
       "      <td>2.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>58.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314</td>\n",
       "      <td>2.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>215.000</td>\n",
       "      <td>209.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>96.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>education</td>\n",
       "      <td>195.000</td>\n",
       "      <td>238.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>28.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>education</td>\n",
       "      <td>88.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>28.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>education</td>\n",
       "      <td>27.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>28.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>205.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>105.000</td>\n",
       "      <td>145.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>191.000</td>\n",
       "      <td>175.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.259</td>\n",
       "      <td>1.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>120.000</td>\n",
       "      <td>134.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.259</td>\n",
       "      <td>1.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>269.000</td>\n",
       "      <td>288.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>15.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>16.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>15.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>14.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>15.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>5.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>15.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>15.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>15.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>178.000</td>\n",
       "      <td>175.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>97.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>35.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>194.000</td>\n",
       "      <td>220.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028</td>\n",
       "      <td>4.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>116.000</td>\n",
       "      <td>89.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028</td>\n",
       "      <td>4.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>138.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>117.000</td>\n",
       "      <td>165.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>55.000</td>\n",
       "      <td>37.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>excluded</th>\n",
       "      <th>included</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>age</td>\n",
       "      <td>46.572</td>\n",
       "      <td>34.753</td>\n",
       "      <td>9.690</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>cisscore</td>\n",
       "      <td>9.987</td>\n",
       "      <td>10.513</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.222</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>site</td>\n",
       "      <td>95.000</td>\n",
       "      <td>129.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285</td>\n",
       "      <td>3.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>site</td>\n",
       "      <td>46.000</td>\n",
       "      <td>54.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285</td>\n",
       "      <td>3.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>site</td>\n",
       "      <td>44.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285</td>\n",
       "      <td>3.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>site</td>\n",
       "      <td>44.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285</td>\n",
       "      <td>3.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>109.000</td>\n",
       "      <td>166.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181</td>\n",
       "      <td>3.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>67.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181</td>\n",
       "      <td>3.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>53.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181</td>\n",
       "      <td>3.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>156.000</td>\n",
       "      <td>205.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>73.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>education</td>\n",
       "      <td>132.000</td>\n",
       "      <td>238.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>34.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>education</td>\n",
       "      <td>79.000</td>\n",
       "      <td>56.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>34.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>education</td>\n",
       "      <td>18.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>34.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>159.000</td>\n",
       "      <td>166.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>9.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>70.000</td>\n",
       "      <td>132.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>9.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>140.000</td>\n",
       "      <td>167.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258</td>\n",
       "      <td>1.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>89.000</td>\n",
       "      <td>132.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258</td>\n",
       "      <td>1.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>208.000</td>\n",
       "      <td>276.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>8.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>9.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>8.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>8.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>3.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>8.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>8.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>8.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>134.000</td>\n",
       "      <td>181.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>67.000</td>\n",
       "      <td>83.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>28.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>151.000</td>\n",
       "      <td>206.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>78.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>108.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>29.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>71.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>29.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>50.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>29.090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>excluded</th>\n",
       "      <th>included</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>age</td>\n",
       "      <td>46.369</td>\n",
       "      <td>35.022</td>\n",
       "      <td>8.823</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>cisscore</td>\n",
       "      <td>9.936</td>\n",
       "      <td>10.609</td>\n",
       "      <td>-1.521</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>site</td>\n",
       "      <td>87.000</td>\n",
       "      <td>124.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518</td>\n",
       "      <td>2.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>site</td>\n",
       "      <td>44.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518</td>\n",
       "      <td>2.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>site</td>\n",
       "      <td>38.000</td>\n",
       "      <td>52.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518</td>\n",
       "      <td>2.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>site</td>\n",
       "      <td>34.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518</td>\n",
       "      <td>2.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>87.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>11.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>67.000</td>\n",
       "      <td>64.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>11.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>48.000</td>\n",
       "      <td>51.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>11.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>145.000</td>\n",
       "      <td>190.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>58.000</td>\n",
       "      <td>89.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>education</td>\n",
       "      <td>111.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>education</td>\n",
       "      <td>73.000</td>\n",
       "      <td>51.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>education</td>\n",
       "      <td>18.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>142.000</td>\n",
       "      <td>148.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>60.000</td>\n",
       "      <td>131.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>119.000</td>\n",
       "      <td>160.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>84.000</td>\n",
       "      <td>119.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>186.000</td>\n",
       "      <td>260.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237</td>\n",
       "      <td>6.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237</td>\n",
       "      <td>6.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>3.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237</td>\n",
       "      <td>6.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237</td>\n",
       "      <td>6.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237</td>\n",
       "      <td>6.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237</td>\n",
       "      <td>6.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>123.000</td>\n",
       "      <td>161.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>57.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>22.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>122.000</td>\n",
       "      <td>202.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "      <td>7.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>80.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008</td>\n",
       "      <td>7.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>89.000</td>\n",
       "      <td>109.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>67.000</td>\n",
       "      <td>139.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>46.000</td>\n",
       "      <td>31.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# is exclusion related to baseline variables?\n",
    "baseline_continuous =  ['age', 'cisscore']\n",
    "baseline_categorical = ['site', 'cis', 'dep', 'education', \\\n",
    "                        'AD_past','sex', 'ethnic', 'fin', 'empstat', 'marstat']\n",
    "tab_list = []\n",
    "for t in range(1,number_of_sessions+1):\n",
    "    tab = pd.DataFrame()\n",
    "    for i in baseline_continuous:\n",
    "        tmp = panda.group_ttest(df_panda[i], df_panda['exclusiontot' + str(t)], ['excluded', 'included'])\n",
    "        tmp.insert(0, 'variable', i)\n",
    "        tab = pd.concat((tab, tmp))\n",
    "    for i in baseline_categorical:\n",
    "        tmp = panda.chi2_test(df_panda[i], df_panda['exclusiontot'  + str(t)], ['excluded', 'included'])\n",
    "        tmp.insert(0, 'variable', i)\n",
    "        tab = pd.concat((tab, tmp))\n",
    "    tab_list.append(tab)\n",
    "disp.display_side_by_side(tab_list[0],tab_list[1],tab_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.616344\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot1</td>  <th>  No. Observations:  </th>  <td>   619</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   606</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    12</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 29 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1108</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>18:43:52</td>     <th>  Log-Likelihood:    </th> <td> -381.52</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -429.06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>5.090e-15</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -4.3925</td> <td>    0.747</td> <td>   -5.880</td> <td> 0.000</td> <td>   -5.857</td> <td>   -2.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0435</td> <td>    0.007</td> <td>    6.395</td> <td> 0.000</td> <td>    0.030</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore</th>  <td>   -0.0516</td> <td>    0.031</td> <td>   -1.672</td> <td> 0.094</td> <td>   -0.112</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site</th>      <td>    0.0676</td> <td>    0.076</td> <td>    0.893</td> <td> 0.372</td> <td>   -0.081</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis</th>       <td>    0.4157</td> <td>    0.192</td> <td>    2.166</td> <td> 0.030</td> <td>    0.040</td> <td>    0.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep</th>       <td>   -0.1242</td> <td>    0.194</td> <td>   -0.641</td> <td> 0.521</td> <td>   -0.504</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th> <td>    0.6626</td> <td>    0.172</td> <td>    3.849</td> <td> 0.000</td> <td>    0.325</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past</th>   <td>    0.1912</td> <td>    0.188</td> <td>    1.018</td> <td> 0.309</td> <td>   -0.177</td> <td>    0.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>       <td>    0.2805</td> <td>    0.183</td> <td>    1.532</td> <td> 0.125</td> <td>   -0.078</td> <td>    0.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic</th>    <td>    0.2628</td> <td>    0.123</td> <td>    2.138</td> <td> 0.033</td> <td>    0.022</td> <td>    0.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin</th>       <td>    0.0540</td> <td>    0.129</td> <td>    0.419</td> <td> 0.675</td> <td>   -0.199</td> <td>    0.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat</th>   <td>    0.2806</td> <td>    0.197</td> <td>    1.427</td> <td> 0.154</td> <td>   -0.105</td> <td>    0.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat</th>   <td>   -0.0763</td> <td>    0.128</td> <td>   -0.596</td> <td> 0.551</td> <td>   -0.327</td> <td>    0.175</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot1   No. Observations:                  619\n",
       "Model:                          Logit   Df Residuals:                      606\n",
       "Method:                           MLE   Df Model:                           12\n",
       "Date:                Fri, 29 Jul 2022   Pseudo R-squ.:                  0.1108\n",
       "Time:                        18:43:52   Log-Likelihood:                -381.52\n",
       "converged:                       True   LL-Null:                       -429.06\n",
       "Covariance Type:            nonrobust   LLR p-value:                 5.090e-15\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -4.3925      0.747     -5.880      0.000      -5.857      -2.928\n",
       "age            0.0435      0.007      6.395      0.000       0.030       0.057\n",
       "cisscore      -0.0516      0.031     -1.672      0.094      -0.112       0.009\n",
       "site           0.0676      0.076      0.893      0.372      -0.081       0.216\n",
       "cis            0.4157      0.192      2.166      0.030       0.040       0.792\n",
       "dep           -0.1242      0.194     -0.641      0.521      -0.504       0.255\n",
       "education      0.6626      0.172      3.849      0.000       0.325       1.000\n",
       "AD_past        0.1912      0.188      1.018      0.309      -0.177       0.559\n",
       "sex            0.2805      0.183      1.532      0.125      -0.078       0.639\n",
       "ethnic         0.2628      0.123      2.138      0.033       0.022       0.504\n",
       "fin            0.0540      0.129      0.419      0.675      -0.199       0.307\n",
       "empstat        0.2806      0.197      1.427      0.154      -0.105       0.666\n",
       "marstat       -0.0763      0.128     -0.596      0.551      -0.327       0.175\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575059\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot2</td>  <th>  No. Observations:  </th>  <td>   527</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   514</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    12</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 29 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1599</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>18:43:52</td>     <th>  Log-Likelihood:    </th> <td> -303.06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -360.76</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>5.077e-19</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -4.7804</td> <td>    0.855</td> <td>   -5.590</td> <td> 0.000</td> <td>   -6.456</td> <td>   -3.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0608</td> <td>    0.008</td> <td>    7.658</td> <td> 0.000</td> <td>    0.045</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore</th>  <td>    0.0025</td> <td>    0.035</td> <td>    0.072</td> <td> 0.942</td> <td>   -0.067</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site</th>      <td>    0.0439</td> <td>    0.089</td> <td>    0.496</td> <td> 0.620</td> <td>   -0.130</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis</th>       <td>    0.0097</td> <td>    0.215</td> <td>    0.045</td> <td> 0.964</td> <td>   -0.411</td> <td>    0.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep</th>       <td>   -0.0821</td> <td>    0.219</td> <td>   -0.375</td> <td> 0.708</td> <td>   -0.511</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th> <td>    0.8935</td> <td>    0.196</td> <td>    4.550</td> <td> 0.000</td> <td>    0.509</td> <td>    1.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past</th>   <td>   -0.0619</td> <td>    0.221</td> <td>   -0.280</td> <td> 0.779</td> <td>   -0.495</td> <td>    0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>       <td>    0.4144</td> <td>    0.208</td> <td>    1.990</td> <td> 0.047</td> <td>    0.006</td> <td>    0.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic</th>    <td>    0.0550</td> <td>    0.142</td> <td>    0.388</td> <td> 0.698</td> <td>   -0.223</td> <td>    0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin</th>       <td>    0.2578</td> <td>    0.147</td> <td>    1.752</td> <td> 0.080</td> <td>   -0.031</td> <td>    0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat</th>   <td>   -0.1767</td> <td>    0.226</td> <td>   -0.783</td> <td> 0.434</td> <td>   -0.619</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat</th>   <td>    0.0251</td> <td>    0.139</td> <td>    0.180</td> <td> 0.857</td> <td>   -0.248</td> <td>    0.299</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot2   No. Observations:                  527\n",
       "Model:                          Logit   Df Residuals:                      514\n",
       "Method:                           MLE   Df Model:                           12\n",
       "Date:                Fri, 29 Jul 2022   Pseudo R-squ.:                  0.1599\n",
       "Time:                        18:43:52   Log-Likelihood:                -303.06\n",
       "converged:                       True   LL-Null:                       -360.76\n",
       "Covariance Type:            nonrobust   LLR p-value:                 5.077e-19\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -4.7804      0.855     -5.590      0.000      -6.456      -3.104\n",
       "age            0.0608      0.008      7.658      0.000       0.045       0.076\n",
       "cisscore       0.0025      0.035      0.072      0.942      -0.067       0.072\n",
       "site           0.0439      0.089      0.496      0.620      -0.130       0.218\n",
       "cis            0.0097      0.215      0.045      0.964      -0.411       0.430\n",
       "dep           -0.0821      0.219     -0.375      0.708      -0.511       0.347\n",
       "education      0.8935      0.196      4.550      0.000       0.509       1.278\n",
       "AD_past       -0.0619      0.221     -0.280      0.779      -0.495       0.371\n",
       "sex            0.4144      0.208      1.990      0.047       0.006       0.823\n",
       "ethnic         0.0550      0.142      0.388      0.698      -0.223       0.333\n",
       "fin            0.2578      0.147      1.752      0.080      -0.031       0.546\n",
       "empstat       -0.1767      0.226     -0.783      0.434      -0.619       0.266\n",
       "marstat        0.0251      0.139      0.180      0.857      -0.248       0.299\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.560295\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot3</td>  <th>  No. Observations:  </th>  <td>   481</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   468</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    12</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 29 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1764</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>18:43:52</td>     <th>  Log-Likelihood:    </th> <td> -269.50</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -327.21</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>5.031e-19</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -5.0421</td> <td>    0.927</td> <td>   -5.439</td> <td> 0.000</td> <td>   -6.859</td> <td>   -3.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0522</td> <td>    0.008</td> <td>    6.369</td> <td> 0.000</td> <td>    0.036</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore</th>  <td>    0.0477</td> <td>    0.038</td> <td>    1.251</td> <td> 0.211</td> <td>   -0.027</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site</th>      <td>    0.0630</td> <td>    0.098</td> <td>    0.643</td> <td> 0.520</td> <td>   -0.129</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis</th>       <td>   -0.3555</td> <td>    0.231</td> <td>   -1.539</td> <td> 0.124</td> <td>   -0.808</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep</th>       <td>   -0.1296</td> <td>    0.235</td> <td>   -0.550</td> <td> 0.582</td> <td>   -0.591</td> <td>    0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th> <td>    1.0413</td> <td>    0.209</td> <td>    4.988</td> <td> 0.000</td> <td>    0.632</td> <td>    1.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past</th>   <td>    0.2394</td> <td>    0.232</td> <td>    1.032</td> <td> 0.302</td> <td>   -0.215</td> <td>    0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>       <td>    0.2227</td> <td>    0.221</td> <td>    1.009</td> <td> 0.313</td> <td>   -0.210</td> <td>    0.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic</th>    <td>    0.0390</td> <td>    0.140</td> <td>    0.279</td> <td> 0.780</td> <td>   -0.235</td> <td>    0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin</th>       <td>    0.0121</td> <td>    0.159</td> <td>    0.076</td> <td> 0.939</td> <td>   -0.299</td> <td>    0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat</th>   <td>    0.3946</td> <td>    0.234</td> <td>    1.686</td> <td> 0.092</td> <td>   -0.064</td> <td>    0.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat</th>   <td>    0.1462</td> <td>    0.148</td> <td>    0.991</td> <td> 0.322</td> <td>   -0.143</td> <td>    0.435</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot3   No. Observations:                  481\n",
       "Model:                          Logit   Df Residuals:                      468\n",
       "Method:                           MLE   Df Model:                           12\n",
       "Date:                Fri, 29 Jul 2022   Pseudo R-squ.:                  0.1764\n",
       "Time:                        18:43:52   Log-Likelihood:                -269.50\n",
       "converged:                       True   LL-Null:                       -327.21\n",
       "Covariance Type:            nonrobust   LLR p-value:                 5.031e-19\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -5.0421      0.927     -5.439      0.000      -6.859      -3.225\n",
       "age            0.0522      0.008      6.369      0.000       0.036       0.068\n",
       "cisscore       0.0477      0.038      1.251      0.211      -0.027       0.122\n",
       "site           0.0630      0.098      0.643      0.520      -0.129       0.255\n",
       "cis           -0.3555      0.231     -1.539      0.124      -0.808       0.097\n",
       "dep           -0.1296      0.235     -0.550      0.582      -0.591       0.332\n",
       "education      1.0413      0.209      4.988      0.000       0.632       1.451\n",
       "AD_past        0.2394      0.232      1.032      0.302      -0.215       0.694\n",
       "sex            0.2227      0.221      1.009      0.313      -0.210       0.655\n",
       "ethnic         0.0390      0.140      0.279      0.780      -0.235       0.313\n",
       "fin            0.0121      0.159      0.076      0.939      -0.299       0.324\n",
       "empstat        0.3946      0.234      1.686      0.092      -0.064       0.853\n",
       "marstat        0.1462      0.148      0.991      0.322      -0.143       0.435\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    model = smf.logit('exclusiontot' + str(i) +' ~  age + cisscore + site + cis + ' + \\\n",
    "                  'dep + education + AD_past + sex + ethnic + fin + empstat + marstat', data=df_panda).fit()\n",
    "    display(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "exclusiontot1: \tbeta: -0.01,\tCI: [-0.06,0.03],\tpvalue: 0.5114\n",
      "exclusiontot2: \tbeta: -0.01,\tCI: [-0.06,0.03],\tpvalue: 0.5872\n",
      "exclusiontot3: \tbeta: -0.03,\tCI: [-0.1,0.03],\tpvalue: 0.2857\n",
      "phq:\n",
      "exclusiontot1: \tbeta: -0.02,\tCI: [-0.06,0.02],\tpvalue: 0.3098\n",
      "exclusiontot2: \tbeta: -0.03,\tCI: [-0.08,0.01],\tpvalue: 0.1235\n",
      "exclusiontot3: \tbeta: -0.02,\tCI: [-0.08,0.04],\tpvalue: 0.4501\n",
      "beck:\n",
      "exclusiontot1: \tbeta: -0.02,\tCI: [-0.05,0.02],\tpvalue: 0.3391\n",
      "exclusiontot2: \tbeta: -0.03,\tCI: [-0.08,0.02],\tpvalue: 0.2504\n",
      "exclusiontot3: \tbeta: -0.02,\tCI: [-0.08,0.05],\tpvalue: 0.6499\n"
     ]
    }
   ],
   "source": [
    "# is exclusion related to psychiatric scores at specific time point?\n",
    "psychiatric_questionnaires = ['gad', 'phq', 'beck']\n",
    "for i in psychiatric_questionnaires:\n",
    "    print(i + ':')\n",
    "    panda.glm((i + '1log ~ exclusiontot1'), df_panda, ['exclusiontot1'])\n",
    "    for t in range(2,number_of_sessions+1):\n",
    "          panda.glm((i + str(t) + 'log ~ gad1log + exclusiontot' + str(t)), df_panda, ['exclusiontot' + str(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "--\n",
    "- more patients not doing the task properly in the sertraline group at week 2 (follow-up 1)\n",
    "- patients not doing the task properly were older, less educated, not single, and were taking antidepressants in the past over all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>go to win</th>\n",
       "      <th>go to avoid</th>\n",
       "      <th>nogo to win</th>\n",
       "      <th>nogo to avoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.77±0.26</td>\n",
       "      <td>0.52±0.24</td>\n",
       "      <td>0.43±0.32</td>\n",
       "      <td>0.73±0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T=2</th>\n",
       "      <td>0.83±0.23</td>\n",
       "      <td>0.59±0.25</td>\n",
       "      <td>0.43±0.34</td>\n",
       "      <td>0.74±0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T=6</th>\n",
       "      <td>0.82±0.26</td>\n",
       "      <td>0.61±0.27</td>\n",
       "      <td>0.47±0.37</td>\n",
       "      <td>0.75±0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          go to win go to avoid nogo to win nogo to avoid\n",
       "baseline  0.77±0.26   0.52±0.24   0.43±0.32     0.73±0.17\n",
       "T=2       0.83±0.23   0.59±0.25   0.43±0.34     0.74±0.16\n",
       "T=6       0.82±0.26   0.61±0.27   0.47±0.37     0.75±0.17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of included task runs\n",
    "tablist = []\n",
    "for i in range(1,4):\n",
    "    tab = []\n",
    "    x = [x + str(i) for x in gng_columns if 'acc_' in x]\n",
    "    for j in x:\n",
    "        tab.append(str(np.round(df_panda[j][df_panda['exclusiontot'+str(i)]==0].mean(),2)) \\\n",
    "                   + '±' +  str(np.round(df_panda[j][df_panda['exclusiontot'+str(i)]==0].std(),2)))\n",
    "    tablist.append(tab)\n",
    "pd.DataFrame(tablist, columns=['go to win', 'go to avoid', 'nogo to win', 'nogo to avoid'], index=['baseline','T=2','T=6'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "acc_g2w1: \tbeta: -0.01,\tCI: [-0.09,0.07],\tpvalue: 0.781\n",
      "acc_g2a1: \tbeta: -0.01,\tCI: [-0.1,0.08],\tpvalue: 0.8564\n",
      "acc_ng2w1: \tbeta: 0.0,\tCI: [-0.06,0.07],\tpvalue: 0.9341\n",
      "acc_ng2a1: \tbeta: -0.07,\tCI: [-0.19,0.05],\tpvalue: 0.2327\n",
      "phq:\n",
      "acc_g2w1: \tbeta: -0.0,\tCI: [-0.08,0.07],\tpvalue: 0.9692\n",
      "acc_g2a1: \tbeta: 0.01,\tCI: [-0.07,0.09],\tpvalue: 0.8342\n",
      "acc_ng2w1: \tbeta: 0.03,\tCI: [-0.03,0.08],\tpvalue: 0.3938\n",
      "acc_ng2a1: \tbeta: -0.01,\tCI: [-0.12,0.1],\tpvalue: 0.8286\n",
      "beck:\n",
      "acc_g2w1: \tbeta: -0.04,\tCI: [-0.11,0.03],\tpvalue: 0.2266\n",
      "acc_g2a1: \tbeta: -0.02,\tCI: [-0.1,0.05],\tpvalue: 0.5393\n",
      "acc_ng2w1: \tbeta: -0.0,\tCI: [-0.06,0.05],\tpvalue: 0.9405\n",
      "acc_ng2a1: \tbeta: 0.02,\tCI: [-0.08,0.13],\tpvalue: 0.6281\n"
     ]
    }
   ],
   "source": [
    "# performance related to baseline psychiatric score\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    x = [x for x in gng_columns if 'acc_' in x]\n",
    "    for i in x:\n",
    "        panda.glm(j + '1log ~ '+i+'1 + sex + age + cis + dep + site + education + marstat + AD_past', df_panda[df_panda['exclusiontot1']==0], [i+'1'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "rew_se1: \tbeta: 0.02,\tCI: [-0.0,0.04],\tpvalue: 0.121\n",
      "loss_se1: \tbeta: -0.01,\tCI: [-0.03,0.02],\tpvalue: 0.5514\n",
      "rew_LR1: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.7008\n",
      "loss_LR1: \tbeta: 0.01,\tCI: [-0.0,0.02],\tpvalue: 0.1305\n",
      "app_Pav1: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.8613\n",
      "av_Pav1: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.7883\n",
      "noise1: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.7687\n",
      "bias1: \tbeta: 0.0,\tCI: [-0.02,0.03],\tpvalue: 0.7168\n",
      "phq:\n",
      "rew_se1: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.5059\n",
      "loss_se1: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.672\n",
      "rew_LR1: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.3466\n",
      "loss_LR1: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.2803\n",
      "app_Pav1: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.5315\n",
      "av_Pav1: \tbeta: -0.01,\tCI: [-0.03,0.02],\tpvalue: 0.607\n",
      "noise1: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.8985\n",
      "bias1: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.6824\n",
      "beck:\n",
      "rew_se1: \tbeta: 0.02,\tCI: [0.0,0.04],\tpvalue: 0.0134\n",
      "loss_se1: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9047\n",
      "rew_LR1: \tbeta: -0.01,\tCI: [-0.03,0.0],\tpvalue: 0.1065\n",
      "loss_LR1: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.3082\n",
      "app_Pav1: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.7608\n",
      "av_Pav1: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.268\n",
      "noise1: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.6517\n",
      "bias1: \tbeta: -0.02,\tCI: [-0.04,-0.0],\tpvalue: 0.0468\n"
     ]
    }
   ],
   "source": [
    "# baseline psychiatric scores related to baseline cognitive parameters:(only included task runs)\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.glm(j + '1log ~ '+i+'1 + sex + age + cis + dep + site + education + marstat + AD_past + fin', df_panda[df_panda['exclusiontot1']==0], [i+'1'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go bias is related to beck score not corrected for multiple comparison! The higher beck the higher go bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "MIXED EFFECTS MODELLING\n",
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for mixed-effects modelling\n",
    "mle_df = pd.DataFrame(columns = gng_columns + rct_columns + ['gad1', 'phq1', 'beck1'])\n",
    "for c in gng_columns:\n",
    "    mle_df[c] = pd.concat([df_panda[c + '1'], df_panda[c + '2'], df_panda[c + '3']], axis=0, ignore_index=True)\n",
    "    \n",
    "for c in rct_columns:\n",
    "    mle_df[c] = pd.concat([df_panda[c], df_panda[c], df_panda[c]], axis=0, ignore_index=True)\n",
    "\n",
    "mle_df['subject'] = np.hstack((np.arange(len(df_panda)),np.arange(len(df_panda)),np.arange(len(df_panda))))\n",
    "mle_df['time'] = np.hstack((np.tile(1, len(df_panda)),np.tile(2, len(df_panda)),np.tile(3, len(df_panda))))\n",
    "mle_df['weeks'] = np.hstack((np.tile(weeks[0], len(df_panda)),np.tile(weeks[1], len(df_panda)),np.tile(weeks[2], len(df_panda))))\n",
    "mle_df['group'] = np.hstack((np.tile(0, len(df_panda)),df_panda['group'],df_panda['group']))\n",
    "for i in psychiatric_questionnaires:\n",
    "    mle_df[i + 'log'] = np.hstack((df_panda[i + '1log'],df_panda[i + '2log'],df_panda[i + '3log']))\n",
    "mle_df['anhedonia'] = np.hstack((df_panda['phq1_1'],df_panda['phq1_2'],df_panda['phq1_3']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zcore variables\n",
    "for col in parameter_labels:\n",
    "    mle_df[col + '_zscore'] = (mle_df[col] - mle_df[col].mean()) / mle_df[col].std(ddof=0)\n",
    "for i in psychiatric_questionnaires:\n",
    "    mle_df[i + 'log_zscore'] = (mle_df[i + 'log'] - mle_df[i + 'log'].mean()) / mle_df[i + 'log'].std(ddof=0)\n",
    "mle_df['anhedonia_zscore'] = (mle_df['anhedonia'] - mle_df['anhedonia'].mean()) / mle_df['anhedonia'].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.595399\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot</td>   <th>  No. Observations:  </th>  <td>  1623</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1608</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    14</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 15 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1358</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>09:38:04</td>     <th>  Log-Likelihood:    </th> <td> -966.33</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1118.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.105e-56</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -5.0658</td> <td>    0.521</td> <td>   -9.717</td> <td> 0.000</td> <td>   -6.088</td> <td>   -4.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log</th>   <td>    0.4730</td> <td>    0.329</td> <td>    1.437</td> <td> 0.151</td> <td>   -0.172</td> <td>    1.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>phq1log</th>   <td>    0.2663</td> <td>    0.399</td> <td>    0.668</td> <td> 0.504</td> <td>   -0.515</td> <td>    1.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0522</td> <td>    0.004</td> <td>   11.856</td> <td> 0.000</td> <td>    0.044</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore</th>  <td>   -0.0175</td> <td>    0.021</td> <td>   -0.821</td> <td> 0.411</td> <td>   -0.059</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site</th>      <td>    0.0674</td> <td>    0.049</td> <td>    1.370</td> <td> 0.171</td> <td>   -0.029</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis</th>       <td>   -0.0190</td> <td>    0.132</td> <td>   -0.145</td> <td> 0.885</td> <td>   -0.277</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep</th>       <td>   -0.1137</td> <td>    0.122</td> <td>   -0.930</td> <td> 0.352</td> <td>   -0.353</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th> <td>    0.8112</td> <td>    0.109</td> <td>    7.411</td> <td> 0.000</td> <td>    0.597</td> <td>    1.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past</th>   <td>    0.1293</td> <td>    0.121</td> <td>    1.071</td> <td> 0.284</td> <td>   -0.107</td> <td>    0.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>       <td>    0.2986</td> <td>    0.116</td> <td>    2.570</td> <td> 0.010</td> <td>    0.071</td> <td>    0.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic</th>    <td>    0.1255</td> <td>    0.075</td> <td>    1.673</td> <td> 0.094</td> <td>   -0.021</td> <td>    0.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin</th>       <td>    0.0906</td> <td>    0.083</td> <td>    1.090</td> <td> 0.276</td> <td>   -0.072</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat</th>   <td>    0.1672</td> <td>    0.124</td> <td>    1.347</td> <td> 0.178</td> <td>   -0.076</td> <td>    0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat</th>   <td>    0.0188</td> <td>    0.079</td> <td>    0.237</td> <td> 0.813</td> <td>   -0.136</td> <td>    0.174</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:           exclusiontot   No. Observations:                 1623\n",
       "Model:                          Logit   Df Residuals:                     1608\n",
       "Method:                           MLE   Df Model:                           14\n",
       "Date:                Fri, 15 Jul 2022   Pseudo R-squ.:                  0.1358\n",
       "Time:                        09:38:04   Log-Likelihood:                -966.33\n",
       "converged:                       True   LL-Null:                       -1118.1\n",
       "Covariance Type:            nonrobust   LLR p-value:                 2.105e-56\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -5.0658      0.521     -9.717      0.000      -6.088      -4.044\n",
       "gad1log        0.4730      0.329      1.437      0.151      -0.172       1.118\n",
       "phq1log        0.2663      0.399      0.668      0.504      -0.515       1.047\n",
       "age            0.0522      0.004     11.856      0.000       0.044       0.061\n",
       "cisscore      -0.0175      0.021     -0.821      0.411      -0.059       0.024\n",
       "site           0.0674      0.049      1.370      0.171      -0.029       0.164\n",
       "cis           -0.0190      0.132     -0.145      0.885      -0.277       0.239\n",
       "dep           -0.1137      0.122     -0.930      0.352      -0.353       0.126\n",
       "education      0.8112      0.109      7.411      0.000       0.597       1.026\n",
       "AD_past        0.1293      0.121      1.071      0.284      -0.107       0.366\n",
       "sex            0.2986      0.116      2.570      0.010       0.071       0.526\n",
       "ethnic         0.1255      0.075      1.673      0.094      -0.021       0.272\n",
       "fin            0.0906      0.083      1.090      0.276      -0.072       0.254\n",
       "empstat        0.1672      0.124      1.347      0.178      -0.076       0.410\n",
       "marstat        0.0188      0.079      0.237      0.813      -0.136       0.174\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = smf.logit('exclusiontot ~ gad1log + phq1log + age + cisscore + site + cis + ' + \\\n",
    "                  'dep + education + AD_past + sex + ethnic + fin + empstat + marstat', data=mle_df).fit()\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "age, sex, and education is related to uninformativ task runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 2, sert: -0.66(0.73) placebo: -0.54(0.75)\tgroup: \tbeta: -0.08,\tCI: [-0.24,0.09],\tpvalue: 0.3612\n",
      "T = 6, sert: -0.67(0.78) placebo: -0.74(0.8)\tgroup: \tbeta: 0.13,\tCI: [-0.04,0.3],\tpvalue: 0.1463\n",
      "over time\t\t\t\t\tgroup: \tbeta: 0.01,\tCI: [-0.11,0.14],\tpvalue: 0.8225\n",
      "time group interaction\t\t\t\tgroup: \tbeta: -0.24,\tCI: [-0.64,0.16],\tpvalue: 0.233\n"
     ]
    }
   ],
   "source": [
    "# H1: aversive Pav related to sertraline?\n",
    "timing = [((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==0), \\\n",
    "       ((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==0), (mle_df['exclusiontot']==0)]\n",
    "timing_label = ['2','6','over time', 'time x group']\n",
    "\n",
    "for i in range(2):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print('T = '+timing_label[i]+', sert: ' + str(np.round(tmp['av_Pav'][(tmp['group']==1)&(tmp['time']==i+2)].mean(),2)) + '(' \\\n",
    "      + str(np.round(tmp['av_Pav'][(tmp['group']==1)&(tmp['time']==i+2)].std(),2)) + ') placebo: ' \\\n",
    "      + str(np.round(tmp['av_Pav'][(tmp['group']==0)&(tmp['time']==i+2)].mean(),2)) + '(' \\\n",
    "      + str(np.round(tmp['av_Pav'][(tmp['group']==0)&(tmp['time']==i+2)].std(),2)) +')', end='\\t')\n",
    "    panda.mle('av_Pav ~ group + time + site + cis + dep + sex + age + education', tmp, ['group'],[])\n",
    "    \n",
    "print('over time', end='\\t\\t\\t\\t\\t')\n",
    "panda.mle('av_Pav ~ group + time + site + cis + dep + sex + age + education', mle_df[timing[2]], ['group'],[])\n",
    "print('time group interaction', end='\\t\\t\\t\\t')\n",
    "panda.mle('av_Pav ~ group * time + site + cis + dep + sex + age + education', mle_df[timing[2]], ['group'],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:\tav_Pav_zscore: \tbeta: -0.03,\tCI: [-0.08,0.03],\tpvalue: 0.3695\n",
      "6:\tav_Pav_zscore: \tbeta: -0.02,\tCI: [-0.08,0.05],\tpvalue: 0.6546\n",
      "over time:\tav_Pav_zscore: \tbeta: -0.03,\tCI: [-0.08,0.02],\tpvalue: 0.1833\n"
     ]
    }
   ],
   "source": [
    "# H2: aversive Pav related to anxiety?\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
    "    panda.mle('gadlog_zscore ~ av_Pav_zscore + group + time + site + cis + dep + sex + age + education', tmp, ['av_Pav_zscore'], 'av_Pav_zscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:\tapp_Pav_zscore: \tbeta: -0.03,\tCI: [-0.08,0.02],\tpvalue: 0.1892\n",
      "6:\tapp_Pav_zscore: \tbeta: -0.08,\tCI: [-0.14,-0.02],\tpvalue: 0.0139\n",
      "over time:\tapp_Pav_zscore: \tbeta: -0.06,\tCI: [-0.11,-0.02],\tpvalue: 0.0086\n"
     ]
    }
   ],
   "source": [
    "# H5: appetitive Pav related to anxiety?\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
    "    panda.mle('phqlog_zscore ~ app_Pav_zscore + group + time + site + cis + dep + sex + age + education', tmp, ['app_Pav_zscore'], 'app_Pav_zscore')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:\trew_se_zscore: \tbeta: -0.03,\tCI: [-0.11,0.05],\tpvalue: 0.5102\n",
      "6:\trew_se_zscore: \tbeta: 0.04,\tCI: [-0.06,0.13],\tpvalue: 0.4427\n",
      "over time:\trew_se_zscore: \tbeta: -0.02,\tCI: [-0.09,0.05],\tpvalue: 0.6257\n"
     ]
    }
   ],
   "source": [
    "# H6: reward sensitivity related to anhedonia?\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
    "    panda.mle('anhedonia_zscore ~ rew_se_zscore + group + time + site + cis + dep + sex + age + education', tmp, ['rew_se_zscore'], 'rew_se_zscore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only found evidence for Hypothesis 5, appetitive Pavlovian bias was reduced with depression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Analyses\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.11,0.01],\tpvalue: 0.0969\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.02,\tCI: [-0.09,0.06],\tpvalue: 0.6394\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.02,\tCI: [-0.08,0.03],\tpvalue: 0.3819\n",
      "phq:\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.1,-0.0],\tpvalue: 0.0429\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.01,\tCI: [-0.08,0.05],\tpvalue: 0.6691\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.08,0.01],\tpvalue: 0.1358\n",
      "beck:\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.1,0.01],\tpvalue: 0.0845\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.1,0.04],\tpvalue: 0.4132\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.09,0.01],\tpvalue: 0.0815\n"
     ]
    }
   ],
   "source": [
    "# does exlusion criteria have an effect on the effect of sertraline on anxiety overall?\n",
    "timing = [((mle_df['time']==1)|(mle_df['time']==2)),((mle_df['time']==1)|(mle_df['time']==3)), \\\n",
    "          ((mle_df['time']==1)|(mle_df['time']==2)|(mle_df['time']==3))]\n",
    "timing_label = ['T = 2\\t','T = 6\\t','over time']\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in range(3):\n",
    "        tmp = mle_df[timing[i]]\n",
    "        print(timing_label[i] + ':', end='\\t')\n",
    "        panda.mle(j + 'log ~ group * exclusiontot + time + site + cis + dep', tmp, ['group:exclusiontot'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "T = 2: exclusion = 0\tgroup: \tbeta: -0.0,\tCI: [-0.05,0.05],\tpvalue: 0.8806\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.1,\tCI: [-0.16,-0.04],\tpvalue: 0.0006\n",
      "phq:\n",
      "T = 2: exclusion = 0\tgroup: \tbeta: 0.0,\tCI: [-0.04,0.04],\tpvalue: 0.8364\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.06,\tCI: [-0.11,-0.0],\tpvalue: 0.0355\n",
      "beck:\n",
      "T = 2: exclusion = 0\tgroup: \tbeta: 0.02,\tCI: [-0.03,0.06],\tpvalue: 0.5078\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.06,\tCI: [-0.11,-0.0],\tpvalue: 0.0489\n"
     ]
    }
   ],
   "source": [
    "# posthoc at T = 2\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in range(2):\n",
    "        print('T = 2: exclusion = ' + str(i), end='\\t') \n",
    "        panda.mle(j + 'log ~ group + time + site + cis + dep', \\\n",
    "                  mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==i)], ['group'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "T = 3: exclusion = 0\tgroup: \tbeta: -0.1,\tCI: [-0.16,-0.04],\tpvalue: 0.0016\n",
      "T = 3: exclusion = 1\tgroup: \tbeta: -0.08,\tCI: [-0.16,-0.01],\tpvalue: 0.0302\n",
      "phq:\n",
      "T = 3: exclusion = 0\tgroup: \tbeta: -0.01,\tCI: [-0.06,0.05],\tpvalue: 0.8097\n",
      "T = 3: exclusion = 1\tgroup: \tbeta: -0.03,\tCI: [-0.1,0.04],\tpvalue: 0.3746\n",
      "beck:\n",
      "T = 3: exclusion = 0\tgroup: \tbeta: -0.0,\tCI: [-0.06,0.06],\tpvalue: 0.9703\n",
      "T = 3: exclusion = 1\tgroup: \tbeta: -0.04,\tCI: [-0.11,0.03],\tpvalue: 0.3054\n"
     ]
    }
   ],
   "source": [
    "# posthoc at T = 3\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in range(2):\n",
    "        print('T = 3: exclusion = ' + str(i), end='\\t') \n",
    "        panda.mle(j + 'log ~ group + time + site + cis + dep', \\\n",
    "                  mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==i)], ['group'], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the task properly is NOT modulating the effect of sertraline on anxiety. At week 2 a trend towards an effect of sertraline on anxiety and it does modulate the effect of sertraline on depression: in the exclusion group and none in the included group. Note for anxiety, we might be underpowered to actually see the interaction effect.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "acctot\n",
      "-------\n",
      "T = 2\t:\tgroup: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.7761\n",
      "T = 6\t:\tgroup: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.2301\n",
      "over time:\tgroup: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.4963\n",
      "-------\n",
      "acc_g2w\n",
      "-------\n",
      "T = 2\t:\tgroup: \tbeta: 0.03,\tCI: [-0.02,0.07],\tpvalue: 0.2108\n",
      "T = 6\t:\tgroup: \tbeta: 0.02,\tCI: [-0.03,0.07],\tpvalue: 0.4248\n",
      "over time:\tgroup: \tbeta: 0.03,\tCI: [-0.01,0.06],\tpvalue: 0.1025\n",
      "-------\n",
      "acc_g2a\n",
      "-------\n",
      "T = 2\t:\tgroup: \tbeta: 0.03,\tCI: [-0.01,0.06],\tpvalue: 0.1648\n",
      "T = 6\t:\tgroup: \tbeta: 0.03,\tCI: [-0.01,0.07],\tpvalue: 0.1232\n",
      "over time:\tgroup: \tbeta: 0.03,\tCI: [0.0,0.06],\tpvalue: 0.0407\n",
      "-------\n",
      "acc_ng2w\n",
      "-------\n",
      "T = 2\t:\tgroup: \tbeta: -0.04,\tCI: [-0.09,0.01],\tpvalue: 0.1194\n",
      "T = 6\t:\tgroup: \tbeta: 0.01,\tCI: [-0.05,0.06],\tpvalue: 0.801\n",
      "over time:\tgroup: \tbeta: -0.02,\tCI: [-0.06,0.02],\tpvalue: 0.2532\n",
      "-------\n",
      "acc_ng2a\n",
      "-------\n",
      "T = 2\t:\tgroup: \tbeta: -0.02,\tCI: [-0.05,0.01],\tpvalue: 0.1697\n",
      "T = 6\t:\tgroup: \tbeta: -0.02,\tCI: [-0.05,0.02],\tpvalue: 0.328\n",
      "over time:\tgroup: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1853\n"
     ]
    }
   ],
   "source": [
    "# does the group have an effect on accuracy when controlling for the exclusion group?\n",
    "x = [x for x in gng_columns if 'acc' in x]\n",
    "for j in x:\n",
    "    print('-------\\n' + j + '\\n-------')\n",
    "    for i in range(3):\n",
    "        tmp = mle_df[timing[i]]\n",
    "        print(timing_label[i] + ':', end='\\t')\n",
    "        panda.mle(j + ' ~ group + exclusiontot + time + site + cis + dep', tmp, ['group'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 2\t exclusion = 0\t\tgroup: \tbeta: 0.0,\tCI: [-0.03,0.03],\tpvalue: 0.8992\n",
      "T = 2\t exclusion = 1\t\tgroup: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.5501\n",
      "T = 6\t exclusion = 0\t\tgroup: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.4768\n",
      "T = 6\t exclusion = 1\t\tgroup: \tbeta: 0.01,\tCI: [-0.0,0.03],\tpvalue: 0.0873\n",
      "over time exclusion = 0\t\tgroup: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.4914\n",
      "over time exclusion = 1\t\tgroup: \tbeta: 0.0,\tCI: [-0.01,0.01],\tpvalue: 0.7322\n"
     ]
    }
   ],
   "source": [
    "# posthoc at all timepoints\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        print(timing_label[i] + ' exclusion = ' + str(j), end='\\t\\t') \n",
    "        panda.mle('acctot ~ group + time', \\\n",
    "                  mle_df[timing[i]&(mle_df['exclusiontot']==j)], ['group'], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously in the excluded group accuracy is lower. Over time accuracy increases and apparently time decreases the influence on excluded group on accruacy (in the included group they get better wherease in the excluded group they stay the same). But overall drug group has no effect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction for multiple comparison: pval < 0.00625\n"
     ]
    }
   ],
   "source": [
    "print('correction for multiple comparison: pval < ' + str(0.05/8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se\t"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7f33574be9b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     panda.mle(i + ' ~ group + time + site + cis + dep + age + education + sex', \\\n\u001b[0m\u001b[1;32m      4\u001b[0m                         mle_df[(mle_df['time']==1)&(mle_df['time']==2)], ['group'], [])\n",
      "\u001b[0;32m~/phd/projects/gng_panda_antler/gng_panda/code/loading_preparing_data.py\u001b[0m in \u001b[0;36mmle\u001b[0;34m(self, formula, data, var_of_interest, random_effects)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixedlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre_formula\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvc_formula\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mrandom_effects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'0 + '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrandom_effects\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixedlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre_formula\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvar_of_interest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_of_interest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, re_formula, vc_formula, subset, use_sparse, missing, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exog_vc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog_vc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"groups\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         mod = super(MixedLM, cls).from_formula(\n\u001b[0m\u001b[1;32m   1047\u001b[0m             formula, data, *args, **kwargs)\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0;34m'formula'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# attach formula for unpckling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                        'design_info': design_info})\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# since we got a dataframe, attach the original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, groups, exog_re, exog_vc, use_sqrt, missing, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# Calling super creates self.endog, etc. as ndarrays and the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# original exog, endog, etc. are self.data.endog, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         super(MixedLM, self).__init__(endog, exog, groups=groups,\n\u001b[0m\u001b[1;32m    742\u001b[0m                                       \u001b[0mexog_re\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                       **kwargs)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'missing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0m\u001b[1;32m     93\u001b[0m                                       **kwargs)\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0m\u001b[1;32m    674\u001b[0m                  **kwargs)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_handle_constant\u001b[0;34m(self, hasconst)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# detect where the constant is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mcheck_implicit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mexog_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMissingDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exog contains inf or nans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m     \"\"\"\n\u001b[0;32m-> 2791\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2792\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end='\\t')\n",
    "    panda.mle(i + ' ~ group + time + site + cis + dep + age + education + sex', \\\n",
    "                        mle_df[(mle_df['time']==1)&(mle_df['time']==2)], ['group'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: rew_se    \n",
      "No. Observations: 886     Method:             REML      \n",
      "No. Groups:       427     Scale:              0.9812    \n",
      "Min. group size:  1       Log-Likelihood:     -1265.5939\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.1                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      3.406    0.253 13.478 0.000  2.911  3.901\n",
      "group          0.092    0.081  1.140 0.254 -0.066  0.250\n",
      "time           0.093    0.046  2.052 0.040  0.004  0.183\n",
      "site          -0.040    0.031 -1.300 0.194 -0.099  0.020\n",
      "cis            0.018    0.044  0.403 0.687 -0.069  0.105\n",
      "dep           -0.054    0.073 -0.741 0.458 -0.197  0.089\n",
      "age           -0.004    0.003 -1.670 0.095 -0.010  0.001\n",
      "education     -0.146    0.079 -1.852 0.064 -0.300  0.009\n",
      "sex           -0.084    0.068 -1.232 0.218 -0.217  0.050\n",
      "subject Var    0.000    0.042                           \n",
      "========================================================\n",
      "\n",
      "loss_se\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: loss_se   \n",
      "No. Observations: 886     Method:             REML      \n",
      "No. Groups:       427     Scale:              0.6455    \n",
      "Min. group size:  1       Log-Likelihood:     -1186.7041\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.1                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      1.429    0.262  5.454 0.000  0.915  1.942\n",
      "group          0.025    0.078  0.324 0.746 -0.127  0.177\n",
      "time           0.092    0.039  2.333 0.020  0.015  0.169\n",
      "site           0.009    0.032  0.289 0.773 -0.054  0.073\n",
      "cis            0.012    0.047  0.258 0.796 -0.081  0.105\n",
      "dep            0.107    0.078  1.376 0.169 -0.045  0.260\n",
      "age           -0.006    0.003 -1.969 0.049 -0.011 -0.000\n",
      "education     -0.068    0.082 -0.834 0.404 -0.229  0.092\n",
      "sex           -0.077    0.073 -1.059 0.290 -0.220  0.066\n",
      "subject Var    0.209    0.064                           \n",
      "========================================================\n",
      "\n",
      "rew_LR\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: rew_LR    \n",
      "No. Observations: 886     Method:             REML      \n",
      "No. Groups:       427     Scale:              1.3528    \n",
      "Min. group size:  1       Log-Likelihood:     -1431.8516\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.1                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept     -1.793    0.315 -5.683 0.000 -2.411 -1.174\n",
      "group          0.018    0.099  0.184 0.854 -0.176  0.213\n",
      "time           0.132    0.054  2.421 0.015  0.025  0.238\n",
      "site          -0.001    0.038 -0.022 0.983 -0.076  0.074\n",
      "cis           -0.091    0.056 -1.635 0.102 -0.201  0.018\n",
      "dep           -0.016    0.092 -0.179 0.858 -0.196  0.163\n",
      "age           -0.012    0.003 -3.660 0.000 -0.019 -0.006\n",
      "education      0.183    0.098  1.862 0.063 -0.010  0.375\n",
      "sex           -0.009    0.086 -0.103 0.918 -0.177  0.159\n",
      "subject Var    0.085    0.053                           \n",
      "========================================================\n",
      "\n",
      "loss_LR\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: loss_LR   \n",
      "No. Observations: 886     Method:             REML      \n",
      "No. Groups:       427     Scale:              2.1844    \n",
      "Min. group size:  1       Log-Likelihood:     -1673.5171\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.1                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept     -0.493    0.431 -1.145 0.252 -1.339  0.352\n",
      "group          0.045    0.133  0.335 0.738 -0.216  0.305\n",
      "time           0.163    0.070  2.314 0.021  0.025  0.301\n",
      "site          -0.012    0.053 -0.232 0.817 -0.116  0.091\n",
      "cis           -0.093    0.077 -1.210 0.226 -0.245  0.058\n",
      "dep           -0.295    0.127 -2.329 0.020 -0.543 -0.047\n",
      "age           -0.010    0.005 -2.155 0.031 -0.019 -0.001\n",
      "education      0.024    0.135  0.176 0.860 -0.240  0.287\n",
      "sex            0.133    0.119  1.124 0.261 -0.099  0.366\n",
      "subject Var    0.336    0.087                           \n",
      "========================================================\n",
      "\n",
      "app_Pav\t         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: app_Pav  \n",
      "No. Observations: 886     Method:             REML     \n",
      "No. Groups:       427     Scale:              0.2251   \n",
      "Min. group size:  1       Log-Likelihood:     -709.4814\n",
      "Max. group size:  3       Converged:          Yes      \n",
      "Mean group size:  2.1                                  \n",
      "-------------------------------------------------------\n",
      "             Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------\n",
      "Intercept    -0.516    0.149 -3.452 0.001 -0.808 -0.223\n",
      "group         0.029    0.045  0.647 0.518 -0.059  0.117\n",
      "time         -0.080    0.023 -3.468 0.001 -0.125 -0.035\n",
      "site         -0.043    0.018 -2.331 0.020 -0.079 -0.007\n",
      "cis           0.044    0.027  1.619 0.106 -0.009  0.096\n",
      "dep          -0.060    0.044 -1.349 0.177 -0.146  0.027\n",
      "age           0.011    0.002  6.771 0.000  0.008  0.014\n",
      "education     0.042    0.047  0.897 0.370 -0.050  0.133\n",
      "sex           0.008    0.041  0.184 0.854 -0.074  0.089\n",
      "subject Var   0.060    0.032                           \n",
      "=======================================================\n",
      "\n",
      "av_Pav\t         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: av_Pav   \n",
      "No. Observations: 886     Method:             REML     \n",
      "No. Groups:       427     Scale:              0.3540   \n",
      "Min. group size:  1       Log-Likelihood:     -985.9057\n",
      "Max. group size:  3       Converged:          Yes      \n",
      "Mean group size:  2.1                                  \n",
      "-------------------------------------------------------\n",
      "             Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------\n",
      "Intercept    -1.093    0.224 -4.883 0.000 -1.532 -0.655\n",
      "group         0.014    0.062  0.224 0.823 -0.108  0.136\n",
      "time         -0.109    0.030 -3.617 0.000 -0.168 -0.050\n",
      "site         -0.024    0.028 -0.861 0.389 -0.079  0.031\n",
      "cis           0.065    0.041  1.577 0.115 -0.016  0.145\n",
      "dep           0.066    0.067  0.979 0.327 -0.066  0.198\n",
      "age           0.011    0.002  4.755 0.000  0.007  0.016\n",
      "education     0.057    0.070  0.808 0.419 -0.081  0.194\n",
      "sex           0.038    0.063  0.593 0.553 -0.087  0.162\n",
      "subject Var   0.219    0.061                           \n",
      "=======================================================\n",
      "\n",
      "noise\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: noise     \n",
      "No. Observations: 886     Method:             REML      \n",
      "No. Groups:       427     Scale:              0.9456    \n",
      "Min. group size:  1       Log-Likelihood:     -1330.6052\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.1                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      2.443    0.301  8.115 0.000  1.853  3.033\n",
      "group         -0.054    0.091 -0.595 0.552 -0.232  0.124\n",
      "time           0.080    0.047  1.693 0.091 -0.013  0.172\n",
      "site           0.081    0.037  2.179 0.029  0.008  0.153\n",
      "cis            0.025    0.054  0.471 0.638 -0.080  0.131\n",
      "dep            0.167    0.089  1.887 0.059 -0.007  0.341\n",
      "age           -0.003    0.003 -1.082 0.279 -0.010  0.003\n",
      "education     -0.100    0.094 -1.066 0.287 -0.284  0.084\n",
      "sex            0.041    0.083  0.489 0.625 -0.122  0.204\n",
      "subject Var    0.222    0.069                           \n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: bias      \n",
      "No. Observations: 886     Method:             REML      \n",
      "No. Groups:       427     Scale:              0.6596    \n",
      "Min. group size:  1       Log-Likelihood:     -1204.1603\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.1                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      1.284    0.270  4.762 0.000  0.755  1.812\n",
      "group          0.060    0.079  0.762 0.446 -0.095  0.216\n",
      "time           0.152    0.040  3.811 0.000  0.074  0.230\n",
      "site          -0.005    0.033 -0.138 0.890 -0.070  0.061\n",
      "cis           -0.090    0.049 -1.830 0.067 -0.185  0.006\n",
      "dep           -0.083    0.080 -1.031 0.302 -0.240  0.075\n",
      "age           -0.027    0.003 -9.442 0.000 -0.033 -0.022\n",
      "education     -0.030    0.084 -0.359 0.719 -0.195  0.135\n",
      "sex           -0.006    0.075 -0.084 0.933 -0.154  0.141\n",
      "subject Var    0.235    0.060                           \n",
      "========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ between drug group (included group)?\n",
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end='\\t')\n",
    "    panda.mle(i + ' ~ group + time + site + cis + dep + age + education + sex', \\\n",
    "                        mle_df[mle_df['exclusiontot']==0], [], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rew and loss  sensitivities and LRs and bias increases over time\n",
    "- app and av Pav decrease over time\n",
    "- noise differs between site\n",
    "- Note: learning rates, Pavlovian biases, and go bias are related to age!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = parameter_labels\n",
    "for col in cols:\n",
    "    col_zscore = col + '_zscore'\n",
    "    mle_df[col_zscore] = (mle_df[col] - mle_df[col].mean())/mle_df[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se\tgroup:time: \tbeta: -0.13,\tCI: [-0.39,0.12],\tpvalue: 0.3063\n",
      "loss_se\tgroup:time: \tbeta: 0.05,\tCI: [-0.16,0.27],\tpvalue: 0.6447\n",
      "rew_LR\tgroup:time: \tbeta: -0.01,\tCI: [-0.31,0.3],\tpvalue: 0.9606\n",
      "loss_LR\tgroup:time: \tbeta: -0.38,\tCI: [-0.77,0.01],\tpvalue: 0.0533\n",
      "app_Pav\tgroup:time: \tbeta: -0.08,\tCI: [-0.21,0.04],\tpvalue: 0.1962\n",
      "av_Pav\tgroup:time: \tbeta: 0.11,\tCI: [-0.05,0.27],\tpvalue: 0.1829\n",
      "noise\tgroup:time: \tbeta: 0.11,\tCI: [-0.15,0.36],\tpvalue: 0.4264\n",
      "bias\tgroup:time: \tbeta: -0.12,\tCI: [-0.34,0.1],\tpvalue: 0.2954\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ time x group interaction (included group)?\n",
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end = '\\t');\n",
    "    panda.mle(i + ' ~ group * time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "               mle_df[(mle_df['exclusiontot']==0)], ['group:time'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se\tgroup: \tbeta: 0.33,\tCI: [0.11,0.56],\tpvalue: 0.0036\n",
      "loss_se\tgroup: \tbeta: -0.16,\tCI: [-0.37,0.04],\tpvalue: 0.1135\n",
      "rew_LR\tgroup: \tbeta: -0.1,\tCI: [-0.37,0.17],\tpvalue: 0.4713\n",
      "loss_LR\tgroup: \tbeta: 0.6,\tCI: [0.25,0.95],\tpvalue: 0.0009\n",
      "app_Pav\tgroup: \tbeta: 0.09,\tCI: [-0.03,0.21],\tpvalue: 0.1369\n",
      "av_Pav\tgroup: \tbeta: -0.08,\tCI: [-0.24,0.09],\tpvalue: 0.3617\n",
      "noise\tgroup: \tbeta: -0.19,\tCI: [-0.44,0.06],\tpvalue: 0.1384\n",
      "bias\tgroup: \tbeta: 0.0,\tCI: [-0.2,0.21],\tpvalue: 0.9631\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ between drug group at session 2 (included group)?\n",
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end = '\\t');\n",
    "    panda.mle(i + ' ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "               mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==0)], ['group'],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se\tgroup: \tbeta: -0.1,\tCI: [-0.33,0.14],\tpvalue: 0.4197\n",
      "loss_se\tgroup: \tbeta: 0.15,\tCI: [-0.06,0.36],\tpvalue: 0.1665\n",
      "rew_LR\tgroup: \tbeta: 0.07,\tCI: [-0.21,0.35],\tpvalue: 0.6116\n",
      "loss_LR\tgroup: \tbeta: -0.41,\tCI: [-0.79,-0.03],\tpvalue: 0.0323\n",
      "app_Pav\tgroup: \tbeta: -0.03,\tCI: [-0.16,0.09],\tpvalue: 0.6123\n",
      "av_Pav\tgroup: \tbeta: 0.13,\tCI: [-0.04,0.3],\tpvalue: 0.1422\n",
      "noise\tgroup: \tbeta: 0.06,\tCI: [-0.2,0.31],\tpvalue: 0.6483\n",
      "bias\tgroup: \tbeta: 0.03,\tCI: [-0.19,0.25],\tpvalue: 0.8035\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ between drug group at session 3 (included group)?\n",
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end = '\\t');\n",
    "    panda.mle(i + ' ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "               mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==0)], ['group'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline bias\tgroup: \tbeta: 0.22,\tCI: [-0.0,0.45],\tpvalue: 0.0522\n"
     ]
    }
   ],
   "source": [
    "# does the random bias differ between drug group in the excluded group?\n",
    "print('random baseline bias', end = '\\t');\n",
    "panda.mle('rbias ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "           mle_df[mle_df['exclusiontot']==1], ['group'],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline bias at T = 2\tgroup: \tbeta: 0.16,\tCI: [-0.14,0.47],\tpvalue: 0.2886\n",
      "random baseline bias at T = 3\tgroup: \tbeta: 0.28,\tCI: [-0.05,0.62],\tpvalue: 0.0981\n"
     ]
    }
   ],
   "source": [
    "# does the random bias differ between drug group in the excluded group at session 2 and 3?\n",
    "for i in range(2,4):\n",
    "    print('random baseline bias at T = ' + str(i), end = '\\t');\n",
    "    panda.mle('rbias ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                            mle_df[((mle_df['time']==1)|(mle_df['time']==i))&(mle_df['exclusiontot']==1)], ['group'],[])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drug effects:\n",
    "- at 2 weeks:\n",
    "    - reward sensitivity HIGHER in sertraline group\n",
    "    - loss LR HIGHER in sertraline group\n",
    "    - trend towards LOWER av Pav in sertraline group\n",
    "- at 6 weeks:\n",
    "    - trend towards loss LR LOWER in sertraline group at 6 weeks\n",
    "- over all sessions:\n",
    "    - -\n",
    "- time x group:\n",
    "    - for loss LR interaction significant over time effect reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "rew_se_zscore: \tbeta: 0.01,\tCI: [-0.05,0.07],\tpvalue: 0.7089\n",
      "loss_se_zscore: \tbeta: -0.04,\tCI: [-0.1,0.02],\tpvalue: 0.2059\n",
      "rew_LR_zscore: \tbeta: 0.0,\tCI: [-0.05,0.05],\tpvalue: 0.8971\n",
      "loss_LR_zscore: \tbeta: 0.07,\tCI: [0.02,0.12],\tpvalue: 0.004\n",
      "app_Pav_zscore: \tbeta: -0.01,\tCI: [-0.06,0.04],\tpvalue: 0.6077\n",
      "av_Pav_zscore: \tbeta: -0.04,\tCI: [-0.09,0.01],\tpvalue: 0.1518\n",
      "noise_zscore: \tbeta: 0.02,\tCI: [-0.05,0.08],\tpvalue: 0.6502\n",
      "bias_zscore: \tbeta: 0.0,\tCI: [-0.07,0.08],\tpvalue: 0.9843\n",
      "phq:\n",
      "rew_se_zscore: \tbeta: -0.0,\tCI: [-0.06,0.06],\tpvalue: 0.9736\n",
      "loss_se_zscore: \tbeta: -0.02,\tCI: [-0.08,0.04],\tpvalue: 0.4962\n",
      "rew_LR_zscore: \tbeta: 0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9081\n",
      "loss_LR_zscore: \tbeta: 0.04,\tCI: [-0.01,0.09],\tpvalue: 0.1348\n",
      "app_Pav_zscore: \tbeta: -0.07,\tCI: [-0.11,-0.02],\tpvalue: 0.0078\n",
      "av_Pav_zscore: \tbeta: -0.01,\tCI: [-0.06,0.04],\tpvalue: 0.5944\n",
      "noise_zscore: \tbeta: 0.03,\tCI: [-0.03,0.1],\tpvalue: 0.3371\n",
      "bias_zscore: \tbeta: 0.03,\tCI: [-0.04,0.11],\tpvalue: 0.3784\n",
      "beck:\n",
      "rew_se_zscore: \tbeta: 0.04,\tCI: [-0.02,0.09],\tpvalue: 0.2391\n",
      "loss_se_zscore: \tbeta: -0.04,\tCI: [-0.09,0.02],\tpvalue: 0.2161\n",
      "rew_LR_zscore: \tbeta: -0.01,\tCI: [-0.07,0.05],\tpvalue: 0.7233\n",
      "loss_LR_zscore: \tbeta: 0.05,\tCI: [0.0,0.11],\tpvalue: 0.0355\n",
      "app_Pav_zscore: \tbeta: -0.03,\tCI: [-0.08,0.02],\tpvalue: 0.2852\n",
      "av_Pav_zscore: \tbeta: -0.02,\tCI: [-0.07,0.03],\tpvalue: 0.4388\n",
      "noise_zscore: \tbeta: -0.02,\tCI: [-0.09,0.05],\tpvalue: 0.6293\n",
      "bias_zscore: \tbeta: 0.03,\tCI: [-0.05,0.1],\tpvalue: 0.4587\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters have an effect on symptoms over time in the included group controlled drug group?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.mle(j + 'log_zscore ~ ' + i + '_zscore + group + time + site + dep + cis + age + education + sex + fin', \\\n",
    "                            mle_df[mle_df['exclusiontot']==0], [i + '_zscore'], i + '_zscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "rew_se_zscore: \tbeta: 0.0,\tCI: [-0.06,0.07],\tpvalue: 0.8856\n",
      "loss_se_zscore: \tbeta: -0.05,\tCI: [-0.11,0.02],\tpvalue: 0.1461\n",
      "rew_LR_zscore: \tbeta: 0.01,\tCI: [-0.05,0.07],\tpvalue: 0.6935\n",
      "loss_LR_zscore: \tbeta: 0.07,\tCI: [0.01,0.13],\tpvalue: 0.0143\n",
      "app_Pav_zscore: \tbeta: 0.0,\tCI: [-0.05,0.06],\tpvalue: 0.9112\n",
      "av_Pav_zscore: \tbeta: -0.03,\tCI: [-0.08,0.03],\tpvalue: 0.3103\n",
      "noise_zscore: \tbeta: -0.02,\tCI: [-0.09,0.05],\tpvalue: 0.5379\n",
      "bias_zscore: \tbeta: 0.05,\tCI: [-0.03,0.13],\tpvalue: 0.2434\n",
      "phq:\n",
      "rew_se_zscore: \tbeta: -0.03,\tCI: [-0.08,0.03],\tpvalue: 0.3875\n",
      "loss_se_zscore: \tbeta: -0.04,\tCI: [-0.1,0.02],\tpvalue: 0.1483\n",
      "rew_LR_zscore: \tbeta: 0.02,\tCI: [-0.04,0.07],\tpvalue: 0.5489\n",
      "loss_LR_zscore: \tbeta: 0.03,\tCI: [-0.02,0.09],\tpvalue: 0.2113\n",
      "app_Pav_zscore: \tbeta: -0.03,\tCI: [-0.08,0.01],\tpvalue: 0.1692\n",
      "av_Pav_zscore: \tbeta: 0.02,\tCI: [-0.04,0.07],\tpvalue: 0.544\n",
      "noise_zscore: \tbeta: -0.01,\tCI: [-0.08,0.05],\tpvalue: 0.7152\n",
      "bias_zscore: \tbeta: 0.04,\tCI: [-0.03,0.11],\tpvalue: 0.2866\n",
      "beck:\n",
      "rew_se_zscore: \tbeta: 0.02,\tCI: [-0.04,0.08],\tpvalue: 0.5443\n",
      "loss_se_zscore: \tbeta: -0.04,\tCI: [-0.1,0.02],\tpvalue: 0.2109\n",
      "rew_LR_zscore: \tbeta: -0.0,\tCI: [-0.06,0.05],\tpvalue: 0.8697\n",
      "loss_LR_zscore: \tbeta: 0.04,\tCI: [-0.01,0.09],\tpvalue: 0.1568\n",
      "app_Pav_zscore: \tbeta: 0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9184\n",
      "av_Pav_zscore: \tbeta: 0.0,\tCI: [-0.05,0.06],\tpvalue: 0.8507\n",
      "noise_zscore: \tbeta: -0.04,\tCI: [-0.11,0.03],\tpvalue: 0.2684\n",
      "bias_zscore: \tbeta: 0.02,\tCI: [-0.06,0.09],\tpvalue: 0.6517\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters have an effect on symptoms at 2 weeks in the included group controlled drug group?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.mle(j + 'log_zscore ~ ' + i + '_zscore + group + time + site + dep + cis + age + education + sex + fin', \\\n",
    "                            mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==0)], \\\n",
    "                            [i + '_zscore'], i + '_zscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "rew_se_zscore: \tbeta: 0.05,\tCI: [-0.03,0.13],\tpvalue: 0.2212\n",
      "loss_se_zscore: \tbeta: -0.01,\tCI: [-0.09,0.06],\tpvalue: 0.7823\n",
      "rew_LR_zscore: \tbeta: -0.01,\tCI: [-0.08,0.06],\tpvalue: 0.766\n",
      "loss_LR_zscore: \tbeta: 0.05,\tCI: [-0.01,0.12],\tpvalue: 0.1205\n",
      "app_Pav_zscore: \tbeta: -0.03,\tCI: [-0.1,0.04],\tpvalue: 0.4192\n",
      "av_Pav_zscore: \tbeta: -0.02,\tCI: [-0.09,0.05],\tpvalue: 0.5833\n",
      "noise_zscore: \tbeta: 0.04,\tCI: [-0.04,0.12],\tpvalue: 0.3478\n",
      "bias_zscore: \tbeta: -0.03,\tCI: [-0.12,0.07],\tpvalue: 0.5875\n",
      "phq:\n",
      "rew_se_zscore: \tbeta: 0.04,\tCI: [-0.04,0.12],\tpvalue: 0.3142\n",
      "loss_se_zscore: \tbeta: -0.01,\tCI: [-0.08,0.07],\tpvalue: 0.9007\n",
      "rew_LR_zscore: \tbeta: 0.0,\tCI: [-0.07,0.07],\tpvalue: 0.9791\n",
      "loss_LR_zscore: \tbeta: 0.05,\tCI: [-0.02,0.11],\tpvalue: 0.1909\n",
      "app_Pav_zscore: \tbeta: -0.08,\tCI: [-0.14,-0.02],\tpvalue: 0.0116\n",
      "av_Pav_zscore: \tbeta: -0.02,\tCI: [-0.09,0.04],\tpvalue: 0.5007\n",
      "noise_zscore: \tbeta: 0.05,\tCI: [-0.03,0.14],\tpvalue: 0.2272\n",
      "bias_zscore: \tbeta: -0.01,\tCI: [-0.11,0.09],\tpvalue: 0.8649\n",
      "beck:\n",
      "rew_se_zscore: \tbeta: 0.07,\tCI: [-0.01,0.15],\tpvalue: 0.0924\n",
      "loss_se_zscore: \tbeta: -0.02,\tCI: [-0.09,0.06],\tpvalue: 0.6284\n",
      "rew_LR_zscore: \tbeta: -0.03,\tCI: [-0.1,0.04],\tpvalue: 0.3947\n",
      "loss_LR_zscore: \tbeta: 0.06,\tCI: [-0.01,0.13],\tpvalue: 0.0942\n",
      "app_Pav_zscore: \tbeta: -0.03,\tCI: [-0.1,0.03],\tpvalue: 0.319\n",
      "av_Pav_zscore: \tbeta: -0.04,\tCI: [-0.11,0.02],\tpvalue: 0.198\n",
      "noise_zscore: \tbeta: 0.02,\tCI: [-0.07,0.11],\tpvalue: 0.6213\n",
      "bias_zscore: \tbeta: -0.02,\tCI: [-0.12,0.08],\tpvalue: 0.6631\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters have an effect on symptoms at 6 weeks in the included group controlled drug group?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.mle(j + 'log_zscore ~ ' + i + '_zscore + group + time + site + dep + cis + age + education + sex + fin', \\\n",
    "                            mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==0)], \\\n",
    "                            [i + '_zscore'], i + '_zscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "T = 2\t:\trbias_zscore: \tbeta: -0.06,\tCI: [-0.12,0.0],\tpvalue: 0.0711\n",
      "T = 6\t:\trbias_zscore: \tbeta: 0.01,\tCI: [-0.06,0.09],\tpvalue: 0.7743\n",
      "over time:\trbias_zscore: \tbeta: 0.0,\tCI: [-0.05,0.06],\tpvalue: 0.9188\n",
      "phq:\n",
      "T = 2\t:\trbias_zscore: \tbeta: -0.03,\tCI: [-0.1,0.04],\tpvalue: 0.3974\n",
      "T = 6\t:\trbias_zscore: \tbeta: 0.03,\tCI: [-0.05,0.1],\tpvalue: 0.4882\n",
      "over time:\trbias_zscore: \tbeta: 0.01,\tCI: [-0.05,0.08],\tpvalue: 0.6837\n",
      "beck:\n",
      "T = 2\t:\trbias_zscore: \tbeta: 0.01,\tCI: [-0.06,0.07],\tpvalue: 0.8231\n",
      "T = 6\t:\trbias_zscore: \tbeta: 0.06,\tCI: [-0.01,0.14],\tpvalue: 0.0871\n",
      "over time:\trbias_zscore: \tbeta: 0.06,\tCI: [-0.0,0.12],\tpvalue: 0.0567\n"
     ]
    }
   ],
   "source": [
    "# do rbias have an effect on symptoms in the excluded group controlled drug group?\n",
    "timing_ex = [((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==1), \\\n",
    "       ((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==1), (mle_df['exclusiontot']==1)]\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in range(3):\n",
    "        tmp = mle_df[timing_ex[i]]\n",
    "        print(timing_label[i] + ':', end='\\t')\n",
    "        panda.mle(j + 'log_zscore ~ rbias_zscore + group + time + age + education + marstat + AD_past', \\\n",
    "                    tmp,['rbias_zscore'], 'rbias_zscore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAD:\n",
    "- at 2 weeks:\n",
    "    - loss LR is POSITIVELY related to GAD over all sessions\n",
    "- at 6 weeks:\n",
    "    - app Pav is NEGATIVELY related to PHQ over all sessions\n",
    "- over all sessions:\n",
    "    - loss LR is POSITIVELY related to GAD over all sessions\n",
    "    - app Pav is NEGATIVELY related to PHQ over all sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "--\n",
    "- Patients who did the task properly:\n",
    "    <br><b>Drug effects:</b>\n",
    "    - at 2 weeks:\n",
    "        - reward sensitivity HIGHER in sertraline group\n",
    "        - loss LR HIGHER in sertraline group\n",
    "        - trend towards LOWER av Pav in sertraline group\n",
    "    - at 6 weeks:\n",
    "        - trend towards loss LR LOWER in sertraline group at 6 weeks\n",
    "    - over all sessions:\n",
    "        - -\n",
    "    - time x group:\n",
    "        - for loss LR interaction significant over time effect reduced\n",
    "    <br><b>GAD:</b>\n",
    "    - at 2 weeks:\n",
    "        - loss LR is POSITIVELY related to GAD over all sessions\n",
    "    - at 6 weeks:\n",
    "        - app Pav is NEGATIVELY related to PHQ over all sessions\n",
    "    - over all sessions:\n",
    "        - loss LR is POSITIVELY related to GAD over all sessions\n",
    "        - app Pav is NEGATIVELY related to PHQ over all sessions\n",
    "- patients who did NOT do the task properly:\n",
    "    <br><b>Drug effects:</b>\n",
    "    - over all sessions:\n",
    "        - trend towards random baseline bias HIGHER in sertraline group over all sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "Change in parameter\n",
    "--\n",
    "early changes in cognitive processing predicting treatment outcome?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(3)\n",
    "for i in parameter_labels:\n",
    "    df_panda[i + '_slope12'] = df_panda[i + str(2)] - df_panda[i + str(1)]\n",
    "    df_panda[i + '_slope23'] = df_panda[i + str(3)] - df_panda[i + str(2)]\n",
    "    df_panda[i + '_slope13'] = df_panda[i + str(3)] - df_panda[i + str(1)]\n",
    "    for sj in range(len(df_panda)):\n",
    "        y = df_panda[[i + str(1),i + str(2),i + str(3)]].iloc[sj]\n",
    "        idx = ~y.isna()\n",
    "        if sum(idx) > 1:\n",
    "            df_panda.loc[sj, i + '_slope'] = np.polyfit(x[idx],y[idx],1)[0]\n",
    "            \n",
    "questionnaires = ['gad', 'phq']\n",
    "for i in questionnaires:\n",
    "    df_panda[i + 'log_slope'] = df_panda[i + '4log'] - df_panda[i + '1log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group: \tbeta: 0.21,\tCI: [-0.16,0.58],\tpvalue: 0.2622\n",
      "group: \tbeta: -0.31,\tCI: [-0.61,-0.01],\tpvalue: 0.0413\n",
      "group: \tbeta: 0.0,\tCI: [-0.44,0.44],\tpvalue: 0.9915\n",
      "group: \tbeta: 0.71,\tCI: [0.16,1.27],\tpvalue: 0.0119\n",
      "group: \tbeta: 0.14,\tCI: [-0.04,0.33],\tpvalue: 0.1235\n",
      "group: \tbeta: -0.02,\tCI: [-0.24,0.21],\tpvalue: 0.8618\n",
      "group: \tbeta: -0.32,\tCI: [-0.69,0.04],\tpvalue: 0.0845\n",
      "group: \tbeta: 0.03,\tCI: [-0.28,0.34],\tpvalue: 0.8426\n"
     ]
    }
   ],
   "source": [
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm(i + '_slope12 ~ group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], ['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group: \tbeta: -0.49,\tCI: [-0.88,-0.1],\tpvalue: 0.0137\n",
      "group: \tbeta: 0.37,\tCI: [0.07,0.67],\tpvalue: 0.0158\n",
      "group: \tbeta: 0.26,\tCI: [-0.21,0.73],\tpvalue: 0.2848\n",
      "group: \tbeta: -0.95,\tCI: [-1.51,-0.38],\tpvalue: 0.001\n",
      "group: \tbeta: -0.14,\tCI: [-0.32,0.04],\tpvalue: 0.121\n",
      "group: \tbeta: 0.11,\tCI: [-0.12,0.34],\tpvalue: 0.3446\n",
      "group: \tbeta: 0.33,\tCI: [-0.03,0.69],\tpvalue: 0.074\n",
      "group: \tbeta: 0.13,\tCI: [-0.18,0.45],\tpvalue: 0.4128\n"
     ]
    }
   ],
   "source": [
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm(i + '_slope23 ~ '+i+'1 + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot2']==0)&(df_panda['exclusiontot3']==0)], ['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "rew_se_slope12: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.8145\n",
      "loss_se_slope12: \tbeta: -0.0,\tCI: [-0.04,0.04],\tpvalue: 0.909\n",
      "rew_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1825\n",
      "loss_LR_slope12: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9718\n",
      "app_Pav_slope12: \tbeta: -0.01,\tCI: [-0.07,0.06],\tpvalue: 0.7703\n",
      "av_Pav_slope12: \tbeta: 0.02,\tCI: [-0.04,0.07],\tpvalue: 0.5384\n",
      "noise_slope12: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.4764\n",
      "bias_slope12: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.816\n",
      "phq:\n",
      "rew_se_slope12: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.7847\n",
      "loss_se_slope12: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.5891\n",
      "rew_LR_slope12: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.6742\n",
      "loss_LR_slope12: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.6229\n",
      "app_Pav_slope12: \tbeta: -0.06,\tCI: [-0.11,-0.0],\tpvalue: 0.0405\n",
      "av_Pav_slope12: \tbeta: 0.02,\tCI: [-0.02,0.06],\tpvalue: 0.3781\n",
      "noise_slope12: \tbeta: 0.02,\tCI: [-0.01,0.05],\tpvalue: 0.1809\n",
      "bias_slope12: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.5031\n",
      "beck:\n",
      "rew_se_slope12: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.6135\n",
      "loss_se_slope12: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.5605\n",
      "rew_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1969\n",
      "loss_LR_slope12: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.3996\n",
      "app_Pav_slope12: \tbeta: -0.0,\tCI: [-0.07,0.06],\tpvalue: 0.9163\n",
      "av_Pav_slope12: \tbeta: 0.04,\tCI: [-0.01,0.09],\tpvalue: 0.1684\n",
      "noise_slope12: \tbeta: 0.02,\tCI: [-0.01,0.05],\tpvalue: 0.1657\n",
      "bias_slope12: \tbeta: -0.01,\tCI: [-0.05,0.02],\tpvalue: 0.4728\n"
     ]
    }
   ],
   "source": [
    "# early parameter change related to future symptoms at week 6\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.glm(j + '3log ~ ' + i + '_slope12 + ' + j + '1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gad:\n",
      "rew_se_slope12: \tbeta: -0.0,\tCI: [-0.04,0.03],\tpvalue: 0.9205\n",
      "loss_se_slope12: \tbeta: 0.02,\tCI: [-0.02,0.07],\tpvalue: 0.2977\n",
      "rew_LR_slope12: \tbeta: -0.03,\tCI: [-0.06,-0.0],\tpvalue: 0.0255\n",
      "loss_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.2014\n",
      "app_Pav_slope12: \tbeta: -0.02,\tCI: [-0.1,0.05],\tpvalue: 0.5049\n",
      "av_Pav_slope12: \tbeta: 0.02,\tCI: [-0.04,0.08],\tpvalue: 0.5158\n",
      "noise_slope12: \tbeta: 0.04,\tCI: [0.01,0.08],\tpvalue: 0.0118\n",
      "bias_slope12: \tbeta: 0.02,\tCI: [-0.02,0.06],\tpvalue: 0.4034\n",
      "phq:\n",
      "rew_se_slope12: \tbeta: -0.01,\tCI: [-0.05,0.02],\tpvalue: 0.3819\n",
      "loss_se_slope12: \tbeta: 0.03,\tCI: [-0.01,0.07],\tpvalue: 0.1794\n",
      "rew_LR_slope12: \tbeta: -0.02,\tCI: [-0.05,0.01],\tpvalue: 0.1105\n",
      "loss_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.0],\tpvalue: 0.094\n",
      "app_Pav_slope12: \tbeta: -0.06,\tCI: [-0.12,0.01],\tpvalue: 0.1081\n",
      "av_Pav_slope12: \tbeta: 0.06,\tCI: [0.0,0.12],\tpvalue: 0.0337\n",
      "noise_slope12: \tbeta: 0.05,\tCI: [0.01,0.08],\tpvalue: 0.0063\n",
      "bias_slope12: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.6052\n",
      "beck:\n",
      "rew_se_slope12: \tbeta: -0.02,\tCI: [-0.05,0.02],\tpvalue: 0.2969\n",
      "loss_se_slope12: \tbeta: 0.02,\tCI: [-0.02,0.07],\tpvalue: 0.2936\n",
      "rew_LR_slope12: \tbeta: -0.02,\tCI: [-0.05,0.01],\tpvalue: 0.3021\n",
      "loss_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1459\n",
      "app_Pav_slope12: \tbeta: -0.02,\tCI: [-0.09,0.06],\tpvalue: 0.6291\n",
      "av_Pav_slope12: \tbeta: 0.06,\tCI: [0.01,0.12],\tpvalue: 0.0328\n",
      "noise_slope12: \tbeta: 0.04,\tCI: [0.01,0.08],\tpvalue: 0.021\n",
      "bias_slope12: \tbeta: 0.0,\tCI: [-0.04,0.04],\tpvalue: 0.9674\n"
     ]
    }
   ],
   "source": [
    "# early parameter change related to future symptoms at week 12\n",
    "for j in psychiatric_questionnaires:\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.glm(j + '4log ~ ' + i + '_slope12 + ' + j + '1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "ax = sns.regplot(x=\"av_Pav_slope12\", y=\"phq_slope\", data=df_panda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se_slope12: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.8145\n",
      "loss_se_slope12: \tbeta: -0.0,\tCI: [-0.04,0.04],\tpvalue: 0.909\n",
      "rew_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1825\n",
      "loss_LR_slope12: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9718\n",
      "app_Pav_slope12: \tbeta: -0.01,\tCI: [-0.07,0.06],\tpvalue: 0.7703\n",
      "av_Pav_slope12: \tbeta: 0.02,\tCI: [-0.04,0.07],\tpvalue: 0.5384\n",
      "noise_slope12: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.4764\n",
      "bias_slope12: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.816\n"
     ]
    }
   ],
   "source": [
    "# early changes in cognitive processing predicting treatment outcome?\n",
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('gad3log ~' + i + '_slope12 + gad1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se_slope12: \tbeta: -0.0,\tCI: [-0.04,0.03],\tpvalue: 0.9205\n",
      "loss_se_slope12: \tbeta: 0.02,\tCI: [-0.02,0.07],\tpvalue: 0.2977\n",
      "rew_LR_slope12: \tbeta: -0.03,\tCI: [-0.06,-0.0],\tpvalue: 0.0255\n",
      "loss_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.2014\n",
      "app_Pav_slope12: \tbeta: -0.02,\tCI: [-0.1,0.05],\tpvalue: 0.5049\n",
      "av_Pav_slope12: \tbeta: 0.02,\tCI: [-0.04,0.08],\tpvalue: 0.5158\n",
      "noise_slope12: \tbeta: 0.04,\tCI: [0.01,0.08],\tpvalue: 0.0118\n",
      "bias_slope12: \tbeta: 0.02,\tCI: [-0.02,0.06],\tpvalue: 0.4034\n"
     ]
    }
   ],
   "source": [
    "# early changes in cognitive processing predicting treatment outcome?\n",
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('gad4log ~' + i + '_slope12 + gad1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se_slope12: \tbeta: -0.01,\tCI: [-0.05,0.02],\tpvalue: 0.3819\n",
      "loss_se_slope12: \tbeta: 0.03,\tCI: [-0.01,0.07],\tpvalue: 0.1794\n",
      "rew_LR_slope12: \tbeta: -0.02,\tCI: [-0.05,0.01],\tpvalue: 0.1105\n",
      "loss_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.0],\tpvalue: 0.094\n",
      "app_Pav_slope12: \tbeta: -0.06,\tCI: [-0.12,0.01],\tpvalue: 0.1081\n",
      "av_Pav_slope12: \tbeta: 0.06,\tCI: [0.0,0.12],\tpvalue: 0.0337\n",
      "noise_slope12: \tbeta: 0.05,\tCI: [0.01,0.08],\tpvalue: 0.0063\n",
      "bias_slope12: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.6052\n"
     ]
    }
   ],
   "source": [
    "# early changes in cognitive processing predicting treatment outcome?\n",
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('phq4log ~' + i + '_slope12 + phq1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 220\n"
     ]
    }
   ],
   "source": [
    "print('N = ' + str(sum((df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbias_slope12: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.4235\n",
      "rbias_slope12: \tbeta: -0.02,\tCI: [-0.05,0.01],\tpvalue: 0.1125\n"
     ]
    }
   ],
   "source": [
    "panda.glm('gad4log ~ rbias_slope12 + gad1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda, ['rbias_slope12'])\n",
    "panda.glm('phq4log ~ rbias_slope12 + phq1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda, ['rbias_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
