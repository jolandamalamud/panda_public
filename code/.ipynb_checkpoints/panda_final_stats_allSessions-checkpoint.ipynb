{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotting import Plotting\n",
    "from loading_preparing_data import Panda\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/jolandamalamud/phd/projects/gng_panda_antler/gng_panda/data/'\n",
    "psychiatric_questionnaires = ['gad', 'phq', 'bdi']\n",
    "panda = Panda(filepath, psychiatric_questionnaires)\n",
    "disp = Plotting()\n",
    "# load raw task data\n",
    "D = panda.load_data()\n",
    "# prep action choices to plot\n",
    "data = panda.extract_data(D)\n",
    "# load modelling data\n",
<<<<<<< HEAD
    "modelling = panda.load_modelling_results('modelling_results/', ['ll2b2a2epxb', 'llb'])\n",
    "#transform_params = panda.transform_parameters(modelling['emmap'])\n",
    "random_baseline_model = panda.load_modelling_results('modelling_results/', ['llb'])\n",
=======
    "modelling = panda.load_modelling_results('modelling_results/final_results/', ['ll2b2a2epxb', 'llb'])\n",
    "#transform_params = panda.transform_parameters(modelling['emmap'])\n",
    "random_baseline_model = panda.load_modelling_results('modelling_results/final_results/', ['llb'])\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
    "# load rct data\n",
    "dfRCT = panda.load_rctdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# modelling = panda.load_modelling_results('modelling_results/', ['ll2b2a2epxb', 'llb'])\n",
    "# print(sum(modelling['excluding']==1), modelling['ibic']/1641)\n",
    "# modelling = panda.load_modelling_results('modelling_results/final_results/', ['ll2b2a2epxb', 'llb'])\n",
    "# print(sum(modelling['excluding']==1), modelling['ibic']/1641)\n",
    "# modelling = panda.load_modelling_results('modelling_results/paper_results/', ['ll2b2a2epxb', 'llb'])\n",
    "# print(sum(modelling['excluding']==1), modelling['ibic']/1641)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "new_flat_list = []\n",
    "questionnaires = {'gad': 7,'phq': 9, 'bdi': 21}\n",
=======
    "new_flat_list = []\n",
    "old_flat_list = []\n",
    "questionnaires = {'gad': 7,'phq': 9, 'beck': 21}\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
    "for q in questionnaires:\n",
    "    y = []\n",
    "    for i in range(1,questionnaires[q]+1):\n",
    "        x = [x for x in dfRCT.columns if q + str(i) in x]\n",
    "        y.append(x)\n",
    "    flat_list = [x for xs in y for x in xs]\n",
    "    flat_list = [x for x in flat_list if 'bin' not in x]\n",
    "    old_flat_list = old_flat_list + flat_list\n",
    "    for i in flat_list:\n",
    "        if '12wk' in i: new_flat_list.append(i[:len(q)+1] + '_4')\n",
    "        elif '6wk' in i: new_flat_list.append(i[:len(q)+1] + '_3')\n",
    "        elif '2wk' in i: new_flat_list.append(i[:len(q)+1] + '_2')\n",
    "        else: new_flat_list.append(i[:4] + '_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all variables of interest\n",
    "parameter_labels = ['rew_se', 'loss_se', 'rew_LR', 'loss_LR', 'app_Pav', 'av_Pav', 'noise', 'bias', 'rbias']\n",
    "parameter_labels_transformed = []\n",
    "# for i in parameter_labels[:-2]: parameter_labels_transformed.append(i + '_trans')\n",
    "gng_columns = parameter_labels + \\\n",
    "                ['exclusion', 'goprotot', 'gopro_g2w', 'gopro_g2a', 'gopro_ng2w', \\\n",
    "                 'gopro_ng2a', 'acctot', 'acc_g2w', 'acc_g2a', 'acc_ng2w', 'acc_ng2a','iL']\n",
    "\n",
    "gng_data = np.vstack((modelling['emmap'], random_baseline_model['emmap'], modelling['excluding'], \\\n",
    "                       np.nanmean(data['a_go'],axis=(0,1)), np.nanmean(data['a_go'],axis=0), \\\n",
    "                       np.nanmean(data['a_correct'],axis=(0,1)), np.nanmean(data['a_correct'],axis=0), \\\n",
    "                       modelling['iL']))\n",
    "\n",
    "rct_columns = ['rctid', 'group', 'gad1', 'gad2log', 'gad3log', 'gad4log', 'phq1', 'phq2log', \\\n",
    "               'phq3log', 'phq4log', 'bdi1', 'bdi2log', 'bdi3log', 'bdi4log',\\\n",
    "               'site', 'cis', 'dep', 'age', 'education', 'AD_past', \\\n",
    "               'sex', 'ethnic', 'fin', 'empstat', 'marstat', 'cisscore'] + new_flat_list\n",
    "\n",
    "rct_data = dfRCT[['identifier_n', 'group', 'gadtot', 'log_gadtot_2wk', 'log_gadtot_6wk', 'log_gadtot_12wk', \\\n",
    "                  'phqtot', 'log_phqtot_2wk', 'log_phqtot_6wk', 'log_phqtot_12wk', 'becktot', \\\n",
    "                  'log_becktot_2wk','log_becktot_6wk','log_becktot_12wk','_site_n', \\\n",
    "                  'cistotal_cat', 'depr_dur_2years', 'age', 'edu3', 'antidepressantsinpast', \\\n",
<<<<<<< HEAD
    "                  'sex', 'ethnic', 'fin3', 'empstat2', 'marstat3', 'cisdepscore'] + flat_list]\n",
    "psychiatric_questionnaires = ['gad', 'phq', 'bdi']"
=======
    "                  'sex', 'ethnic', 'fin3', 'empstat2', 'marstat3', 'cisdepscore'] + old_flat_list]"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of all variables of interest\n",
    "df_panda = panda.create_panda_df(data, gng_columns, gng_data, rct_columns, rct_data)\n",
    "for i in psychiatric_questionnaires:\n",
    "    if i + '1log' in df_panda.columns: rct_columns.append(i + '1log')\n",
    "\n",
    "number_of_sessions = max(data['sess']).astype(int)\n",
    "number_of_subjects = len(df_panda)\n",
    "weeks = [0, 2, 6, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bdi1log</th>\n",
       "      <th>phq1log</th>\n",
       "      <th>gad1log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bdi1log</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791180</td>\n",
       "      <td>0.664799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phq1log</th>\n",
       "      <td>0.791180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gad1log</th>\n",
       "      <td>0.664799</td>\n",
       "      <td>0.708296</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bdi1log   phq1log   gad1log\n",
       "bdi1log  1.000000  0.791180  0.664799\n",
       "phq1log  0.791180  1.000000  0.708296\n",
       "gad1log  0.664799  0.708296  1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "df_panda[['bdi1log','phq1log', 'gad1log']].corr()"
=======
    "df_panda[['beck1log','phq1log', 'gad1log']].corr()"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "Exclusion\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of excluded subjects at week 0:\t315.0 (0.51%)\n",
      "# of excluded subjects at week 2:\t229.0 (0.43%)\n",
      "# of excluded subjects at week 6:\t201.0 (0.42%)\n",
      "total # of excluded subjects:\t\t745.0 (46%)\n"
     ]
    }
   ],
   "source": [
    "# number of excluded subjects due to not performing the task properly\n",
    "for t in range(1,number_of_sessions+1):\n",
    "    no_data = df_panda['exclusion' + str(t)].isna()\n",
    "    ex1 = df_panda['exclusion' + str(t)] == 1\n",
    "    ex2 = df_panda['acctot' + str(t)] < 0.5\n",
    "#     df_panda.loc[~no_data, 'exclusiontot' + str(t)] = (ex1[~no_data] | ex2[~no_data]).astype(int)\n",
    "    df_panda.loc[~no_data, 'exclusiontot' + str(t)] = (ex1[~no_data]).astype(int)\n",
    "\n",
    "gng_columns.append('exclusiontot')\n",
    "\n",
    "n_subject = []\n",
    "exclusion = []\n",
    "for t in range(number_of_sessions):\n",
    "    exclusion.append(np.nansum(df_panda['exclusiontot' + str(t+1)]))\n",
    "    n_subject.append(sum(~df_panda['exclusiontot' + str(t+1)].isna()))\n",
    "    print('# of excluded subjects at week ' + str(weeks[t]) + ':\\t' + str(exclusion[t]) + \\\n",
    "      ' (' + str(round(exclusion[t]/n_subject[t],2)) +'%)')\n",
    "print('total # of excluded subjects:\\t\\t' + str(sum(exclusion)) + \\\n",
    "      ' (' + str(round(sum(exclusion)/(sum(n_subject))*100)) +'%)')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>133</td>\n",
       "      <td>166</td>\n",
       "      <td>6.818</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>129</td>\n",
       "      <td>100</td>\n",
       "      <td>6.818</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variable  sertraline  placebo     X2      p\n",
       "0.0  exclusiontot         133      166  6.818  0.009\n",
       "1.0  exclusiontot         129      100  6.818  0.009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 528, sertraline (N=324): 49.24%, placebo (N=329): 37.59%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>137</td>\n",
       "      <td>144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>98</td>\n",
       "      <td>103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variable  sertraline  placebo   X2    p\n",
       "0.0  exclusiontot         137      144  0.0  1.0\n",
       "1.0  exclusiontot          98      103  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 482, sertraline (N=324): 41.7%, placebo (N=329): 41.7%\n"
     ]
    }
   ],
   "source": [
    "# chi2 pearson test to compare included vs excluded in sertraline vs placebo\n",
    "for t in range(2,number_of_sessions+1):\n",
    "    tmp = panda.chi2_test(df_panda['exclusiontot' + str(t)], df_panda['group'], ['sertraline', 'placebo'])\n",
    "    tmp.insert(0, 'variable', 'exclusiontot')\n",
    "    display(tmp)\n",
    "    print('N = ' + str(sum(~df_panda['exclusiontot' + str(t)].isna() & ~df_panda['group'].isna())) + \\\n",
    "          ', sertraline (N=' + str(sum(df_panda['group']==1)) + \\\n",
    "          '): ' + str(np.round(df_panda['exclusiontot' + str(t)][df_panda['group']==1].mean()*100, 2)) + '%'\\\n",
    "          ', placebo (N=' + str(sum(df_panda['group']==0)) + \\\n",
    "          '): ' + str(np.round(df_panda['exclusiontot' + str(t)][df_panda['group']==0].mean()*100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included task runs: 885\n",
      "434 (66% of those randomised)\n"
     ]
=======
     "data": {
      "text/plain": [
       "0.5633187772925764"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
    }
   ],
   "source": [
    "included = df_panda[['exclusiontot1', 'exclusiontot2', 'exclusiontot3']] == 0\n",
    "print('included task runs: ' + str(included.values.sum()))\n",
    "print(str((included.sum(axis=1) > 0).sum()) + ' ('+ str(round((included.sum(axis=1) > 0).sum()/ len(df_panda) * 100)) +  '% of those randomised)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>0.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>133</td>\n",
       "      <td>166</td>\n",
       "      <td>6.818</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>129</td>\n",
       "      <td>100</td>\n",
       "      <td>6.818</td>\n",
       "      <td>0.009</td>\n",
=======
       "      <th>both_bad</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>97</td>\n",
       "      <td>76</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both_good</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>94</td>\n",
       "      <td>119</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "         variable  sertraline  placebo     X2      p\n",
       "0.0  exclusiontot         133      166  6.818  0.009\n",
       "1.0  exclusiontot         129      100  6.818  0.009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 528, sertraline (N=324): 49.24%, placebo (N=329): 37.59%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>137</td>\n",
       "      <td>144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>exclusiontot</td>\n",
       "      <td>98</td>\n",
       "      <td>103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variable  sertraline  placebo   X2    p\n",
       "0.0  exclusiontot         137      144  0.0  1.0\n",
       "1.0  exclusiontot          98      103  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 482, sertraline (N=324): 41.7%, placebo (N=329): 41.7%\n"
     ]
    }
   ],
   "source": [
    "# chi2 pearson test to compare included vs excluded in sertraline vs placebo\n",
    "for t in range(2,number_of_sessions+1):\n",
    "    tmp = panda.chi2_test(df_panda['exclusiontot' + str(t)], df_panda['group'], ['sertraline', 'placebo'])\n",
    "    tmp.insert(0, 'variable', 'exclusiontot')\n",
    "    display(tmp)\n",
    "    print('N = ' + str(sum(~df_panda['exclusiontot' + str(t)].isna() & ~df_panda['group'].isna())) + \\\n",
    "          ', sertraline (N=' + str(sum(df_panda['group']==1)) + \\\n",
    "          '): ' + str(np.round(df_panda['exclusiontot' + str(t)][df_panda['group']==1].mean()*100, 2)) + '%'\\\n",
    "          ', placebo (N=' + str(sum(df_panda['group']==0)) + \\\n",
    "          '): ' + str(np.round(df_panda['exclusiontot' + str(t)][df_panda['group']==0].mean()*100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56% of excluded patients are in the sertraline group at week 2\n"
     ]
    }
   ],
   "source": [
    "print(str(round(sum(df_panda['group'][df_panda['exclusiontot2']==1])/sum(df_panda['exclusiontot2']==1)*100))\\\n",
    "      + '% of excluded patients are in the sertraline group at week 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>X2</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>both_bad</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>97</td>\n",
       "      <td>76</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both_good</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>94</td>\n",
       "      <td>119</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>exclusion</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>7.994</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
=======
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "            variable  sertraline  placebo     X2      p\n",
       "both_bad   exclusion          97       76  7.994  0.046\n",
       "both_good  exclusion          94      119  7.994  0.046\n",
       "better     exclusion          38       47  7.994  0.046\n",
       "worse      exclusion          30       21  7.994  0.046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chi2 pearson test to compare categories of how exclusion changes in sertraline vs placebo\n",
    "tmp = {'both_bad': [1,1], 'both_good': [0,0], 'worse': [0,1], 'better': [1,0]}\n",
    "for i in tmp.keys():\n",
    "    idx = (df_panda['exclusiontot1']==tmp[i][0]) & (df_panda['exclusiontot2']==tmp[i][1])\n",
    "    df_panda.loc[idx, 'exclusion_overall'] = np.tile(i, sum(idx))\n",
    "\n",
    "tmp = panda.chi2_test(df_panda['exclusion_overall'], df_panda['group'], ['sertraline', 'placebo'])\n",
    "tmp.insert(0, 'variable', 'exclusion')\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline variable</th>\n",
       "      <th>estimate</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gad1log</td>\n",
<<<<<<< HEAD
       "      <td>-0.136</td>\n",
       "      <td>0.652</td>\n",
=======
       "      <td>0.943</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.652</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phq1log</td>\n",
<<<<<<< HEAD
       "      <td>-0.309</td>\n",
       "      <td>0.355</td>\n",
=======
       "      <td>1.047</td>\n",
       "      <td>1.065</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>0.355</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bdi1log</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age</td>\n",
<<<<<<< HEAD
       "      <td>0.045</td>\n",
=======
       "      <td>43.756</td>\n",
       "      <td>34.669</td>\n",
       "      <td>8.054</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cisscore</td>\n",
<<<<<<< HEAD
       "      <td>-0.021</td>\n",
       "      <td>0.201</td>\n",
=======
       "      <td>10.239</td>\n",
       "      <td>10.741</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>0.201</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>group</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>site</td>\n",
<<<<<<< HEAD
       "      <td>0.088</td>\n",
       "      <td>0.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cis</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dep</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
=======
       "      <td>124.000</td>\n",
       "      <td>133.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570</td>\n",
       "      <td>2.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>site</td>\n",
       "      <td>71.000</td>\n",
       "      <td>56.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570</td>\n",
       "      <td>2.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>site</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570</td>\n",
       "      <td>2.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>site</td>\n",
       "      <td>58.000</td>\n",
       "      <td>55.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570</td>\n",
       "      <td>2.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>166.000</td>\n",
       "      <td>168.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>90.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>58.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>219.000</td>\n",
       "      <td>205.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>96.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "      <td>education</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.000</td>\n",
<<<<<<< HEAD
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fin</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>empstat</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>marstat</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.460</td>\n",
=======
       "      <td>26.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>education</td>\n",
       "      <td>88.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>education</td>\n",
       "      <td>27.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>208.000</td>\n",
       "      <td>161.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>11.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>106.000</td>\n",
       "      <td>144.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>11.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>193.000</td>\n",
       "      <td>173.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285</td>\n",
       "      <td>1.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>122.000</td>\n",
       "      <td>132.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.285</td>\n",
       "      <td>1.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>273.000</td>\n",
       "      <td>284.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013</td>\n",
       "      <td>14.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>16.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013</td>\n",
       "      <td>14.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>14.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013</td>\n",
       "      <td>14.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>5.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013</td>\n",
       "      <td>14.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013</td>\n",
       "      <td>14.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013</td>\n",
       "      <td>14.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>181.000</td>\n",
       "      <td>172.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>98.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>35.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>199.000</td>\n",
       "      <td>215.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073</td>\n",
       "      <td>3.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>115.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073</td>\n",
       "      <td>3.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>140.000</td>\n",
       "      <td>105.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>118.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>56.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.724</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline variable</th>\n",
       "      <th>estimate</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gad1log</td>\n",
<<<<<<< HEAD
       "      <td>-0.261</td>\n",
       "      <td>0.421</td>\n",
=======
       "      <td>0.922</td>\n",
       "      <td>0.942</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.427</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phq1log</td>\n",
<<<<<<< HEAD
       "      <td>-0.305</td>\n",
       "      <td>0.406</td>\n",
=======
       "      <td>1.037</td>\n",
       "      <td>1.055</td>\n",
       "      <td>-0.827</td>\n",
       "      <td>0.409</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bdi1log</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age</td>\n",
<<<<<<< HEAD
       "      <td>0.058</td>\n",
=======
       "      <td>46.459</td>\n",
       "      <td>34.839</td>\n",
       "      <td>9.508</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cisscore</td>\n",
<<<<<<< HEAD
       "      <td>-0.025</td>\n",
       "      <td>0.160</td>\n",
=======
       "      <td>9.943</td>\n",
       "      <td>10.547</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>0.162</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>group</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>site</td>\n",
<<<<<<< HEAD
       "      <td>0.012</td>\n",
       "      <td>0.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cis</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dep</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
=======
       "      <td>97.000</td>\n",
       "      <td>127.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213</td>\n",
       "      <td>4.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>site</td>\n",
       "      <td>47.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213</td>\n",
       "      <td>4.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>site</td>\n",
       "      <td>43.000</td>\n",
       "      <td>44.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213</td>\n",
       "      <td>4.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>site</td>\n",
       "      <td>42.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213</td>\n",
       "      <td>4.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>109.000</td>\n",
       "      <td>166.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181</td>\n",
       "      <td>3.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>67.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181</td>\n",
       "      <td>3.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>53.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181</td>\n",
       "      <td>3.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>155.000</td>\n",
       "      <td>206.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>74.000</td>\n",
       "      <td>93.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "      <td>education</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.000</td>\n",
<<<<<<< HEAD
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
=======
       "      <td>31.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>education</td>\n",
       "      <td>77.000</td>\n",
       "      <td>58.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>31.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>education</td>\n",
       "      <td>18.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>31.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "      <td>AD_past</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.001</td>\n",
<<<<<<< HEAD
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fin</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>empstat</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>marstat</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.881</td>\n",
=======
       "      <td>10.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>69.000</td>\n",
       "      <td>133.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>142.000</td>\n",
       "      <td>165.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.137</td>\n",
       "      <td>2.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>87.000</td>\n",
       "      <td>134.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.137</td>\n",
       "      <td>2.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>209.000</td>\n",
       "      <td>275.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "      <td>6.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>9.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "      <td>6.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>6.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "      <td>6.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>3.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "      <td>6.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "      <td>6.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "      <td>6.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>135.000</td>\n",
       "      <td>180.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>66.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>28.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>152.000</td>\n",
       "      <td>205.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>77.000</td>\n",
       "      <td>93.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>108.000</td>\n",
       "      <td>107.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>72.000</td>\n",
       "      <td>158.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>49.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.707</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline variable</th>\n",
       "      <th>estimate</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gad1log</td>\n",
<<<<<<< HEAD
       "      <td>-1.008</td>\n",
       "      <td>0.004</td>\n",
=======
       "      <td>0.890</td>\n",
       "      <td>0.962</td>\n",
       "      <td>-2.843</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phq1log</td>\n",
<<<<<<< HEAD
       "      <td>-0.637</td>\n",
       "      <td>0.100</td>\n",
=======
       "      <td>1.026</td>\n",
       "      <td>1.063</td>\n",
       "      <td>-1.651</td>\n",
       "      <td>0.099</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bdi1log</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age</td>\n",
<<<<<<< HEAD
       "      <td>0.057</td>\n",
=======
       "      <td>46.473</td>\n",
       "      <td>35.028</td>\n",
       "      <td>8.878</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cisscore</td>\n",
<<<<<<< HEAD
       "      <td>-0.029</td>\n",
       "      <td>0.136</td>\n",
=======
       "      <td>9.940</td>\n",
       "      <td>10.601</td>\n",
       "      <td>-1.487</td>\n",
       "      <td>0.138</td>\n",
       "      <td>NaN</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>group</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>site</td>\n",
<<<<<<< HEAD
       "      <td>0.077</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cis</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dep</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>education</td>\n",
       "      <td>1.163</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fin</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>empstat</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>marstat</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.413</td>\n",
=======
       "      <td>85.000</td>\n",
       "      <td>126.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482</td>\n",
       "      <td>2.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>site</td>\n",
       "      <td>44.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482</td>\n",
       "      <td>2.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>site</td>\n",
       "      <td>38.000</td>\n",
       "      <td>52.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482</td>\n",
       "      <td>2.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>site</td>\n",
       "      <td>34.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482</td>\n",
       "      <td>2.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>87.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "      <td>10.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>65.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "      <td>10.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>48.000</td>\n",
       "      <td>51.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006</td>\n",
       "      <td>10.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>143.000</td>\n",
       "      <td>192.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>58.000</td>\n",
       "      <td>89.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>education</td>\n",
       "      <td>110.000</td>\n",
       "      <td>227.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>education</td>\n",
       "      <td>72.000</td>\n",
       "      <td>52.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>education</td>\n",
       "      <td>18.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>141.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>59.000</td>\n",
       "      <td>132.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>118.000</td>\n",
       "      <td>161.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>83.000</td>\n",
       "      <td>120.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>186.000</td>\n",
       "      <td>260.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490</td>\n",
       "      <td>4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>5.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490</td>\n",
       "      <td>4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>3.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490</td>\n",
       "      <td>4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490</td>\n",
       "      <td>4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490</td>\n",
       "      <td>4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490</td>\n",
       "      <td>4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>123.000</td>\n",
       "      <td>161.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>56.000</td>\n",
       "      <td>85.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>21.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>121.000</td>\n",
       "      <td>203.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "      <td>6.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>79.000</td>\n",
       "      <td>78.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009</td>\n",
       "      <td>6.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>89.000</td>\n",
       "      <td>109.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>66.000</td>\n",
       "      <td>140.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>45.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.658</td>\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple comparisons: 0.001\n"
     ]
    }
   ],
   "source": [
    "df_panda['ethnic'] = df_panda['ethnic'].replace([2, 3, 4, 5, 6], 2)\n",
    "baseline_continuous =  ['gad1log', 'phq1log', 'bdi1log', 'age', 'cisscore']\n",
    "baseline_categorical = ['group','site', 'cis', 'dep', 'education', \\\n",
    "                        'AD_past','sex', 'ethnic', 'fin', 'empstat', 'marstat']\n",
    "tab_list = []\n",
<<<<<<< HEAD
=======
    "for t in range(1,number_of_sessions+1):\n",
    "    tab = pd.DataFrame()\n",
    "    for i in baseline_continuous:\n",
    "        tmp = panda.group_ttest(df_panda[i], df_panda['exclusiontot' + str(t)], ['excluded', 'included'])\n",
    "        tmp.insert(0, 'variable', i)\n",
    "        tab = pd.concat((tab, tmp))\n",
    "    for i in baseline_categorical:\n",
    "        tmp = panda.chi2_test(df_panda[i], df_panda['exclusiontot'  + str(t)], ['excluded', 'included'])\n",
    "        tmp.insert(0, 'variable', i)\n",
    "        tab = pd.concat((tab, tmp))\n",
    "    tab_list.append(tab)\n",
    "disp.display_side_by_side(tab_list[0],tab_list[1],tab_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['gad1log','phq1log','age','cisscore','site','cis','dep','education','AD_past','sex','ethnic','fin','empstat','marstat']\n",
    "for col in cols:\n",
    "    col_zscore = col + '_zscore'\n",
    "    df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.641843\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot1</td>  <th>  No. Observations:  </th>  <td>   619</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   616</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 08 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.07387</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:39:53</td>     <th>  Log-Likelihood:    </th> <td> -397.30</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -428.99</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.724e-14</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -2.3915</td> <td>    0.461</td> <td>   -5.185</td> <td> 0.000</td> <td>   -3.295</td> <td>   -1.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log</th>   <td>    0.5937</td> <td>    0.332</td> <td>    1.788</td> <td> 0.074</td> <td>   -0.057</td> <td>    1.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0476</td> <td>    0.006</td> <td>    7.529</td> <td> 0.000</td> <td>    0.035</td> <td>    0.060</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot1   No. Observations:                  619\n",
       "Model:                          Logit   Df Residuals:                      616\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 08 Jul 2022   Pseudo R-squ.:                 0.07387\n",
       "Time:                        16:39:53   Log-Likelihood:                -397.30\n",
       "converged:                       True   LL-Null:                       -428.99\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.724e-14\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -2.3915      0.461     -5.185      0.000      -3.295      -1.487\n",
       "gad1log        0.5937      0.332      1.788      0.074      -0.057       1.245\n",
       "age            0.0476      0.006      7.529      0.000       0.035       0.060\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.601060\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot2</td>  <th>  No. Observations:  </th>  <td>   528</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   525</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 08 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1217</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:39:53</td>     <th>  Log-Likelihood:    </th> <td> -317.36</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -361.33</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.034e-20</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -3.5653</td> <td>    0.540</td> <td>   -6.599</td> <td> 0.000</td> <td>   -4.624</td> <td>   -2.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log</th>   <td>    0.7961</td> <td>    0.373</td> <td>    2.135</td> <td> 0.033</td> <td>    0.065</td> <td>    1.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0631</td> <td>    0.007</td> <td>    8.572</td> <td> 0.000</td> <td>    0.049</td> <td>    0.078</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot2   No. Observations:                  528\n",
       "Model:                          Logit   Df Residuals:                      525\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 08 Jul 2022   Pseudo R-squ.:                  0.1217\n",
       "Time:                        16:39:53   Log-Likelihood:                -317.36\n",
       "converged:                       True   LL-Null:                       -361.33\n",
       "Covariance Type:            nonrobust   LLR p-value:                 8.034e-20\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -3.5653      0.540     -6.599      0.000      -4.624      -2.506\n",
       "gad1log        0.7961      0.373      2.135      0.033       0.065       1.527\n",
       "age            0.0631      0.007      8.572      0.000       0.049       0.078\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.603501\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot3</td>  <th>  No. Observations:  </th>  <td>   482</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   479</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 08 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1116</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:39:53</td>     <th>  Log-Likelihood:    </th> <td> -290.89</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -327.43</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.352e-16</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -2.3833</td> <td>    0.530</td> <td>   -4.497</td> <td> 0.000</td> <td>   -3.422</td> <td>   -1.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log</th>   <td>   -0.2357</td> <td>    0.384</td> <td>   -0.614</td> <td> 0.539</td> <td>   -0.987</td> <td>    0.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0558</td> <td>    0.007</td> <td>    7.519</td> <td> 0.000</td> <td>    0.041</td> <td>    0.070</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot3   No. Observations:                  482\n",
       "Model:                          Logit   Df Residuals:                      479\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 08 Jul 2022   Pseudo R-squ.:                  0.1116\n",
       "Time:                        16:39:53   Log-Likelihood:                -290.89\n",
       "converged:                       True   LL-Null:                       -327.43\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.352e-16\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -2.3833      0.530     -4.497      0.000      -3.422      -1.344\n",
       "gad1log       -0.2357      0.384     -0.614      0.539      -0.987       0.516\n",
       "age            0.0558      0.007      7.519      0.000       0.041       0.070\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    model = smf.logit('exclusiontot' + str(i) +' ~  gad1log + age', data=df_panda).fit()\n",
    "    display(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.615462\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot1</td>  <th>  No. Observations:  </th>  <td>   617</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   602</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    14</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 08 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1120</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:39:53</td>     <th>  Log-Likelihood:    </th> <td> -379.74</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -427.63</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>3.032e-14</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>    0.0794</td> <td>    0.088</td> <td>    0.900</td> <td> 0.368</td> <td>   -0.094</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log_zscore</th>   <td>    0.0903</td> <td>    0.139</td> <td>    0.652</td> <td> 0.515</td> <td>   -0.181</td> <td>    0.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>phq1log_zscore</th>   <td>    0.0439</td> <td>    0.151</td> <td>    0.291</td> <td> 0.771</td> <td>   -0.252</td> <td>    0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age_zscore</th>       <td>    0.6780</td> <td>    0.104</td> <td>    6.515</td> <td> 0.000</td> <td>    0.474</td> <td>    0.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore_zscore</th>  <td>   -0.3453</td> <td>    0.165</td> <td>   -2.097</td> <td> 0.036</td> <td>   -0.668</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site_zscore</th>      <td>    0.0680</td> <td>    0.091</td> <td>    0.751</td> <td> 0.452</td> <td>   -0.109</td> <td>    0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis_zscore</th>       <td>    0.3574</td> <td>    0.165</td> <td>    2.162</td> <td> 0.031</td> <td>    0.033</td> <td>    0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep_zscore</th>       <td>   -0.0807</td> <td>    0.091</td> <td>   -0.887</td> <td> 0.375</td> <td>   -0.259</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_zscore</th> <td>    0.3574</td> <td>    0.100</td> <td>    3.586</td> <td> 0.000</td> <td>    0.162</td> <td>    0.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past_zscore</th>   <td>    0.1064</td> <td>    0.092</td> <td>    1.153</td> <td> 0.249</td> <td>   -0.074</td> <td>    0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_zscore</th>       <td>    0.1283</td> <td>    0.091</td> <td>    1.415</td> <td> 0.157</td> <td>   -0.049</td> <td>    0.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic_zscore</th>    <td>    0.2089</td> <td>    0.104</td> <td>    2.006</td> <td> 0.045</td> <td>    0.005</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin_zscore</th>       <td>    0.0186</td> <td>    0.093</td> <td>    0.199</td> <td> 0.842</td> <td>   -0.164</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat_zscore</th>   <td>    0.0981</td> <td>    0.093</td> <td>    1.051</td> <td> 0.293</td> <td>   -0.085</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat_zscore</th>   <td>   -0.0436</td> <td>    0.090</td> <td>   -0.482</td> <td> 0.630</td> <td>   -0.221</td> <td>    0.134</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot1   No. Observations:                  617\n",
       "Model:                          Logit   Df Residuals:                      602\n",
       "Method:                           MLE   Df Model:                           14\n",
       "Date:                Fri, 08 Jul 2022   Pseudo R-squ.:                  0.1120\n",
       "Time:                        16:39:53   Log-Likelihood:                -379.74\n",
       "converged:                       True   LL-Null:                       -427.63\n",
       "Covariance Type:            nonrobust   LLR p-value:                 3.032e-14\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept            0.0794      0.088      0.900      0.368      -0.094       0.252\n",
       "gad1log_zscore       0.0903      0.139      0.652      0.515      -0.181       0.362\n",
       "phq1log_zscore       0.0439      0.151      0.291      0.771      -0.252       0.339\n",
       "age_zscore           0.6780      0.104      6.515      0.000       0.474       0.882\n",
       "cisscore_zscore     -0.3453      0.165     -2.097      0.036      -0.668      -0.023\n",
       "site_zscore          0.0680      0.091      0.751      0.452      -0.109       0.245\n",
       "cis_zscore           0.3574      0.165      2.162      0.031       0.033       0.681\n",
       "dep_zscore          -0.0807      0.091     -0.887      0.375      -0.259       0.098\n",
       "education_zscore     0.3574      0.100      3.586      0.000       0.162       0.553\n",
       "AD_past_zscore       0.1064      0.092      1.153      0.249      -0.074       0.287\n",
       "sex_zscore           0.1283      0.091      1.415      0.157      -0.049       0.306\n",
       "ethnic_zscore        0.2089      0.104      2.006      0.045       0.005       0.413\n",
       "fin_zscore           0.0186      0.093      0.199      0.842      -0.164       0.201\n",
       "empstat_zscore       0.0981      0.093      1.051      0.293      -0.085       0.281\n",
       "marstat_zscore      -0.0436      0.090     -0.482      0.630      -0.221       0.134\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.573534\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot2</td>  <th>  No. Observations:  </th>  <td>   526</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   511</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    14</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 08 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1618</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:39:53</td>     <th>  Log-Likelihood:    </th> <td> -301.68</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -359.92</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>3.055e-18</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>   -0.2970</td> <td>    0.101</td> <td>   -2.931</td> <td> 0.003</td> <td>   -0.496</td> <td>   -0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log_zscore</th>   <td>    0.2973</td> <td>    0.161</td> <td>    1.852</td> <td> 0.064</td> <td>   -0.017</td> <td>    0.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>phq1log_zscore</th>   <td>    0.1374</td> <td>    0.178</td> <td>    0.771</td> <td> 0.440</td> <td>   -0.212</td> <td>    0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age_zscore</th>       <td>    0.9548</td> <td>    0.122</td> <td>    7.807</td> <td> 0.000</td> <td>    0.715</td> <td>    1.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore_zscore</th>  <td>   -0.1585</td> <td>    0.190</td> <td>   -0.834</td> <td> 0.404</td> <td>   -0.531</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site_zscore</th>      <td>   -0.0069</td> <td>    0.106</td> <td>   -0.065</td> <td> 0.948</td> <td>   -0.215</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis_zscore</th>       <td>   -0.1462</td> <td>    0.185</td> <td>   -0.789</td> <td> 0.430</td> <td>   -0.509</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep_zscore</th>       <td>   -0.0049</td> <td>    0.103</td> <td>   -0.048</td> <td> 0.962</td> <td>   -0.206</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_zscore</th> <td>    0.4466</td> <td>    0.112</td> <td>    3.981</td> <td> 0.000</td> <td>    0.227</td> <td>    0.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past_zscore</th>   <td>   -0.0103</td> <td>    0.109</td> <td>   -0.095</td> <td> 0.924</td> <td>   -0.223</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_zscore</th>       <td>    0.2419</td> <td>    0.103</td> <td>    2.338</td> <td> 0.019</td> <td>    0.039</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic_zscore</th>    <td>    0.0041</td> <td>    0.120</td> <td>    0.034</td> <td> 0.973</td> <td>   -0.232</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin_zscore</th>       <td>    0.1363</td> <td>    0.106</td> <td>    1.287</td> <td> 0.198</td> <td>   -0.071</td> <td>    0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat_zscore</th>   <td>   -0.0923</td> <td>    0.107</td> <td>   -0.860</td> <td> 0.390</td> <td>   -0.303</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat_zscore</th>   <td>   -0.0063</td> <td>    0.098</td> <td>   -0.064</td> <td> 0.949</td> <td>   -0.198</td> <td>    0.186</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot2   No. Observations:                  526\n",
       "Model:                          Logit   Df Residuals:                      511\n",
       "Method:                           MLE   Df Model:                           14\n",
       "Date:                Fri, 08 Jul 2022   Pseudo R-squ.:                  0.1618\n",
       "Time:                        16:39:53   Log-Likelihood:                -301.68\n",
       "converged:                       True   LL-Null:                       -359.92\n",
       "Covariance Type:            nonrobust   LLR p-value:                 3.055e-18\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept           -0.2970      0.101     -2.931      0.003      -0.496      -0.098\n",
       "gad1log_zscore       0.2973      0.161      1.852      0.064      -0.017       0.612\n",
       "phq1log_zscore       0.1374      0.178      0.771      0.440      -0.212       0.486\n",
       "age_zscore           0.9548      0.122      7.807      0.000       0.715       1.194\n",
       "cisscore_zscore     -0.1585      0.190     -0.834      0.404      -0.531       0.214\n",
       "site_zscore         -0.0069      0.106     -0.065      0.948      -0.215       0.201\n",
       "cis_zscore          -0.1462      0.185     -0.789      0.430      -0.509       0.217\n",
       "dep_zscore          -0.0049      0.103     -0.048      0.962      -0.206       0.197\n",
       "education_zscore     0.4466      0.112      3.981      0.000       0.227       0.667\n",
       "AD_past_zscore      -0.0103      0.109     -0.095      0.924      -0.223       0.203\n",
       "sex_zscore           0.2419      0.103      2.338      0.019       0.039       0.445\n",
       "ethnic_zscore        0.0041      0.120      0.034      0.973      -0.232       0.240\n",
       "fin_zscore           0.1363      0.106      1.287      0.198      -0.071       0.344\n",
       "empstat_zscore      -0.0923      0.107     -0.860      0.390      -0.303       0.118\n",
       "marstat_zscore      -0.0063      0.098     -0.064      0.949      -0.198       0.186\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.557603\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot3</td>  <th>  No. Observations:  </th>  <td>   480</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   465</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    14</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 08 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1782</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:39:53</td>     <th>  Log-Likelihood:    </th> <td> -267.65</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -325.67</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>3.731e-18</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>   -0.3580</td> <td>    0.108</td> <td>   -3.314</td> <td> 0.001</td> <td>   -0.570</td> <td>   -0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log_zscore</th>   <td>   -0.1358</td> <td>    0.168</td> <td>   -0.807</td> <td> 0.420</td> <td>   -0.466</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>phq1log_zscore</th>   <td>    0.2548</td> <td>    0.186</td> <td>    1.371</td> <td> 0.170</td> <td>   -0.109</td> <td>    0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age_zscore</th>       <td>    0.7983</td> <td>    0.124</td> <td>    6.417</td> <td> 0.000</td> <td>    0.554</td> <td>    1.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore_zscore</th>  <td>    0.1314</td> <td>    0.205</td> <td>    0.642</td> <td> 0.521</td> <td>   -0.270</td> <td>    0.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site_zscore</th>      <td>    0.0989</td> <td>    0.117</td> <td>    0.846</td> <td> 0.398</td> <td>   -0.130</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis_zscore</th>       <td>   -0.2588</td> <td>    0.201</td> <td>   -1.291</td> <td> 0.197</td> <td>   -0.652</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep_zscore</th>       <td>   -0.0455</td> <td>    0.111</td> <td>   -0.410</td> <td> 0.682</td> <td>   -0.263</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education_zscore</th> <td>    0.5969</td> <td>    0.120</td> <td>    4.956</td> <td> 0.000</td> <td>    0.361</td> <td>    0.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past_zscore</th>   <td>    0.1100</td> <td>    0.114</td> <td>    0.962</td> <td> 0.336</td> <td>   -0.114</td> <td>    0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_zscore</th>       <td>    0.1229</td> <td>    0.110</td> <td>    1.120</td> <td> 0.263</td> <td>   -0.092</td> <td>    0.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic_zscore</th>    <td>   -0.0209</td> <td>    0.119</td> <td>   -0.176</td> <td> 0.860</td> <td>   -0.253</td> <td>    0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin_zscore</th>       <td>   -0.0440</td> <td>    0.115</td> <td>   -0.383</td> <td> 0.702</td> <td>   -0.269</td> <td>    0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat_zscore</th>   <td>    0.1867</td> <td>    0.111</td> <td>    1.677</td> <td> 0.093</td> <td>   -0.031</td> <td>    0.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat_zscore</th>   <td>    0.0705</td> <td>    0.104</td> <td>    0.681</td> <td> 0.496</td> <td>   -0.133</td> <td>    0.274</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          exclusiontot3   No. Observations:                  480\n",
       "Model:                          Logit   Df Residuals:                      465\n",
       "Method:                           MLE   Df Model:                           14\n",
       "Date:                Fri, 08 Jul 2022   Pseudo R-squ.:                  0.1782\n",
       "Time:                        16:39:53   Log-Likelihood:                -267.65\n",
       "converged:                       True   LL-Null:                       -325.67\n",
       "Covariance Type:            nonrobust   LLR p-value:                 3.731e-18\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept           -0.3580      0.108     -3.314      0.001      -0.570      -0.146\n",
       "gad1log_zscore      -0.1358      0.168     -0.807      0.420      -0.466       0.194\n",
       "phq1log_zscore       0.2548      0.186      1.371      0.170      -0.109       0.619\n",
       "age_zscore           0.7983      0.124      6.417      0.000       0.554       1.042\n",
       "cisscore_zscore      0.1314      0.205      0.642      0.521      -0.270       0.532\n",
       "site_zscore          0.0989      0.117      0.846      0.398      -0.130       0.328\n",
       "cis_zscore          -0.2588      0.201     -1.291      0.197      -0.652       0.134\n",
       "dep_zscore          -0.0455      0.111     -0.410      0.682      -0.263       0.172\n",
       "education_zscore     0.5969      0.120      4.956      0.000       0.361       0.833\n",
       "AD_past_zscore       0.1100      0.114      0.962      0.336      -0.114       0.334\n",
       "sex_zscore           0.1229      0.110      1.120      0.263      -0.092       0.338\n",
       "ethnic_zscore       -0.0209      0.119     -0.176      0.860      -0.253       0.212\n",
       "fin_zscore          -0.0440      0.115     -0.383      0.702      -0.269       0.181\n",
       "empstat_zscore       0.1867      0.111      1.677      0.093      -0.031       0.405\n",
       "marstat_zscore       0.0705      0.104      0.681      0.496      -0.133       0.274\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
    "for i in range(1,4):\n",
    "    tab = []\n",
    "    for j in baseline_continuous + baseline_categorical:\n",
    "        model = smf.logit('exclusiontot' + str(i) +' ~ ' + j, data=df_panda).fit(disp=False);\n",
    "        tab.append([j, round(model.params[1],3), round(model.pvalues[1],3)])\n",
    "    tab_list.append(pd.DataFrame(tab, columns=['baseline variable', 'estimate', 'pvalue']))\n",
    "disp.display_side_by_side(tab_list[0],tab_list[1],tab_list[2])\n",
    "print('multiple comparisons: ' + str(round(0.05/(3*len(baseline_continuous + baseline_categorical)),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline: age, education, AD past, ethnic\\\n",
    "at week 2: age, education, AD past, group\\\n",
    "at week 6: gad, age, cis, education, AD past, empstat\\\n",
    "Control for those variables!"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 15,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "#######################################################################\n",
      "gad:\n",
      "exclusiontot1: \tbeta: -0.01,\tCI: [-0.05,0.03],\tpvalue: 0.6521\n",
      "exclusiontot2: \tbeta: -0.01,\tCI: [-0.05,0.03],\tpvalue: 0.6603\n",
      "exclusiontot3: \tbeta: -0.04,\tCI: [-0.1,0.02],\tpvalue: 0.2039\n",
      "#######################################################################\n",
      "phq:\n",
      "exclusiontot1: \tbeta: -0.02,\tCI: [-0.06,0.02],\tpvalue: 0.3548\n",
      "exclusiontot2: \tbeta: -0.02,\tCI: [-0.06,0.02],\tpvalue: 0.3166\n",
      "exclusiontot3: \tbeta: -0.03,\tCI: [-0.08,0.02],\tpvalue: 0.2483\n",
      "#######################################################################\n",
      "bdi:\n",
      "exclusiontot1: \tbeta: -0.01,\tCI: [-0.05,0.02],\tpvalue: 0.422\n",
      "exclusiontot2: \tbeta: -0.01,\tCI: [-0.05,0.03],\tpvalue: 0.7548\n",
      "exclusiontot3: \tbeta: -0.03,\tCI: [-0.08,0.03],\tpvalue: 0.4008\n"
=======
      "exclusiontot1: \tbeta: -0.01,\tCI: [-0.05,0.03],\tpvalue: 0.6521\n",
      "exclusiontot2: \tbeta: -0.01,\tCI: [-0.05,0.03],\tpvalue: 0.6603\n",
      "exclusiontot3: \tbeta: -0.04,\tCI: [-0.1,0.02],\tpvalue: 0.2039\n",
      "exclusiontot1: \tbeta: -0.02,\tCI: [-0.06,0.02],\tpvalue: 0.3548\n",
      "exclusiontot2: \tbeta: -0.03,\tCI: [-0.07,0.02],\tpvalue: 0.238\n",
      "exclusiontot3: \tbeta: -0.03,\tCI: [-0.09,0.03],\tpvalue: 0.3404\n",
      "exclusiontot1: \tbeta: -0.01,\tCI: [-0.05,0.02],\tpvalue: 0.422\n",
      "exclusiontot2: \tbeta: -0.02,\tCI: [-0.07,0.03],\tpvalue: 0.3658\n",
      "exclusiontot3: \tbeta: -0.02,\tCI: [-0.08,0.05],\tpvalue: 0.5892\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# is exclusion related to psychiatric scores at specific time point?\n",
    "for i in psychiatric_questionnaires:\n",
    "    print('#######################################################################')\n",
    "    print(i + ':')\n",
    "    panda.glm((i + '1log ~ exclusiontot1'), df_panda, ['exclusiontot1'])\n",
    "    for t in range(2,number_of_sessions+1):\n",
    "          panda.glm((i + str(t) + 'log ~ ' + i + '1log + exclusiontot' + str(t)), df_panda, ['exclusiontot' + str(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "--\n",
    "- more patients not doing the task properly in the sertraline group at week 2 (follow-up 1)\n",
    "- patients not doing the task properly were older, not single, and were taking antidepressants in the past over all sessions --> control for age, education, and AD in the past\n",
    "- at baseline exclusion additionally related to ethnicity\n",
    "- at week 6 exclusion additionally related to baseline gad and employment status"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics\n",
    "df_panda['ethnic'] = df_panda['ethnic'].replace([2, 3, 4, 5, 6], 2)\n",
    "tmp = (df_panda['exclusiontot1'] == 0) | (df_panda['exclusiontot2'] == 0) | (df_panda['exclusiontot3'] == 0)\n",
    "group_label = ['placebo', 'sertraline', 'overall']\n",
    "demographic_variables1 = ['age', 'gad1', 'phq1', 'bdi1', 'cisscore']\n",
    "demographic_variables2 = ['site', 'cis', 'dep', 'education', 'AD_past', 'sex', \\\n",
    "                         'ethnic', 'fin', 'empstat', 'marstat']\n",
    "T = np.empty([100,4], dtype=object)\n",
    "T[0,3] = group_label[2] + ' (N = ' + str(len(df_panda[tmp])) + ')'\n",
    "                          \n",
    "for dd, d in enumerate(demographic_variables1):\n",
    "    T[dd+1,0] = d\n",
    "    T[dd+1,3] = str(np.round(df_panda[d][tmp].mean(),2)) + ' (' + str(np.round(df_panda[d][tmp].std(),2)) + ')'\n",
    "    for g in range(2):\n",
    "        T[dd+1,g+1] = str(np.round(df_panda[d][(df_panda['group']==g)&tmp].mean(),2)) + ' (' + \\\n",
    "                    str(np.round(df_panda[d][(df_panda['group']==g)&tmp].std(),2)) + ')'\n",
    "\n",
    "for g in range(2):\n",
    "    T[0,g+1] = group_label[g] + ' (N = ' + str(sum((df_panda['group']==g)&tmp)) + ')'\n",
    "    dd = len(demographic_variables1)+1\n",
    "    for d in demographic_variables2:\n",
    "        for i in range(int(max(df_panda[d][tmp]))):\n",
    "            T[dd,0] = d + str(i)\n",
    "            T[dd,3] = str(sum(df_panda[d][tmp]==i+1)) + ' (' + str(round(sum(df_panda[d][tmp]==i+1)/number_of_subjects*100)) + '%)'\n",
    "            T[dd,g+1] = str(sum((df_panda[d][tmp]==i+1) & (df_panda['group'][tmp]==g))) + ' (' + str(round(sum((df_panda[d][tmp]==i+1) & (df_panda['group'][tmp]==g))/sum((df_panda['group'][tmp]==g))*100)) + '%)'\n",
    "            dd += 1\n",
    "T = T[~(T == None).all(axis=1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print demographics table in latex\n",
    "from tabulate import tabulate\n",
    "# print(tabulate(T, headers='firstrow', tablefmt='latex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>placebo</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>age</td>\n",
       "      <td>36.784</td>\n",
       "      <td>36.032</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.567</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>phq1</td>\n",
       "      <td>11.742</td>\n",
       "      <td>12.466</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>0.188</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>bdi1</td>\n",
       "      <td>24.085</td>\n",
       "      <td>24.190</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.913</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>gad1</td>\n",
       "      <td>9.300</td>\n",
       "      <td>9.602</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>0.542</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>cisscore</td>\n",
       "      <td>10.420</td>\n",
       "      <td>10.995</td>\n",
       "      <td>-1.244</td>\n",
       "      <td>0.214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>site</td>\n",
       "      <td>92.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>site</td>\n",
       "      <td>46.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>site</td>\n",
       "      <td>38.000</td>\n",
       "      <td>36.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>site</td>\n",
       "      <td>37.000</td>\n",
       "      <td>39.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>117.000</td>\n",
       "      <td>126.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.548</td>\n",
       "      <td>1.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>51.000</td>\n",
       "      <td>58.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.548</td>\n",
       "      <td>1.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>cis</td>\n",
       "      <td>44.000</td>\n",
       "      <td>37.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.548</td>\n",
       "      <td>1.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>147.000</td>\n",
       "      <td>146.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>dep</td>\n",
       "      <td>66.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>education</td>\n",
       "      <td>160.000</td>\n",
       "      <td>165.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>education</td>\n",
       "      <td>49.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>education</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>113.000</td>\n",
       "      <td>124.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>AD_past</td>\n",
       "      <td>99.000</td>\n",
       "      <td>97.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>129.000</td>\n",
       "      <td>120.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>sex</td>\n",
       "      <td>84.000</td>\n",
       "      <td>101.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>197.000</td>\n",
       "      <td>205.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>15.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>121.000</td>\n",
       "      <td>130.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>65.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>fin</td>\n",
       "      <td>26.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>146.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>empstat</td>\n",
       "      <td>66.000</td>\n",
       "      <td>62.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>107.000</td>\n",
       "      <td>110.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497</td>\n",
       "      <td>1.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>74.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497</td>\n",
       "      <td>1.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>marstat</td>\n",
       "      <td>31.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497</td>\n",
       "      <td>1.398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      variable  sertraline  placebo      t      p     X2\n",
       "0.0        age      36.784   36.032  0.574  0.567    NaN\n",
       "0.0       phq1      11.742   12.466 -1.320  0.188    NaN\n",
       "0.0       bdi1      24.085   24.190 -0.109  0.913    NaN\n",
       "0.0       gad1       9.300    9.602 -0.610  0.542    NaN\n",
       "0.0   cisscore      10.420   10.995 -1.244  0.214    NaN\n",
       "1.0       site      92.000   98.000    NaN  0.979  0.191\n",
       "3.0       site      46.000   48.000    NaN  0.979  0.191\n",
       "4.0       site      38.000   36.000    NaN  0.979  0.191\n",
       "2.0       site      37.000   39.000    NaN  0.979  0.191\n",
       "3.0        cis     117.000  126.000    NaN  0.548  1.201\n",
       "2.0        cis      51.000   58.000    NaN  0.548  1.201\n",
       "1.0        cis      44.000   37.000    NaN  0.548  1.201\n",
       "1.0        dep     147.000  146.000    NaN  0.580  0.307\n",
       "2.0        dep      66.000   75.000    NaN  0.580  0.307\n",
       "1.0  education     160.000  165.000    NaN  0.977  0.047\n",
       "2.0  education      49.000   53.000    NaN  0.977  0.047\n",
       "3.0  education       3.000    3.000    NaN  0.977  0.047\n",
       "2.0    AD_past     113.000  124.000    NaN  0.624  0.240\n",
       "1.0    AD_past      99.000   97.000    NaN  0.624  0.240\n",
       "2.0        sex     129.000  120.000    NaN  0.222  1.494\n",
       "1.0        sex      84.000  101.000    NaN  0.222  1.494\n",
       "1.0     ethnic     197.000  205.000    NaN  1.000  0.000\n",
       "2.0     ethnic      15.000   16.000    NaN  1.000  0.000\n",
       "1.0        fin     121.000  130.000    NaN  0.922  0.163\n",
       "2.0        fin      65.000   66.000    NaN  0.922  0.163\n",
       "3.0        fin      26.000   25.000    NaN  0.922  0.163\n",
       "1.0    empstat     146.000  159.000    NaN  0.551  0.356\n",
       "2.0    empstat      66.000   62.000    NaN  0.551  0.356\n",
       "2.0    marstat     107.000  110.000    NaN  0.497  1.398\n",
       "1.0    marstat      74.000   86.000    NaN  0.497  1.398\n",
       "3.0    marstat      31.000   25.000    NaN  0.497  1.398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# is exclusion related to baseline variables?\n",
    "ex = (df_panda['exclusiontot1'] == 0) | (df_panda['exclusiontot2'] == 0) | (df_panda['exclusiontot3'] == 0)\n",
    "demographic_variables1 = ['age', 'phq1', 'bdi1', 'gad1', 'cisscore']\n",
    "demographic_variables2 = ['site', 'cis', 'dep', 'education', 'AD_past', 'sex', \\\n",
    "                         'ethnic', 'fin', 'empstat', 'marstat']\n",
    "\n",
    "tab = pd.DataFrame()\n",
    "for i in demographic_variables1:\n",
    "    tmp = panda.group_ttest(df_panda[i][ex], df_panda['group'][ex], ['sertraline', 'placebo'])\n",
    "    tmp.insert(0, 'variable', i)\n",
    "    tab = pd.concat((tab, tmp))\n",
    "for i in demographic_variables2:\n",
    "    tmp = panda.chi2_test(df_panda[i][ex], df_panda['group'][ex], ['sertraline', 'placebo'])\n",
    "    tmp.insert(0, 'variable', i)\n",
    "    tab = pd.concat((tab, tmp))\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here only included task runs:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
=======
   "execution_count": 16,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>go to win</th>\n",
       "      <th>go to avoid</th>\n",
       "      <th>nogo to win</th>\n",
       "      <th>nogo to avoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.780.26</td>\n",
       "      <td>0.520.24</td>\n",
       "      <td>0.430.33</td>\n",
       "      <td>0.730.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T=2</th>\n",
       "      <td>0.830.24</td>\n",
       "      <td>0.590.25</td>\n",
       "      <td>0.440.34</td>\n",
       "      <td>0.740.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T=6</th>\n",
       "      <td>0.820.27</td>\n",
       "      <td>0.610.27</td>\n",
       "      <td>0.470.36</td>\n",
       "      <td>0.750.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          go to win go to avoid nogo to win nogo to avoid\n",
       "baseline  0.780.26   0.520.24   0.430.33     0.730.17\n",
       "T=2       0.830.24   0.590.25   0.440.34     0.740.16\n",
       "T=6       0.820.27   0.610.27   0.470.36     0.750.17"
      ]
     },
<<<<<<< HEAD
     "execution_count": 28,
=======
     "execution_count": 16,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average performance\n",
    "tablist = []\n",
    "for i in range(1,4):\n",
    "    tab = []\n",
    "    x = [x + str(i) for x in gng_columns if 'acc_' in x]\n",
    "    for j in x:\n",
    "        tab.append(str(np.round(df_panda[j][df_panda['exclusiontot'+str(i)]==0].mean(),2)) \\\n",
    "                   + '' +  str(np.round(df_panda[j][df_panda['exclusiontot'+str(i)]==0].std(),2)))\n",
    "    tablist.append(tab)\n",
    "pd.DataFrame(tablist, columns=['go to win', 'go to avoid', 'nogo to win', 'nogo to avoid'], index=['baseline','T=2','T=6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline psychiatric score related to task?\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 17,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################################\n",
      "gad:\n",
      "acc_g2w1: \tbeta: -0.02,\tCI: [-0.1,0.06],\tpvalue: 0.6504\n",
      "acc_g2a1: \tbeta: -0.01,\tCI: [-0.1,0.08],\tpvalue: 0.8473\n",
      "acc_ng2w1: \tbeta: 0.0,\tCI: [-0.06,0.07],\tpvalue: 0.9096\n",
      "acc_ng2a1: \tbeta: -0.07,\tCI: [-0.19,0.05],\tpvalue: 0.2814\n",
<<<<<<< HEAD
      "#######################################################################\n",
=======
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
      "phq:\n",
      "acc_g2w1: \tbeta: -0.02,\tCI: [-0.09,0.06],\tpvalue: 0.6342\n",
      "acc_g2a1: \tbeta: 0.02,\tCI: [-0.06,0.1],\tpvalue: 0.5954\n",
      "acc_ng2w1: \tbeta: 0.02,\tCI: [-0.04,0.08],\tpvalue: 0.4497\n",
      "acc_ng2a1: \tbeta: 0.0,\tCI: [-0.11,0.11],\tpvalue: 0.9898\n",
<<<<<<< HEAD
      "#######################################################################\n",
      "bdi:\n",
=======
      "beck:\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
      "acc_g2w1: \tbeta: -0.06,\tCI: [-0.13,0.01],\tpvalue: 0.0877\n",
      "acc_g2a1: \tbeta: -0.01,\tCI: [-0.08,0.07],\tpvalue: 0.8857\n",
      "acc_ng2w1: \tbeta: -0.01,\tCI: [-0.06,0.04],\tpvalue: 0.7656\n",
      "acc_ng2a1: \tbeta: 0.04,\tCI: [-0.06,0.14],\tpvalue: 0.4428\n"
     ]
    }
   ],
   "source": [
    "# baseline performance related to baseline psychiatric score\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('#######################################################################')\n",
    "    print(j + ':')\n",
    "    x = [x for x in gng_columns if 'acc_' in x]\n",
    "    for i in x:\n",
<<<<<<< HEAD
    "        df_panda[i+'1'] = pd.to_numeric(df_panda[i+'1'])\n",
    "        panda.glm(j + '1log ~ '+i+'1 + sex + age + cis + dep + site + education + marstat + AD_past + fin', df_panda[df_panda['exclusiontot1']==0], [i+'1'])"
=======
    "        panda.glm(j + '1log ~ '+i+'1 + sex + age + cis + dep + site + education + marstat + AD_past + fin', df_panda[df_panda['exclusiontot1']==0], [i+'1'])\n",
    "        "
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 18,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################################\n",
      "gad:\n",
      "rew_se1: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.4519\n",
      "loss_se1: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.7327\n",
      "rew_LR1: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9619\n",
      "loss_LR1: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.2693\n",
      "app_Pav1: \tbeta: -0.0,\tCI: [-0.04,0.03],\tpvalue: 0.8954\n",
      "av_Pav1: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.8118\n",
      "noise1: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9917\n",
      "bias1: \tbeta: 0.0,\tCI: [-0.02,0.03],\tpvalue: 0.7699\n",
<<<<<<< HEAD
      "#######################################################################\n",
=======
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
      "phq:\n",
      "rew_se1: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.7888\n",
      "loss_se1: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.6323\n",
      "rew_LR1: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.364\n",
      "loss_LR1: \tbeta: 0.01,\tCI: [-0.0,0.02],\tpvalue: 0.2677\n",
      "app_Pav1: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.495\n",
      "av_Pav1: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.4749\n",
      "noise1: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.7675\n",
      "bias1: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.5444\n",
<<<<<<< HEAD
      "#######################################################################\n",
      "bdi:\n",
=======
      "beck:\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
      "rew_se1: \tbeta: 0.01,\tCI: [-0.0,0.03],\tpvalue: 0.1099\n",
      "loss_se1: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9024\n",
      "rew_LR1: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.2063\n",
      "loss_LR1: \tbeta: 0.01,\tCI: [-0.0,0.02],\tpvalue: 0.2265\n",
      "app_Pav1: \tbeta: 0.01,\tCI: [-0.03,0.04],\tpvalue: 0.7444\n",
      "av_Pav1: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.2138\n",
      "noise1: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.5393\n",
<<<<<<< HEAD
      "bias1: \tbeta: -0.02,\tCI: [-0.04,-0.0],\tpvalue: 0.0368\n",
      "\n",
      "multiple comparison: 0.002\n"
=======
      "bias1: \tbeta: -0.02,\tCI: [-0.04,-0.0],\tpvalue: 0.0368\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# baseline psychiatric scores related to baseline cognitive parameters:\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('#######################################################################')\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        df_panda[i+'1'] = pd.to_numeric(df_panda[i+'1'])\n",
    "        panda.glm(j + '1log ~ '+i+'1 + sex + age + cis + dep + site + education + marstat + AD_past + fin', df_panda[df_panda['exclusiontot1']==0], [i+'1'])\n",
    "\n",
    "print('\\nmultiple comparison: ' + str(round(0.05/(len(parameter_labels)*len(psychiatric_questionnaires)),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS:\n",
    "---\n",
    "Trend indicating the higher BDI score the lower go bias (only included task runs!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "MIXED EFFECTS MODELLING\n",
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 19,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for mixed-effects modelling\n",
    "mle_df = pd.DataFrame(columns = gng_columns + rct_columns + ['gad1', 'phq1', 'bdi1'])\n",
    "for c in gng_columns:\n",
    "    mle_df[c] = pd.to_numeric(pd.concat([df_panda[c + '1'], df_panda[c + '2'], df_panda[c + '3']], axis=0, ignore_index=True))\n",
    "    \n",
    "for c in rct_columns:\n",
    "    mle_df[c] = pd.concat([df_panda[c], df_panda[c], df_panda[c]], axis=0, ignore_index=True)\n",
    "\n",
    "mle_df['subject'] = np.hstack((np.arange(len(df_panda)),np.arange(len(df_panda)),np.arange(len(df_panda))))\n",
    "mle_df['time'] = np.hstack((np.tile(1, len(df_panda)),np.tile(2, len(df_panda)),np.tile(3, len(df_panda))))\n",
    "mle_df['weeks'] = np.hstack((np.tile(weeks[0], len(df_panda)),np.tile(weeks[1], len(df_panda)),np.tile(weeks[2], len(df_panda))))\n",
    "mle_df['group'] = np.hstack((np.tile(0, len(df_panda)),df_panda['group'],df_panda['group']))\n",
    "for i in psychiatric_questionnaires:\n",
    "    mle_df[i + 'log'] = np.hstack((df_panda[i + '1log'],df_panda[i + '2log'],df_panda[i + '3log']))\n",
<<<<<<< HEAD
    "mle_df['anhedonia'] = np.hstack((df_panda['phq1_1'],df_panda['phq1_2'],df_panda['phq1_3']))"
=======
    "mle_df['anhedonia'] = np.hstack((df_panda['phq1_1'],df_panda['phq1_2'],df_panda['phq1_3']))\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.597917\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>exclusiontot</td>   <th>  No. Observations:  </th>  <td>  1623</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1608</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    14</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 08 Jul 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.1324</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:39:55</td>     <th>  Log-Likelihood:    </th> <td> -970.42</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1118.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>7.523e-55</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -5.0483</td> <td>    0.520</td> <td>   -9.708</td> <td> 0.000</td> <td>   -6.067</td> <td>   -4.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gad1log</th>   <td>    0.3210</td> <td>    0.327</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.321</td> <td>    0.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>phq1log</th>   <td>    0.5102</td> <td>    0.398</td> <td>    1.283</td> <td> 0.199</td> <td>   -0.269</td> <td>    1.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.0518</td> <td>    0.004</td> <td>   11.827</td> <td> 0.000</td> <td>    0.043</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cisscore</th>  <td>   -0.0303</td> <td>    0.021</td> <td>   -1.423</td> <td> 0.155</td> <td>   -0.072</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site</th>      <td>    0.0546</td> <td>    0.049</td> <td>    1.114</td> <td> 0.265</td> <td>   -0.041</td> <td>    0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis</th>       <td>    0.0468</td> <td>    0.131</td> <td>    0.357</td> <td> 0.721</td> <td>   -0.210</td> <td>    0.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep</th>       <td>   -0.1035</td> <td>    0.122</td> <td>   -0.848</td> <td> 0.396</td> <td>   -0.343</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th> <td>    0.7781</td> <td>    0.109</td> <td>    7.153</td> <td> 0.000</td> <td>    0.565</td> <td>    0.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past</th>   <td>    0.1497</td> <td>    0.120</td> <td>    1.245</td> <td> 0.213</td> <td>   -0.086</td> <td>    0.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>       <td>    0.3181</td> <td>    0.116</td> <td>    2.745</td> <td> 0.006</td> <td>    0.091</td> <td>    0.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic</th>    <td>    0.1069</td> <td>    0.075</td> <td>    1.435</td> <td> 0.151</td> <td>   -0.039</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fin</th>       <td>    0.0612</td> <td>    0.083</td> <td>    0.737</td> <td> 0.461</td> <td>   -0.101</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>empstat</th>   <td>    0.1311</td> <td>    0.124</td> <td>    1.058</td> <td> 0.290</td> <td>   -0.112</td> <td>    0.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat</th>   <td>    0.0049</td> <td>    0.079</td> <td>    0.062</td> <td> 0.951</td> <td>   -0.150</td> <td>    0.160</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:           exclusiontot   No. Observations:                 1623\n",
       "Model:                          Logit   Df Residuals:                     1608\n",
       "Method:                           MLE   Df Model:                           14\n",
       "Date:                Fri, 08 Jul 2022   Pseudo R-squ.:                  0.1324\n",
       "Time:                        16:39:55   Log-Likelihood:                -970.42\n",
       "converged:                       True   LL-Null:                       -1118.5\n",
       "Covariance Type:            nonrobust   LLR p-value:                 7.523e-55\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -5.0483      0.520     -9.708      0.000      -6.067      -4.029\n",
       "gad1log        0.3210      0.327      0.980      0.327      -0.321       0.963\n",
       "phq1log        0.5102      0.398      1.283      0.199      -0.269       1.289\n",
       "age            0.0518      0.004     11.827      0.000       0.043       0.060\n",
       "cisscore      -0.0303      0.021     -1.423      0.155      -0.072       0.011\n",
       "site           0.0546      0.049      1.114      0.265      -0.041       0.151\n",
       "cis            0.0468      0.131      0.357      0.721      -0.210       0.304\n",
       "dep           -0.1035      0.122     -0.848      0.396      -0.343       0.136\n",
       "education      0.7781      0.109      7.153      0.000       0.565       0.991\n",
       "AD_past        0.1497      0.120      1.245      0.213      -0.086       0.386\n",
       "sex            0.3181      0.116      2.745      0.006       0.091       0.545\n",
       "ethnic         0.1069      0.075      1.435      0.151      -0.039       0.253\n",
       "fin            0.0612      0.083      0.737      0.461      -0.101       0.224\n",
       "empstat        0.1311      0.124      1.058      0.290      -0.112       0.374\n",
       "marstat        0.0049      0.079      0.062      0.951      -0.150       0.160\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "source": [
    "stratification_covariates = '+ site + dep + cis'\n",
    "exclusion_covariates = '+ age + education + AD_past + gad1log + empstat'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 21,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mle_df.columns:\n",
    "    col_zscore = col + '_zscore'\n",
    "    try:\n",
    "        mle_df[col_zscore] = (mle_df[col] - mle_df[col].mean())/mle_df[col].std(ddof=0)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preregistered Hypotheses\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": 22,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "T = 0, -0.53(0.81)\n",
      "T = 2, sert: -0.71(0.79) placebo: -0.55(0.79)\tgroup: \tbeta: -0.12,\tCI: [-0.29,0.05],\tpvalue: 0.1655\n",
      "T = 6, sert: -0.7(0.83) placebo: -0.77(0.85)\tgroup: \tbeta: 0.12,\tCI: [-0.06,0.3],\tpvalue: 0.2049\n",
      "over time\t\t\t\t\tgroup: \tbeta: -0.01,\tCI: [-0.14,0.12],\tpvalue: 0.8934\n",
      "time group interaction\t\t\t\tgroup: \tbeta: -0.3,\tCI: [-0.72,0.12],\tpvalue: 0.1627\n"
=======
      "T = 2, sert: -0.71(0.79) placebo: -0.55(0.79)\tgroup: \tbeta: -0.12,\tCI: [-0.28,0.05],\tpvalue: 0.1694\n",
      "T = 6, sert: -0.7(0.83) placebo: -0.77(0.85)\tgroup: \tbeta: 0.12,\tCI: [-0.06,0.3],\tpvalue: 0.2087\n",
      "over time\t\t\t\t\tgroup: \tbeta: -0.01,\tCI: [-0.14,0.12],\tpvalue: 0.8929\n",
      "time group interaction\t\t\t\tgroup: \tbeta: -0.3,\tCI: [-0.72,0.12],\tpvalue: 0.1651\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# H1: aversive Pav related to sertraline?\n",
    "timing = [((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==0), \\\n",
    "       ((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==0), (mle_df['exclusiontot']==0)]\n",
    "timing_label = ['2','6','over time', 'time x group']\n",
    "print('T = 0, ' + str(np.round(mle_df['av_Pav'][(mle_df['exclusiontot']==0)&(mle_df['time']==1)].mean(),2)) + '(' \\\n",
    "      + str(np.round(mle_df['av_Pav'][(mle_df['exclusiontot']==0)&(mle_df['time']==1)].std(),2)) +')')\n",
    "for i in range(2):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print('T = '+timing_label[i]+', sert: ' + str(np.round(tmp['av_Pav'][(tmp['group']==1)&(tmp['time']==i+2)].mean(),2)) + '(' \\\n",
    "      + str(np.round(tmp['av_Pav'][(tmp['group']==1)&(tmp['time']==i+2)].std(),2)) + ') placebo: ' \\\n",
    "      + str(np.round(tmp['av_Pav'][(tmp['group']==0)&(tmp['time']==i+2)].mean(),2)) + '(' \\\n",
    "      + str(np.round(tmp['av_Pav'][(tmp['group']==0)&(tmp['time']==i+2)].std(),2)) +')', end='\\t')\n",
    "    panda.mle('av_Pav ~ group + time' + stratification_covariates + exclusion_covariates, tmp, ['group'],[])\n",
    "    \n",
    "print('over time', end='\\t\\t\\t\\t\\t')\n",
    "panda.mle('av_Pav ~ group + time' + stratification_covariates + exclusion_covariates, mle_df[timing[2]], ['group'],[])\n",
    "print('time group interaction', end='\\t\\t\\t\\t')\n",
    "panda.mle('av_Pav ~ group * time' + stratification_covariates + exclusion_covariates, mle_df[timing[2]], ['group'],[])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 153,
=======
   "execution_count": 23,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2:\tav_Pav:group: \tbeta: -0.02,\tCI: [-0.08,0.04],\tpvalue: 0.4561\n",
      "6:\tav_Pav:group: \tbeta: 0.07,\tCI: [-0.01,0.15],\tpvalue: 0.1013\n",
      "over time:\tav_Pav:group: \tbeta: 0.01,\tCI: [-0.05,0.07],\tpvalue: 0.8026\n"
=======
      "2:\tav_Pav: \tbeta: -0.01,\tCI: [-0.04,0.01],\tpvalue: 0.3812\n",
      "6:\tav_Pav: \tbeta: -0.02,\tCI: [-0.05,0.01],\tpvalue: 0.2112\n",
      "over time:\tav_Pav: \tbeta: -0.02,\tCI: [-0.04,0.0],\tpvalue: 0.0636\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# H2: aversive Pav related to anxiety?\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
    "    panda.mle('gadlog ~ av_Pav + group + time' + stratification_covariates + exclusion_covariates, tmp, ['av_Pav'], 'av_Pav')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
=======
   "execution_count": 29,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2:\tapp_Pav: \tbeta: -0.02,\tCI: [-0.05,0.01],\tpvalue: 0.3107\n",
      "6:\tapp_Pav: \tbeta: -0.03,\tCI: [-0.07,0.0],\tpvalue: 0.0685\n",
      "over time:\tapp_Pav: \tbeta: -0.03,\tCI: [-0.06,0.0],\tpvalue: 0.0991\n"
=======
      "2:\tapp_Pav: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.4209\n",
      "6:\tapp_Pav: \tbeta: -0.03,\tCI: [-0.07,0.0],\tpvalue: 0.0545\n",
      "over time:\tapp_Pav: \tbeta: -0.02,\tCI: [-0.05,0.0],\tpvalue: 0.0792\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# H5: appetitive Pav related to anxiety?\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
<<<<<<< HEAD
    "    panda.mle('phqlog ~ app_Pav + group + time' + stratification_covariates + exclusion_covariates, \\\n",
    "              tmp, ['app_Pav'], 'app_Pav')"
=======
    "    panda.mle('phqlog ~ app_Pav + group + time + site + cis + dep + sex + age + education', tmp, ['app_Pav'], 'app_Pav')\n",
    "    "
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": 26,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2:\trew_se: \tbeta: -0.02,\tCI: [-0.08,0.04],\tpvalue: 0.5338\n",
      "6:\trew_se: \tbeta: 0.03,\tCI: [-0.04,0.1],\tpvalue: 0.3773\n",
      "over time:\trew_se: \tbeta: -0.01,\tCI: [-0.06,0.05],\tpvalue: 0.8087\n"
=======
      "2:\trew_se: \tbeta: -0.01,\tCI: [-0.07,0.05],\tpvalue: 0.686\n",
      "6:\trew_se: \tbeta: 0.04,\tCI: [-0.02,0.11],\tpvalue: 0.2061\n",
      "over time:\trew_se: \tbeta: 0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9039\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# H6: reward sensitivity related to anhedonia?\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
    "    panda.mle('anhedonia ~ rew_se + group + time' + stratification_covariates + exclusion_covariates, tmp, ['rew_se'], 'rew_se')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS\n",
    "---\n",
    "We nearly found evidence for Hypothesis 5, appetitive Pavlovian bias was reduced with depression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Analyses\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'phqlog', 'app_Pav']\n",
    "for col in cols:\n",
    "    col_zscore = col + '_zscore'\n",
    "    mle_df[col_zscore] = (mle_df[col] - mle_df[col].mean())/mle_df[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "###############################################################################################\n",
      "gad:\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.11,0.0],\tpvalue: 0.0726\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.1,0.05],\tpvalue: 0.5078\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.08,0.03],\tpvalue: 0.328\n",
      "###############################################################################################\n",
      "phq:\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.11,-0.0],\tpvalue: 0.0324\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.02,\tCI: [-0.09,0.04],\tpvalue: 0.5141\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.09,0.0],\tpvalue: 0.0779\n",
      "###############################################################################################\n",
      "bdi:\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.11,0.0],\tpvalue: 0.0547\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.11,0.04],\tpvalue: 0.3362\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.1,0.0],\tpvalue: 0.0541\n"
=======
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.11,0.0],\tpvalue: 0.0726\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.1,0.05],\tpvalue: 0.5078\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.08,0.03],\tpvalue: 0.328\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# does exlusion criteria have an effect on the effect of sertraline on anxiety overall?\n",
    "timing = [((mle_df['time']==1)|(mle_df['time']==2)),((mle_df['time']==1)|(mle_df['time']==3)), \\\n",
    "          ((mle_df['time']==1)|(mle_df['time']==2)|(mle_df['time']==3))]\n",
    "timing_label = ['T = 2\\t','T = 6\\t','over time']\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('###############################################################################################')\n",
    "    print(j + ':')\n",
    "    for i in range(3):\n",
    "        tmp = mle_df[timing[i]]\n",
    "        print(timing_label[i] + ':', end='\\t')\n",
    "        panda.mle(j + 'log ~ group * exclusiontot + time' + stratification_covariates, tmp, ['group:exclusiontot'], [])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 46,
=======
   "execution_count": 31,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "###############################################################################################\n",
      "gad:\n",
      "T = 2: exclusion = 0\tgroup: \tbeta: 0.01,\tCI: [-0.04,0.05],\tpvalue: 0.8124\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.09,\tCI: [-0.13,-0.05],\tpvalue: 0.0\n",
      "###############################################################################################\n",
      "phq:\n",
      "T = 2: exclusion = 0\tgroup: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.6327\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.06,\tCI: [-0.11,-0.01],\tpvalue: 0.0286\n",
      "###############################################################################################\n",
      "bdi:\n",
      "T = 2: exclusion = 0\tgroup: \tbeta: 0.02,\tCI: [-0.02,0.07],\tpvalue: 0.3328\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.06,\tCI: [-0.11,-0.0],\tpvalue: 0.0377\n"
=======
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.11,-0.0],\tpvalue: 0.0324\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.02,\tCI: [-0.09,0.04],\tpvalue: 0.5141\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.09,0.0],\tpvalue: 0.0779\n"
     ]
    }
   ],
   "source": [
    "# does exlusion criteria have an effect on the effect of sertraline on depression overall?\n",
    "timing = [((mle_df['time']==1)|(mle_df['time']==2)),((mle_df['time']==1)|(mle_df['time']==3)), \\\n",
    "          ((mle_df['time']==1)|(mle_df['time']==2)|(mle_df['time']==3))]\n",
    "timing_label = ['T = 2\\t','T = 6\\t','over time']\n",
    "\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
    "    panda.mle('phqlog ~ group * exclusiontot + time + site + cis + dep', tmp, ['group:exclusiontot'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 2: exclusion = 0\tgroup: \tbeta: -0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9813\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.11,\tCI: [-0.16,-0.05],\tpvalue: 0.0003\n",
      "T = 2: exclusion = 0\tgroup: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.7263\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.06,\tCI: [-0.12,-0.01],\tpvalue: 0.0219\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# posthoc at T = 2\n",
<<<<<<< HEAD
    "for j in psychiatric_questionnaires:\n",
    "    print('###############################################################################################')\n",
    "    print(j + ':')\n",
    "    for i in range(2):\n",
    "        print('T = 2: exclusion = ' + str(i), end='\\t') \n",
    "        panda.mle(j + 'log ~ group + time' + stratification_covariates + exclusion_covariates, \\\n",
    "                  mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==i)], ['group'], [])"
=======
    "for i in range(2):\n",
    "    print('T = 2: exclusion = ' + str(i), end='\\t') \n",
    "    panda.mle('gadlog ~ group + time + site + cis + dep', \\\n",
    "              mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==i)], ['group'], [])\n",
    "for i in range(2):\n",
    "    print('T = 2: exclusion = ' + str(i), end='\\t') \n",
    "    panda.mle('phqlog ~ group + time + site + cis + dep', \\\n",
    "              mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==i)], ['group'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 2: exclusion = 0\tgroup: \tbeta: -0.1,\tCI: [-0.16,-0.04],\tpvalue: 0.0017\n",
      "T = 2: exclusion = 1\tgroup: \tbeta: -0.09,\tCI: [-0.16,-0.01],\tpvalue: 0.0282\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print('T = 2: exclusion = ' + str(i), end='\\t') \n",
    "    panda.mle('gadlog ~ group + time + site + cis + dep', \\\n",
    "              mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==i)], ['group'], [])"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the task properly is NOT modulating the effect of sertraline on anxiety. At week 2 a trend towards an effect of sertraline on anxiety: in the exclusion group and none in the included group.\\\n",
    "Additionally, how sertraline affected anxiety at week 2 (follow-up 1) seemed to differ between patients who performed the task properly and the ones who did not. Sertraline had a larger effect in improving anxiety symptoms in the patients who provided uninformative task data (MD=-0.1, CI=[-0.16,-0.04], p$<$0.001) than in patients who provided informative task data (MD=-0.063, CI=[-0.05,0.05], p=0.063). However, the interaction effect was not significant (group x uninformative task run =-0.05, CI=[-0.11,0.01], p=0.0969).\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
=======
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "acctot\n",
      "-------\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.06,-0.01],\tpvalue: 0.0124\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.07,-0.01],\tpvalue: 0.0084\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.05,-0.01],\tpvalue: 0.0081\n",
      "-------\n",
      "acc_g2w\n",
      "-------\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.02,\tCI: [-0.09,0.05],\tpvalue: 0.5714\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: 0.01,\tCI: [-0.07,0.09],\tpvalue: 0.7809\n",
      "over time:\tgroup:exclusiontot: \tbeta: 0.01,\tCI: [-0.05,0.06],\tpvalue: 0.7786\n",
      "-------\n",
      "acc_g2a\n",
      "-------\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.1,0.01],\tpvalue: 0.1447\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: 0.02,\tCI: [-0.04,0.09],\tpvalue: 0.4741\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9551\n",
      "-------\n",
      "acc_ng2w\n",
      "-------\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.04,\tCI: [-0.12,0.05],\tpvalue: 0.3894\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.11,\tCI: [-0.2,-0.02],\tpvalue: 0.0202\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.07,\tCI: [-0.14,-0.0],\tpvalue: 0.0401\n",
      "-------\n",
      "acc_ng2a\n",
      "-------\n",
      "T = 2\t:\tgroup:exclusiontot: \tbeta: -0.03,\tCI: [-0.08,0.02],\tpvalue: 0.291\n",
      "T = 6\t:\tgroup:exclusiontot: \tbeta: -0.09,\tCI: [-0.14,-0.03],\tpvalue: 0.0029\n",
      "over time:\tgroup:exclusiontot: \tbeta: -0.05,\tCI: [-0.09,-0.01],\tpvalue: 0.023\n"
     ]
    }
   ],
   "source": [
    "# does the group have an effect on accuracy when controlling for the exclusion group?\n",
    "x = [x for x in gng_columns if 'acc' in x]\n",
    "for j in x:\n",
    "    print('-------\\n' + j + '\\n-------')\n",
    "    for i in range(3):\n",
    "        tmp = mle_df[timing[i]]\n",
    "        print(timing_label[i] + ':', end='\\t')\n",
    "        panda.mle(j + ' ~ group * exclusiontot + time + site + cis + dep', tmp, ['group:exclusiontot'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 2\t exclusion = 0\t\tgroup: \tbeta: 0.01,\tCI: [-0.02,0.03],\tpvalue: 0.6974\n",
      "T = 2\t exclusion = 1\t\tgroup: \tbeta: -0.01,\tCI: [-0.02,0.01],\tpvalue: 0.3719\n",
      "T = 6\t exclusion = 0\t\tgroup: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.449\n",
      "T = 6\t exclusion = 1\t\tgroup: \tbeta: 0.01,\tCI: [-0.0,0.03],\tpvalue: 0.123\n",
      "over time exclusion = 0\t\tgroup: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.3884\n",
      "over time exclusion = 1\t\tgroup: \tbeta: 0.0,\tCI: [-0.01,0.01],\tpvalue: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# posthoc at all timepoints\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        print(timing_label[i] + ' exclusion = ' + str(j), end='\\t\\t') \n",
    "        panda.mle('acctot ~ group + time', \\\n",
    "                  mle_df[timing[i]&(mle_df['exclusiontot']==j)], ['group'], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously in the excluded group accuracy is lower. Over time accuracy increases and apparently time decreases the influence on excluded group on accruacy (in the included group they get better wherease in the excluded group they stay the same). But overall drug group has no effect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction for multiple comparison: pval < 0.00625\n"
     ]
    }
   ],
   "source": [
    "print('correction for multiple comparison: pval < ' + str(0.05/8))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
=======
   "execution_count": 37,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "rew_se:\n",
      "time: \tbeta: 0.11,\tCI: [0.02,0.2],\tpvalue: 0.0209\n",
      "age: \tbeta: -0.0,\tCI: [-0.01,0.0],\tpvalue: 0.7207\n",
      "group: \tbeta: 0.02,\tCI: [-0.14,0.17],\tpvalue: 0.8409\n",
      "loss_se:\n",
      "time: \tbeta: 0.09,\tCI: [0.01,0.17],\tpvalue: 0.0202\n",
      "age: \tbeta: -0.01,\tCI: [-0.01,0.0],\tpvalue: 0.0501\n",
      "group: \tbeta: 0.01,\tCI: [-0.14,0.16],\tpvalue: 0.9104\n",
      "rew_LR:\n",
      "time: \tbeta: 0.11,\tCI: [0.01,0.21],\tpvalue: 0.0281\n",
      "age: \tbeta: -0.01,\tCI: [-0.02,-0.01],\tpvalue: 0.0005\n",
      "group: \tbeta: 0.07,\tCI: [-0.12,0.26],\tpvalue: 0.4595\n",
      "loss_LR:\n",
      "time: \tbeta: 0.13,\tCI: [-0.01,0.27],\tpvalue: 0.0752\n",
      "age: \tbeta: -0.01,\tCI: [-0.02,0.0],\tpvalue: 0.0987\n",
      "group: \tbeta: 0.1,\tCI: [-0.17,0.37],\tpvalue: 0.4521\n",
      "app_Pav:\n",
      "time: \tbeta: -0.08,\tCI: [-0.13,-0.04],\tpvalue: 0.0004\n",
      "age: \tbeta: 0.01,\tCI: [0.01,0.01],\tpvalue: 0.0\n",
      "group: \tbeta: 0.0,\tCI: [-0.09,0.09],\tpvalue: 0.961\n",
      "av_Pav:\n",
      "time: \tbeta: -0.1,\tCI: [-0.16,-0.04],\tpvalue: 0.0013\n",
      "age: \tbeta: 0.01,\tCI: [0.01,0.02],\tpvalue: 0.0\n",
      "group: \tbeta: -0.01,\tCI: [-0.14,0.12],\tpvalue: 0.8934\n",
      "noise:\n",
      "time: \tbeta: 0.06,\tCI: [-0.04,0.16],\tpvalue: 0.218\n",
      "age: \tbeta: -0.0,\tCI: [-0.01,0.0],\tpvalue: 0.1696\n",
      "group: \tbeta: -0.03,\tCI: [-0.22,0.15],\tpvalue: 0.7264\n",
      "bias:\n",
      "time: \tbeta: 0.13,\tCI: [0.05,0.21],\tpvalue: 0.0008\n",
      "age: \tbeta: -0.03,\tCI: [-0.03,-0.02],\tpvalue: 0.0\n",
      "group: \tbeta: 0.08,\tCI: [-0.07,0.23],\tpvalue: 0.3094\n",
      "acctot:\n",
      "time: \tbeta: 0.02,\tCI: [0.01,0.03],\tpvalue: 0.0\n",
      "age: \tbeta: -0.0,\tCI: [-0.0,-0.0],\tpvalue: 0.0\n",
      "group: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.447\n"
=======
      "rew_se\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: rew_se    \n",
      "No. Observations: 884     Method:             REML      \n",
      "No. Groups:       433     Scale:              0.9872    \n",
      "Min. group size:  1       Log-Likelihood:     -1265.4267\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.0                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      3.190    0.254 12.567 0.000  2.692  3.687\n",
      "group          0.018    0.081  0.223 0.823 -0.140  0.177\n",
      "time           0.105    0.046  2.306 0.021  0.016  0.195\n",
      "site          -0.045    0.031 -1.472 0.141 -0.105  0.015\n",
      "cis            0.057    0.045  1.269 0.204 -0.031  0.144\n",
      "dep           -0.083    0.073 -1.129 0.259 -0.226  0.061\n",
      "age           -0.001    0.003 -0.520 0.603 -0.007  0.004\n",
      "education     -0.102    0.079 -1.296 0.195 -0.256  0.052\n",
      "sex           -0.048    0.068 -0.705 0.481 -0.182  0.086\n",
      "subject Var    0.000    0.043                           \n",
      "========================================================\n",
      "\n",
      "loss_se\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: loss_se   \n",
      "No. Observations: 884     Method:             REML      \n",
      "No. Groups:       433     Scale:              0.6678    \n",
      "Min. group size:  1       Log-Likelihood:     -1195.1801\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.0                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      1.388    0.264  5.263 0.000  0.871  1.905\n",
      "group          0.018    0.079  0.234 0.815 -0.135  0.172\n",
      "time           0.090    0.040  2.260 0.024  0.012  0.169\n",
      "site          -0.001    0.032 -0.023 0.982 -0.064  0.063\n",
      "cis            0.013    0.048  0.276 0.783 -0.080  0.107\n",
      "dep            0.090    0.078  1.155 0.248 -0.063  0.243\n",
      "age           -0.005    0.003 -1.900 0.057 -0.011  0.000\n",
      "education     -0.023    0.082 -0.277 0.782 -0.183  0.138\n",
      "sex           -0.105    0.073 -1.438 0.150 -0.249  0.038\n",
      "subject Var    0.206    0.065                           \n",
      "========================================================\n",
      "\n",
      "rew_LR\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: rew_LR    \n",
      "No. Observations: 884     Method:             REML      \n",
      "No. Groups:       433     Scale:              1.2102    \n",
      "Min. group size:  1       Log-Likelihood:     -1385.6845\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.0                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept     -1.785    0.303 -5.901 0.000 -2.378 -1.192\n",
      "group          0.070    0.095  0.739 0.460 -0.116  0.256\n",
      "time           0.114    0.052  2.198 0.028  0.012  0.215\n",
      "site          -0.009    0.037 -0.256 0.798 -0.081  0.063\n",
      "cis           -0.107    0.054 -1.996 0.046 -0.212 -0.002\n",
      "dep           -0.016    0.088 -0.185 0.853 -0.189  0.156\n",
      "age           -0.011    0.003 -3.415 0.001 -0.017 -0.005\n",
      "education      0.126    0.094  1.345 0.179 -0.058  0.310\n",
      "sex           -0.050    0.082 -0.608 0.543 -0.211  0.111\n",
      "subject Var    0.095    0.054                           \n",
      "========================================================\n",
      "\n",
      "loss_LR\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: loss_LR   \n",
      "No. Observations: 884     Method:             REML      \n",
      "No. Groups:       433     Scale:              2.1885    \n",
      "Min. group size:  1       Log-Likelihood:     -1688.6858\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.0                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept     -0.444    0.450 -0.986 0.324 -1.326  0.438\n",
      "group          0.087    0.137  0.639 0.523 -0.181  0.355\n",
      "time           0.131    0.071  1.830 0.067 -0.009  0.270\n",
      "site           0.011    0.055  0.203 0.839 -0.097  0.119\n",
      "cis           -0.087    0.081 -1.073 0.283 -0.245  0.072\n",
      "dep           -0.282    0.133 -2.127 0.033 -0.542 -0.022\n",
      "age           -0.008    0.005 -1.766 0.077 -0.018  0.001\n",
      "education     -0.046    0.140 -0.329 0.742 -0.320  0.228\n",
      "sex            0.159    0.124  1.282 0.200 -0.084  0.402\n",
      "subject Var    0.466    0.099                           \n",
      "========================================================\n",
      "\n",
      "app_Pav\t         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: app_Pav  \n",
      "No. Observations: 884     Method:             REML     \n",
      "No. Groups:       433     Scale:              0.2178   \n",
      "Min. group size:  1       Log-Likelihood:     -705.5962\n",
      "Max. group size:  3       Converged:          Yes      \n",
      "Mean group size:  2.0                                  \n",
      "-------------------------------------------------------\n",
      "             Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------\n",
      "Intercept    -0.566    0.151 -3.748 0.000 -0.862 -0.270\n",
      "group         0.002    0.045  0.041 0.968 -0.086  0.090\n",
      "time         -0.081    0.023 -3.554 0.000 -0.126 -0.036\n",
      "site         -0.038    0.019 -2.041 0.041 -0.074 -0.002\n",
      "cis           0.050    0.027  1.835 0.066 -0.003  0.104\n",
      "dep          -0.058    0.045 -1.293 0.196 -0.145  0.030\n",
      "age           0.011    0.002  6.830 0.000  0.008  0.014\n",
      "education     0.026    0.047  0.559 0.576 -0.066  0.118\n",
      "sex           0.016    0.042  0.387 0.698 -0.066  0.098\n",
      "subject Var   0.068    0.034                           \n",
      "=======================================================\n",
      "\n",
      "av_Pav\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: av_Pav    \n",
      "No. Observations: 884     Method:             REML      \n",
      "No. Groups:       433     Scale:              0.3906    \n",
      "Min. group size:  1       Log-Likelihood:     -1025.1363\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.0                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept     -1.208    0.234 -5.168 0.000 -1.666 -0.750\n",
      "group         -0.009    0.065 -0.135 0.893 -0.137  0.120\n",
      "time          -0.102    0.032 -3.217 0.001 -0.164 -0.040\n",
      "site          -0.035    0.029 -1.188 0.235 -0.092  0.022\n",
      "cis            0.086    0.043  2.016 0.044  0.002  0.170\n",
      "dep            0.058    0.070  0.833 0.405 -0.079  0.196\n",
      "age            0.013    0.003  4.961 0.000  0.008  0.017\n",
      "education      0.083    0.073  1.138 0.255 -0.060  0.225\n",
      "sex            0.032    0.066  0.480 0.631 -0.097  0.161\n",
      "subject Var    0.237    0.063                           \n",
      "========================================================\n",
      "\n",
      "noise\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: noise     \n",
      "No. Observations: 884     Method:             REML      \n",
      "No. Groups:       433     Scale:              1.0827    \n",
      "Min. group size:  1       Log-Likelihood:     -1378.9518\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.0                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      2.700    0.316  8.545 0.000  2.081  3.319\n",
      "group         -0.028    0.096 -0.296 0.767 -0.217  0.160\n",
      "time           0.060    0.050  1.204 0.229 -0.038  0.159\n",
      "site           0.086    0.039  2.236 0.025  0.011  0.162\n",
      "cis            0.004    0.057  0.075 0.940 -0.107  0.115\n",
      "dep            0.184    0.093  1.981 0.048  0.002  0.366\n",
      "age           -0.005    0.003 -1.483 0.138 -0.012  0.002\n",
      "education     -0.049    0.098 -0.496 0.620 -0.241  0.143\n",
      "sex            0.003    0.087  0.039 0.969 -0.167  0.174\n",
      "subject Var    0.224    0.074                           \n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias\t         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: bias      \n",
      "No. Observations: 884     Method:             REML      \n",
      "No. Groups:       433     Scale:              0.6251    \n",
      "Min. group size:  1       Log-Likelihood:     -1180.8149\n",
      "Max. group size:  3       Converged:          Yes       \n",
      "Mean group size:  2.0                                   \n",
      "--------------------------------------------------------\n",
      "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "Intercept      1.229    0.264  4.660 0.000  0.712  1.746\n",
      "group          0.077    0.078  0.988 0.323 -0.075  0.229\n",
      "time           0.132    0.039  3.383 0.001  0.056  0.208\n",
      "site          -0.023    0.033 -0.716 0.474 -0.087  0.041\n",
      "cis           -0.090    0.048 -1.874 0.061 -0.183  0.004\n",
      "dep           -0.061    0.078 -0.783 0.433 -0.215  0.092\n",
      "age           -0.025    0.003 -8.997 0.000 -0.031 -0.020\n",
      "education     -0.050    0.082 -0.610 0.542 -0.211  0.111\n",
      "sex            0.000    0.074  0.001 0.999 -0.144  0.144\n",
      "subject Var    0.230    0.061                           \n",
      "========================================================\n",
      "\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ between drug group (included group)?\n",
    "for i in parameter_labels[:-1] + ['acctot']:\n",
    "    print(i + ':')\n",
    "    panda.mle(i + ' ~ group + time' + stratification_covariates + exclusion_covariates, \\\n",
    "                        mle_df[mle_df['exclusiontot']==0], ['time', 'age', 'group'], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pavlovian biases decrease and bias increases over time\n",
    "- Pavlovian biases increase and bias decreases with age"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 50,
=======
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = parameter_labels\n",
    "for col in cols:\n",
    "    col_zscore = col + '_zscore'\n",
    "    mle_df[col_zscore] = (mle_df[col] - mle_df[col].mean())/mle_df[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>loss_LR</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>No. Observations:</td>   <td>603</td>         <td>Method:</td>          <td>REML</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>No. Groups:</td>      <td>390</td>         <td>Scale:</td>          <td>2.1915</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Min. group size:</td>     <td>1</td>      <td>Log-Likelihood:</td>   <td>-1142.3433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Max. group size:</td>     <td>2</td>        <td>Converged:</td>          <td>Yes</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Mean group size:</td>    <td>1.5</td>            <td></td>                <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>        <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>    <td>0.052</td>   <td>0.544</td>   <td>0.095</td> <td>0.924</td> <td>-1.015</td>  <td>1.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group</th>        <td>0.565</td>   <td>0.184</td>   <td>3.070</td> <td>0.002</td>  <td>0.204</td>  <td>0.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time</th>        <td>-0.267</td>   <td>0.148</td>  <td>-1.801</td> <td>0.072</td> <td>-0.557</td>  <td>0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>site</th>        <td>-0.000</td>   <td>0.060</td>  <td>-0.000</td> <td>1.000</td> <td>-0.118</td>  <td>0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cis</th>         <td>-0.003</td>   <td>0.089</td>  <td>-0.037</td> <td>0.971</td> <td>-0.178</td>  <td>0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dep</th>         <td>-0.351</td>   <td>0.147</td>  <td>-2.387</td> <td>0.017</td> <td>-0.639</td> <td>-0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>         <td>-0.012</td>   <td>0.006</td>  <td>-2.056</td> <td>0.040</td> <td>-0.022</td> <td>-0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th>   <td>-0.053</td>   <td>0.154</td>  <td>-0.346</td> <td>0.729</td> <td>-0.354</td>  <td>0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AD_past</th>      <td>0.253</td>   <td>0.139</td>   <td>1.817</td> <td>0.069</td> <td>-0.020</td>  <td>0.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marstat</th>     <td>-0.083</td>   <td>0.106</td>  <td>-0.781</td> <td>0.435</td> <td>-0.292</td>  <td>0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>subject Var</th>  <td>0.320</td>   <td>0.137</td>     <td></td>      <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "         Mixed Linear Model Regression Results\n",
       "========================================================\n",
       "Model:            MixedLM Dependent Variable: loss_LR   \n",
       "No. Observations: 603     Method:             REML      \n",
       "No. Groups:       390     Scale:              2.1915    \n",
       "Min. group size:  1       Log-Likelihood:     -1142.3433\n",
       "Max. group size:  2       Converged:          Yes       \n",
       "Mean group size:  1.5                                   \n",
       "--------------------------------------------------------\n",
       "              Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
       "--------------------------------------------------------\n",
       "Intercept      0.052    0.544  0.095 0.924 -1.015  1.119\n",
       "group          0.565    0.184  3.070 0.002  0.204  0.926\n",
       "time          -0.267    0.148 -1.801 0.072 -0.557  0.024\n",
       "site          -0.000    0.060 -0.000 1.000 -0.118  0.118\n",
       "cis           -0.003    0.089 -0.037 0.971 -0.178  0.172\n",
       "dep           -0.351    0.147 -2.387 0.017 -0.639 -0.063\n",
       "age           -0.012    0.006 -2.056 0.040 -0.022 -0.001\n",
       "education     -0.053    0.154 -0.346 0.729 -0.354  0.248\n",
       "AD_past        0.253    0.139  1.817 0.069 -0.020  0.526\n",
       "marstat       -0.083    0.106 -0.781 0.435 -0.292  0.126\n",
       "subject Var    0.320    0.137                           \n",
       "========================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smf.mixedlm('loss_LR ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                    mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==0)], \\\n",
    "                    groups= 'subject', missing='drop').fit(method='nm')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "rew_se\tgroup:time: \tbeta: -0.13,\tCI: [-0.38,0.13],\tpvalue: 0.3268\n",
      "loss_se\tgroup:time: \tbeta: 0.01,\tCI: [-0.21,0.23],\tpvalue: 0.9306\n",
      "rew_LR\tgroup:time: \tbeta: -0.01,\tCI: [-0.3,0.28],\tpvalue: 0.9492\n",
      "loss_LR\tgroup:time: \tbeta: -0.29,\tCI: [-0.68,0.1],\tpvalue: 0.1451\n",
      "app_Pav\tgroup:time: \tbeta: -0.06,\tCI: [-0.18,0.07],\tpvalue: 0.361\n",
      "av_Pav\tgroup:time: \tbeta: 0.12,\tCI: [-0.05,0.3],\tpvalue: 0.1547\n",
      "noise\tgroup:time: \tbeta: 0.1,\tCI: [-0.18,0.37],\tpvalue: 0.493\n",
      "bias\tgroup:time: \tbeta: -0.13,\tCI: [-0.34,0.08],\tpvalue: 0.2291\n"
=======
      "rew_se\tgroup:time: \tbeta: -0.13,\tCI: [-0.39,0.13],\tpvalue: 0.3185\n",
      "loss_se\tgroup:time: \tbeta: 0.01,\tCI: [-0.21,0.23],\tpvalue: 0.9097\n",
      "rew_LR\tgroup:time: \tbeta: -0.01,\tCI: [-0.29,0.28],\tpvalue: 0.9656\n",
      "loss_LR\tgroup:time: \tbeta: -0.3,\tCI: [-0.69,0.1],\tpvalue: 0.1393\n",
      "app_Pav\tgroup:time: \tbeta: -0.06,\tCI: [-0.18,0.07],\tpvalue: 0.3623\n",
      "av_Pav\tgroup:time: \tbeta: 0.12,\tCI: [-0.05,0.3],\tpvalue: 0.1551\n",
      "noise\tgroup:time: \tbeta: 0.1,\tCI: [-0.18,0.37],\tpvalue: 0.4897\n",
      "bias\tgroup:time: \tbeta: -0.13,\tCI: [-0.34,0.08],\tpvalue: 0.2313\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ time x group interaction (included group)?\n",
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end = '\\t');\n",
<<<<<<< HEAD
    "    panda.mle(i + ' ~ group * time' + stratification_covariates + exclusion_covariates, \\\n",
=======
    "    panda.mle(i + ' ~ group * time + site + cis + dep + age + education + AD_past + marstat', \\\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
    "               mle_df[(mle_df['exclusiontot']==0)], ['group:time'], [])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
=======
   "execution_count": 41,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "rew_se\tgroup: \tbeta: 0.21,\tCI: [-0.01,0.44],\tpvalue: 0.0673\n",
      "loss_se\tgroup: \tbeta: -0.16,\tCI: [-0.37,0.04],\tpvalue: 0.1213\n",
      "rew_LR\tgroup: \tbeta: -0.06,\tCI: [-0.32,0.21],\tpvalue: 0.6752\n",
      "loss_LR\tgroup: \tbeta: 0.57,\tCI: [0.21,0.93],\tpvalue: 0.0021\n",
      "app_Pav\tgroup: \tbeta: 0.06,\tCI: [-0.06,0.18],\tpvalue: 0.3549\n",
      "av_Pav\tgroup: \tbeta: -0.12,\tCI: [-0.29,0.05],\tpvalue: 0.1655\n",
      "noise\tgroup: \tbeta: -0.15,\tCI: [-0.42,0.12],\tpvalue: 0.2765\n",
      "bias\tgroup: \tbeta: 0.03,\tCI: [-0.18,0.23],\tpvalue: 0.8017\n"
=======
      "rew_se\tgroup: \tbeta: 0.21,\tCI: [-0.01,0.44],\tpvalue: 0.0623\n",
      "loss_se\tgroup: \tbeta: -0.16,\tCI: [-0.37,0.05],\tpvalue: 0.1266\n",
      "rew_LR\tgroup: \tbeta: -0.06,\tCI: [-0.32,0.2],\tpvalue: 0.6463\n",
      "loss_LR\tgroup: \tbeta: 0.57,\tCI: [0.2,0.93],\tpvalue: 0.0021\n",
      "app_Pav\tgroup: \tbeta: 0.06,\tCI: [-0.06,0.18],\tpvalue: 0.3492\n",
      "av_Pav\tgroup: \tbeta: -0.12,\tCI: [-0.28,0.05],\tpvalue: 0.1673\n",
      "noise\tgroup: \tbeta: -0.15,\tCI: [-0.41,0.12],\tpvalue: 0.282\n",
      "bias\tgroup: \tbeta: 0.02,\tCI: [-0.18,0.23],\tpvalue: 0.8118\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ between drug group at session 2 (included group)?\n",
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end = '\\t');\n",
<<<<<<< HEAD
    "    panda.mle(i + ' ~ group + time' + stratification_covariates + exclusion_covariates, \\\n",
=======
    "    panda.mle(i + ' ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
    "               mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==0)], ['group'],[])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
=======
   "execution_count": 42,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "rew_se\tgroup: \tbeta: -0.13,\tCI: [-0.36,0.11],\tpvalue: 0.2895\n",
      "loss_se\tgroup: \tbeta: 0.12,\tCI: [-0.1,0.34],\tpvalue: 0.2775\n",
      "rew_LR\tgroup: \tbeta: 0.13,\tCI: [-0.13,0.39],\tpvalue: 0.3323\n",
      "loss_LR\tgroup: \tbeta: -0.28,\tCI: [-0.67,0.11],\tpvalue: 0.1559\n",
      "app_Pav\tgroup: \tbeta: -0.04,\tCI: [-0.17,0.08],\tpvalue: 0.5084\n",
      "av_Pav\tgroup: \tbeta: 0.12,\tCI: [-0.06,0.3],\tpvalue: 0.2049\n",
      "noise\tgroup: \tbeta: 0.06,\tCI: [-0.2,0.33],\tpvalue: 0.6446\n",
      "bias\tgroup: \tbeta: 0.03,\tCI: [-0.18,0.24],\tpvalue: 0.7599\n"
=======
      "rew_se\tgroup: \tbeta: -0.14,\tCI: [-0.37,0.09],\tpvalue: 0.2414\n",
      "loss_se\tgroup: \tbeta: 0.12,\tCI: [-0.1,0.34],\tpvalue: 0.2727\n",
      "rew_LR\tgroup: \tbeta: 0.13,\tCI: [-0.13,0.39],\tpvalue: 0.3324\n",
      "loss_LR\tgroup: \tbeta: -0.29,\tCI: [-0.68,0.1],\tpvalue: 0.1487\n",
      "app_Pav\tgroup: \tbeta: -0.04,\tCI: [-0.17,0.08],\tpvalue: 0.503\n",
      "av_Pav\tgroup: \tbeta: 0.12,\tCI: [-0.06,0.3],\tpvalue: 0.203\n",
      "noise\tgroup: \tbeta: 0.07,\tCI: [-0.2,0.33],\tpvalue: 0.6133\n",
      "bias\tgroup: \tbeta: 0.04,\tCI: [-0.17,0.25],\tpvalue: 0.7004\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# do cognitive parameters differ between drug group at session 3 (included group)?\n",
    "for i in parameter_labels[:-1]:\n",
    "    print(i, end = '\\t');\n",
    "    panda.mle(i + ' ~ group + time' + stratification_covariates + exclusion_covariates, \\\n",
    "               mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==0)], ['group'], [])"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline bias\tgroup: \tbeta: 0.2,\tCI: [-0.02,0.43],\tpvalue: 0.0743\n"
     ]
    }
   ],
   "source": [
    "# does the random bias differ between drug group in the excluded group?\n",
    "print('random baseline bias', end = '\\t');\n",
    "panda.mle('rbias ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "           mle_df[mle_df['exclusiontot']==1], ['group'],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline bias\tgroup: \tbeta: 0.12,\tCI: [-0.18,0.42],\tpvalue: 0.43\n",
      "random baseline bias\tgroup: \tbeta: 0.25,\tCI: [-0.09,0.59],\tpvalue: 0.1435\n"
     ]
    }
   ],
   "source": [
    "# does the random bias differ between drug group in the excluded group at session 2?\n",
    "print('random baseline bias', end = '\\t');\n",
    "panda.mle('rbias ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==1)], ['group'],[])\n",
    "\n",
    "# does the random bias differ between drug group in the excluded group at session 3?\n",
    "print('random baseline bias', end = '\\t');\n",
    "panda.mle('rbias ~ group + time + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==1)], ['group'],[])"
   ]
  },
  {
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interim Results\n",
    "--\n",
    "Drug effects:\n",
    "- at 2 weeks:\n",
    "    - trend towards reward sensitivity HIGHER in sertraline group\n",
    "    - loss LR HIGHER in sertraline group\n",
    "- at 6 weeks:\n",
    "    - trend towards loss LR LOWER in sertraline group at 6 weeks\n",
    "- over all sessions:\n",
    "    - -\n",
    "- time x group:\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 66,
=======
   "execution_count": 45,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "###############################################################################################\n",
      "gad:\n",
      "rew_se: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.6805\n",
      "loss_se: \tbeta: -0.02,\tCI: [-0.04,0.0],\tpvalue: 0.1018\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.03,0.0],\tpvalue: 0.1558\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.0,0.03],\tpvalue: 0.12\n",
      "app_Pav: \tbeta: 0.02,\tCI: [-0.02,0.06],\tpvalue: 0.3257\n",
      "av_Pav: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.704\n",
      "noise: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9399\n",
      "bias: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.4536\n",
      "###############################################################################################\n",
      "phq:\n",
      "rew_se: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.8727\n",
      "loss_se: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.2239\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.02,0.01],\tpvalue: 0.3301\n",
      "loss_LR: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.5976\n",
      "app_Pav: \tbeta: -0.01,\tCI: [-0.05,0.02],\tpvalue: 0.5187\n",
      "av_Pav: \tbeta: 0.01,\tCI: [-0.02,0.03],\tpvalue: 0.6498\n",
      "noise: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.6637\n",
      "bias: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.83\n",
      "###############################################################################################\n",
      "bdi:\n",
      "rew_se: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.3047\n",
      "loss_se: \tbeta: -0.02,\tCI: [-0.04,-0.0],\tpvalue: 0.0352\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.03,0.0],\tpvalue: 0.1681\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.2365\n",
      "app_Pav: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.5864\n",
      "av_Pav: \tbeta: 0.0,\tCI: [-0.03,0.03],\tpvalue: 0.8393\n",
      "noise: \tbeta: -0.01,\tCI: [-0.03,0.0],\tpvalue: 0.1278\n",
      "bias: \tbeta: -0.0,\tCI: [-0.03,0.03],\tpvalue: 0.9164\n"
=======
      "rew_se: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.8394\n",
      "loss_se: \tbeta: -0.02,\tCI: [-0.04,0.0],\tpvalue: 0.0577\n",
      "rew_LR: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.8818\n",
      "loss_LR: \tbeta: 0.02,\tCI: [0.01,0.03],\tpvalue: 0.0009\n",
      "app_Pav: \tbeta: 0.0,\tCI: [-0.03,0.03],\tpvalue: 0.8824\n",
      "av_Pav: \tbeta: -0.02,\tCI: [-0.04,0.0],\tpvalue: 0.0503\n",
      "noise: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.9683\n",
      "bias: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.7416\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "# do cognitive parameters have an effect on psychiatric scores over time in the included group controlled drug group?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('###############################################################################################')\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.mle(j + 'log ~ ' + i + ' + group + time + site + age + education', \\\n",
    "                            mle_df[(mle_df['exclusiontot']==0)], [i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################################################################\n",
      "gad:\n",
      "rew_se: \tbeta: -0.01,\tCI: [-0.03,0.02],\tpvalue: 0.6144\n",
      "loss_se: \tbeta: -0.03,\tCI: [-0.05,-0.0],\tpvalue: 0.0496\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.5513\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.0,0.03],\tpvalue: 0.0618\n",
      "app_Pav: \tbeta: -0.0,\tCI: [-0.05,0.04],\tpvalue: 0.9521\n",
      "av_Pav: \tbeta: -0.02,\tCI: [-0.05,0.02],\tpvalue: 0.3768\n",
      "noise: \tbeta: -0.0,\tCI: [-0.02,0.02],\tpvalue: 0.7714\n",
      "bias: \tbeta: 0.03,\tCI: [-0.01,0.06],\tpvalue: 0.1045\n",
      "###############################################################################################\n",
      "phq:\n",
      "rew_se: \tbeta: -0.01,\tCI: [-0.02,0.01],\tpvalue: 0.5531\n",
      "loss_se: \tbeta: -0.03,\tCI: [-0.05,-0.0],\tpvalue: 0.0291\n",
      "rew_LR: \tbeta: -0.0,\tCI: [-0.02,0.01],\tpvalue: 0.6263\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.2723\n",
      "app_Pav: \tbeta: -0.02,\tCI: [-0.06,0.02],\tpvalue: 0.2679\n",
      "av_Pav: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.7472\n",
      "noise: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9232\n",
      "bias: \tbeta: 0.02,\tCI: [-0.01,0.04],\tpvalue: 0.2457\n",
      "###############################################################################################\n",
      "bdi:\n",
      "rew_se: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.816\n",
      "loss_se: \tbeta: -0.03,\tCI: [-0.05,-0.01],\tpvalue: 0.0128\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.03,0.0],\tpvalue: 0.1459\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.0,0.02],\tpvalue: 0.185\n",
      "app_Pav: \tbeta: 0.0,\tCI: [-0.04,0.04],\tpvalue: 0.9564\n",
      "av_Pav: \tbeta: 0.0,\tCI: [-0.03,0.03],\tpvalue: 0.7869\n",
      "noise: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.3936\n",
      "bias: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.499\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters have an effect on psychiatric scores at 2 weeks in the included group controlled drug group?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('###############################################################################################')\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.mle(j + 'log ~ ' + i + ' + group + time', \\\n",
    "                            mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==0)], [i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################################################################\n",
      "gad:\n",
      "rew_se: \tbeta: 0.02,\tCI: [-0.0,0.05],\tpvalue: 0.1123\n",
      "loss_se: \tbeta: -0.01,\tCI: [-0.03,0.02],\tpvalue: 0.6884\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.4153\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.1854\n",
      "app_Pav: \tbeta: -0.02,\tCI: [-0.07,0.03],\tpvalue: 0.4765\n",
      "av_Pav: \tbeta: -0.01,\tCI: [-0.04,0.03],\tpvalue: 0.6209\n",
      "noise: \tbeta: -0.01,\tCI: [-0.03,0.02],\tpvalue: 0.5188\n",
      "bias: \tbeta: 0.0,\tCI: [-0.03,0.03],\tpvalue: 0.9611\n",
      "###############################################################################################\n",
      "phq:\n",
      "rew_se: \tbeta: 0.01,\tCI: [-0.01,0.04],\tpvalue: 0.2729\n",
      "loss_se: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.8161\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.5133\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.4315\n",
      "app_Pav: \tbeta: -0.04,\tCI: [-0.09,0.0],\tpvalue: 0.0724\n",
      "av_Pav: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.6655\n",
      "noise: \tbeta: 0.01,\tCI: [-0.01,0.03],\tpvalue: 0.521\n",
      "bias: \tbeta: 0.0,\tCI: [-0.03,0.03],\tpvalue: 0.7981\n",
      "###############################################################################################\n",
      "bdi:\n",
      "rew_se: \tbeta: 0.03,\tCI: [0.0,0.05],\tpvalue: 0.0274\n",
      "loss_se: \tbeta: -0.01,\tCI: [-0.04,0.01],\tpvalue: 0.3407\n",
      "rew_LR: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.4483\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.0,0.03],\tpvalue: 0.1718\n",
      "app_Pav: \tbeta: -0.01,\tCI: [-0.05,0.04],\tpvalue: 0.7432\n",
      "av_Pav: \tbeta: -0.01,\tCI: [-0.04,0.02],\tpvalue: 0.4857\n",
      "noise: \tbeta: -0.01,\tCI: [-0.03,0.02],\tpvalue: 0.6403\n",
      "bias: \tbeta: -0.0,\tCI: [-0.03,0.03],\tpvalue: 0.8408\n"
     ]
    }
   ],
   "source": [
    "# do cognitive parameters have an effect on psychiatric scores at 6 weeks in the included group controlled drug group?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('###############################################################################################')\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.mle(j + 'log ~ ' + i + ' + group + time', \\\n",
    "                            mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==0)], [i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interim Results\n",
    "--\n",
    "GAD:\n",
    "- at 2 weeks:\n",
    "    - loss LR is POSITIVELY related to GAD over all sessions\n",
    "- at 6 weeks:\n",
    "    - - \n",
    "- over all sessions:\n",
    "    - loss LR is POSITIVELY related to GAD over all sessions\n",
    "    - app Pav is NEGATIVELY related to PHQ over all sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluded task runs:\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline bias\n",
      "2:\tgroup: \tbeta: 0.13,\tCI: [-0.17,0.43],\tpvalue: 0.4077\n",
      "6:\tgroup: \tbeta: 0.27,\tCI: [-0.06,0.61],\tpvalue: 0.1129\n",
      "over time:\tgroup: \tbeta: 0.21,\tCI: [-0.01,0.44],\tpvalue: 0.0605\n"
     ]
    }
   ],
   "source": [
    "# does the random bias differ between drug group in the excluded group?\n",
    "timing = [((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==1), \\\n",
    "       ((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==1), (mle_df['exclusiontot']==1)]\n",
    "timing_label = ['2','6','over time', 'time x group']\n",
    "print('random baseline bias');\n",
    "for i in range(3):\n",
    "    tmp = mle_df[timing[i]]\n",
    "    print(timing_label[i] + ':', end='\\t')\n",
    "    panda.mle('rbias ~ group + time' + stratification_covariates + exclusion_covariates, tmp, ['group'],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################################################################\n",
      "gad:\n",
      "rbias: \tbeta: -0.01,\tCI: [-0.03,0.02],\tpvalue: 0.6793\n",
      "rbias: \tbeta: -0.02,\tCI: [-0.05,0.0],\tpvalue: 0.0672\n",
      "rbias: \tbeta: -0.0,\tCI: [-0.03,0.02],\tpvalue: 0.7428\n",
      "###############################################################################################\n",
      "phq:\n",
      "rbias: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.8231\n",
      "rbias: \tbeta: -0.01,\tCI: [-0.03,0.01],\tpvalue: 0.3235\n",
      "rbias: \tbeta: 0.01,\tCI: [-0.02,0.03],\tpvalue: 0.6236\n",
      "###############################################################################################\n",
      "beck:\n",
      "rbias: \tbeta: 0.02,\tCI: [-0.01,0.04],\tpvalue: 0.1338\n",
      "rbias: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9832\n",
      "rbias: \tbeta: 0.02,\tCI: [-0.0,0.04],\tpvalue: 0.1189\n"
     ]
    }
   ],
   "source": [
    "# do rbias have an effect on psychiatric scores over sessions in the excluded group controlled drug group?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('###############################################################################################')\n",
    "    print(j + ':')\n",
    "    model = panda.mle(j + 'log ~ rbias + group + time + age + education + marstat + AD_past', \\\n",
    "                        mle_df[mle_df['exclusiontot']==1],['rbias'], 'rbias')\n",
    "    # do rbias have an effect on gad at 2 weeks in the excluded group controlled drug group?\n",
    "    model = panda.mle(j + 'log ~ rbias + group + time + age + education + marstat + AD_past', \\\n",
    "                        mle_df[((mle_df['time']==1)|(mle_df['time']==2))&(mle_df['exclusiontot']==1)],['rbias'], 'rbias')\n",
    "    # do rbias have an effect on gad at 6 weeks in the excluded group controlled drug group?\n",
    "    model = panda.mle(j + 'log ~ rbias + group + time + age + education + marstat + AD_past', \\\n",
    "                        mle_df[((mle_df['time']==1)|(mle_df['time']==3))&(mle_df['exclusiontot']==1)],['rbias'], 'rbias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "--\n",
    "- Patients who did the task properly:\n",
    "    <br><b>Drug effects:</b>\n",
    "    - at 2 weeks:\n",
    "        - reward sensitivity HIGHER in sertraline group\n",
    "        - loss LR HIGHER in sertraline group\n",
    "        - trend towards LOWER av Pav in sertraline group\n",
    "    - at 6 weeks:\n",
    "        - trend towards loss LR LOWER in sertraline group at 6 weeks\n",
    "    - over all sessions:\n",
    "        - -\n",
    "    - time x group:\n",
    "        - for loss LR interaction significant over time effect reduced\n",
    "    <br><b>GAD:</b>\n",
    "    - at 2 weeks:\n",
    "        - loss LR is POSITIVELY related to GAD over all sessions\n",
    "    - at 6 weeks:\n",
    "        - app Pav is NEGATIVELY related to PHQ over all sessions\n",
    "    - over all sessions:\n",
    "        - loss LR is POSITIVELY related to GAD over all sessions\n",
    "        - app Pav is NEGATIVELY related to PHQ over all sessions\n",
    "- patients who did NOT do the task properly:\n",
    "    <br><b>Drug effects:</b>\n",
    "    - at 2 weeks:\n",
    "        - -\n",
    "    - at 6 weeks:\n",
    "        - - \n",
    "    - over all sessions:\n",
    "        - trend towards random baseline bias HIGHER in sertraline group over all sessions\n",
    "    <br><b>GAD:</b>\n",
    "    - at 2 weeks:\n",
    "        - there is a very small trend that the random baseline bias NEGATIVELY associated with gad\n",
    "    - at 6 weeks:\n",
    "        - - \n",
    "    - over all sessions:\n",
    "        - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "Change in parameter\n",
    "--\n",
    "early changes in cognitive processing predicting treatment outcome?\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
      "<ipython-input-158-ceb4565420ca>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n"
     ]
    }
   ],
   "source": [
    "for col in df_panda.columns:\n",
    "    col_zscore = col + '_zscore'\n",
    "    try:\n",
    "        df_panda[col_zscore] = (df_panda[col] - df_panda[col].mean())/df_panda[col].std(ddof=0)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
=======
   "execution_count": 50,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(3)\n",
    "for i in parameter_labels:\n",
    "    df_panda[i + '_slope12'] = df_panda[i + str(2)] - df_panda[i + str(1)]\n",
    "    df_panda[i + '_slope23'] = df_panda[i + str(3)] - df_panda[i + str(2)]\n",
    "    df_panda[i + '_slope13'] = df_panda[i + str(3)] - df_panda[i + str(1)]\n",
    "    for sj in range(len(df_panda)):\n",
    "        y = df_panda[[i + str(1),i + str(2),i + str(3)]].iloc[sj].astype('float64')\n",
    "        idx = ~y.isna()\n",
    "        if sum(idx) > 1:\n",
<<<<<<< HEAD
    "            df_panda.loc[sj, i + '_slope'] = np.polyfit(x[idx],y[idx],1)[0]"
=======
    "            df_panda.loc[sj, i + '_slope'] = np.polyfit(x[idx],y[idx],1)[0]\n",
    "            \n",
    "questionnaires = ['gad', 'phq']\n",
    "for i in questionnaires:\n",
    "    df_panda[i + 'log_slope'] = df_panda[i + '4log'] - df_panda[i + '1log']"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 72,
=======
   "execution_count": 47,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "group: \tbeta: 0.13,\tCI: [-0.25,0.51],\tpvalue: 0.5096\n",
      "group: \tbeta: -0.32,\tCI: [-0.62,-0.01],\tpvalue: 0.0408\n",
      "group: \tbeta: -0.02,\tCI: [-0.45,0.4],\tpvalue: 0.9109\n",
      "group: \tbeta: 0.75,\tCI: [0.18,1.31],\tpvalue: 0.0093\n",
      "group: \tbeta: 0.11,\tCI: [-0.07,0.3],\tpvalue: 0.2245\n",
      "group: \tbeta: -0.08,\tCI: [-0.32,0.15],\tpvalue: 0.4904\n",
      "group: \tbeta: -0.27,\tCI: [-0.67,0.13],\tpvalue: 0.1915\n",
      "group: \tbeta: 0.02,\tCI: [-0.28,0.32],\tpvalue: 0.8853\n"
=======
      "group: \tbeta: 0.16,\tCI: [-0.23,0.54],\tpvalue: 0.4199\n",
      "group: \tbeta: -0.3,\tCI: [-0.61,-0.0],\tpvalue: 0.0493\n",
      "group: \tbeta: -0.04,\tCI: [-0.46,0.38],\tpvalue: 0.8538\n",
      "group: \tbeta: 0.73,\tCI: [0.17,1.29],\tpvalue: 0.0108\n",
      "group: \tbeta: 0.12,\tCI: [-0.06,0.3],\tpvalue: 0.2017\n",
      "group: \tbeta: -0.08,\tCI: [-0.32,0.15],\tpvalue: 0.4944\n",
      "group: \tbeta: -0.28,\tCI: [-0.68,0.12],\tpvalue: 0.1758\n",
      "group: \tbeta: 0.02,\tCI: [-0.27,0.32],\tpvalue: 0.8731\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "for i in parameter_labels[:-1]:\n",
    "    df_panda[i + '_slope12'] = pd.to_numeric(df_panda[i + '_slope12'])\n",
    "    panda.glm(i + '_slope12 ~ group' + stratification_covariates + exclusion_covariates, \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], ['group'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 73,
=======
   "execution_count": 48,
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "group: \tbeta: -0.41,\tCI: [-0.79,-0.03],\tpvalue: 0.0365\n",
      "group: \tbeta: 0.26,\tCI: [-0.05,0.57],\tpvalue: 0.1048\n",
      "group: \tbeta: 0.16,\tCI: [-0.29,0.6],\tpvalue: 0.4923\n",
      "group: \tbeta: -0.72,\tCI: [-1.28,-0.17],\tpvalue: 0.0108\n",
      "group: \tbeta: -0.14,\tCI: [-0.32,0.04],\tpvalue: 0.1231\n",
      "group: \tbeta: 0.09,\tCI: [-0.16,0.34],\tpvalue: 0.4934\n",
      "group: \tbeta: 0.33,\tCI: [-0.06,0.73],\tpvalue: 0.0993\n",
      "group: \tbeta: 0.09,\tCI: [-0.23,0.41],\tpvalue: 0.5852\n"
=======
      "group: \tbeta: -0.43,\tCI: [-0.81,-0.04],\tpvalue: 0.0287\n",
      "group: \tbeta: 0.24,\tCI: [-0.07,0.55],\tpvalue: 0.1283\n",
      "group: \tbeta: 0.16,\tCI: [-0.28,0.6],\tpvalue: 0.4768\n",
      "group: \tbeta: -0.68,\tCI: [-1.24,-0.13],\tpvalue: 0.0153\n",
      "group: \tbeta: -0.13,\tCI: [-0.31,0.04],\tpvalue: 0.1417\n",
      "group: \tbeta: 0.1,\tCI: [-0.15,0.34],\tpvalue: 0.4527\n",
      "group: \tbeta: 0.33,\tCI: [-0.06,0.72],\tpvalue: 0.1019\n",
      "group: \tbeta: 0.1,\tCI: [-0.22,0.42],\tpvalue: 0.5308\n"
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
     ]
    }
   ],
   "source": [
    "for i in parameter_labels[:-1]:\n",
    "    df_panda[i + '_slope23'] = pd.to_numeric(df_panda[i + '_slope23'])\n",
    "    panda.glm(i + '_slope23 ~ '+i+'1 + group' + stratification_covariates + exclusion_covariates, \\\n",
    "                        df_panda[(df_panda['exclusiontot2']==0)&(df_panda['exclusiontot3']==0)], ['group'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se_slope12: \tbeta: 0.01,\tCI: [-0.02,0.05],\tpvalue: 0.4667\n",
      "loss_se_slope12: \tbeta: 0.02,\tCI: [-0.02,0.07],\tpvalue: 0.3215\n",
      "rew_LR_slope12: \tbeta: -0.04,\tCI: [-0.07,-0.01],\tpvalue: 0.0085\n",
      "loss_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1756\n",
      "app_Pav_slope12: \tbeta: 0.01,\tCI: [-0.07,0.09],\tpvalue: 0.8146\n",
      "av_Pav_slope12: \tbeta: 0.03,\tCI: [-0.03,0.09],\tpvalue: 0.2625\n",
      "noise_slope12: \tbeta: 0.03,\tCI: [-0.01,0.06],\tpvalue: 0.1319\n",
      "bias_slope12: \tbeta: 0.01,\tCI: [-0.03,0.06],\tpvalue: 0.5404\n",
      "rew_se_slope12: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.8696\n",
      "loss_se_slope12: \tbeta: 0.04,\tCI: [-0.01,0.08],\tpvalue: 0.1206\n",
      "rew_LR_slope12: \tbeta: -0.03,\tCI: [-0.06,0.0],\tpvalue: 0.0971\n",
      "loss_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1206\n",
      "app_Pav_slope12: \tbeta: -0.04,\tCI: [-0.12,0.03],\tpvalue: 0.2425\n",
      "av_Pav_slope12: \tbeta: 0.06,\tCI: [0.0,0.12],\tpvalue: 0.038\n",
      "noise_slope12: \tbeta: 0.04,\tCI: [0.01,0.07],\tpvalue: 0.022\n",
      "bias_slope12: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.6716\n"
     ]
    }
   ],
   "source": [
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('gadlog_slope ~' + i + '_slope12 + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])\n",
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('phqlog_slope ~' + i + '_slope12 + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12516\n",
      "Method:                          IRLS   Log-Likelihood:                -70.694\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.780\n",
      "Time:                        15:06:40   Pearson chi2:                     23.8\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04092\n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -0.1837      0.216     -0.850      0.395      -0.607       0.240\n",
      "rew_se_slope12           0.0132      0.025      0.536      0.592      -0.035       0.062\n",
      "group                   -0.0387      0.051     -0.757      0.449      -0.139       0.061\n",
      "rew_se_slope12:group    -0.0278      0.037     -0.748      0.455      -0.101       0.045\n",
      "site                     0.0075      0.024      0.314      0.753      -0.039       0.054\n",
      "cis                     -0.0351      0.034     -1.030      0.303      -0.102       0.032\n",
      "dep                      0.1328      0.056      2.390      0.017       0.024       0.242\n",
      "age                     -0.0025      0.002     -1.040      0.298      -0.007       0.002\n",
      "education                0.0053      0.068      0.079      0.937      -0.127       0.138\n",
      "AD_past                 -0.0237      0.052     -0.458      0.647      -0.125       0.078\n",
      "marstat                 -0.0253      0.044     -0.582      0.560      -0.111       0.060\n",
      "========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12469\n",
      "Method:                          IRLS   Log-Likelihood:                -70.317\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.691\n",
      "Time:                        15:06:40   Pearson chi2:                     23.7\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04446\n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                -0.2286      0.220     -1.038      0.299      -0.660       0.203\n",
      "loss_se_slope12           0.0313      0.032      0.988      0.323      -0.031       0.093\n",
      "group                    -0.0335      0.052     -0.648      0.517      -0.135       0.068\n",
      "loss_se_slope12:group    -0.0116      0.047     -0.249      0.804      -0.103       0.080\n",
      "site                      0.0085      0.024      0.358      0.721      -0.038       0.055\n",
      "cis                      -0.0345      0.034     -1.012      0.312      -0.101       0.032\n",
      "dep                       0.1316      0.055      2.394      0.017       0.024       0.239\n",
      "age                      -0.0022      0.002     -0.901      0.367      -0.007       0.003\n",
      "education                 0.0105      0.068      0.154      0.878      -0.123       0.144\n",
      "AD_past                  -0.0206      0.052     -0.397      0.691      -0.122       0.081\n",
      "marstat                  -0.0191      0.043     -0.440      0.660      -0.104       0.066\n",
      "=========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12197\n",
      "Method:                          IRLS   Log-Likelihood:                -68.104\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.175\n",
      "Time:                        15:06:40   Pearson chi2:                     23.2\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.06528\n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -0.1381      0.213     -0.648      0.517      -0.556       0.280\n",
      "rew_LR_slope12          -0.0272      0.019     -1.406      0.160      -0.065       0.011\n",
      "group                   -0.0384      0.051     -0.759      0.448      -0.138       0.061\n",
      "rew_LR_slope12:group    -0.0190      0.031     -0.605      0.545      -0.080       0.042\n",
      "site                     0.0080      0.024      0.339      0.735      -0.038       0.054\n",
      "cis                     -0.0386      0.034     -1.141      0.254      -0.105       0.028\n",
      "dep                      0.1362      0.054      2.506      0.012       0.030       0.243\n",
      "age                     -0.0027      0.002     -1.133      0.257      -0.007       0.002\n",
      "education               -0.0169      0.067     -0.251      0.802      -0.149       0.115\n",
      "AD_past                 -0.0307      0.051     -0.600      0.549      -0.131       0.070\n",
      "marstat                 -0.0224      0.043     -0.526      0.599      -0.106       0.061\n",
      "========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12435\n",
      "Method:                          IRLS   Log-Likelihood:                -70.039\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.626\n",
      "Time:                        15:06:40   Pearson chi2:                     23.6\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04708\n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                -0.2140      0.216     -0.992      0.321      -0.637       0.209\n",
      "loss_LR_slope12          -0.0148      0.017     -0.865      0.387      -0.048       0.019\n",
      "group                    -0.0317      0.051     -0.620      0.535      -0.132       0.068\n",
      "loss_LR_slope12:group    -0.0056      0.026     -0.217      0.828      -0.057       0.045\n",
      "site                      0.0092      0.024      0.383      0.701      -0.038       0.056\n",
      "cis                      -0.0343      0.034     -1.009      0.313      -0.101       0.032\n",
      "dep                       0.1329      0.055      2.420      0.016       0.025       0.241\n",
      "age                      -0.0025      0.002     -1.027      0.304      -0.007       0.002\n",
      "education                 0.0110      0.067      0.163      0.871      -0.121       0.143\n",
      "AD_past                  -0.0192      0.052     -0.369      0.712      -0.121       0.083\n",
      "marstat                  -0.0212      0.043     -0.493      0.622      -0.106       0.063\n",
      "=========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12486\n",
      "Method:                          IRLS   Log-Likelihood:                -70.454\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.723\n",
      "Time:                        15:06:40   Pearson chi2:                     23.7\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04318\n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                -0.2087      0.218     -0.960      0.337      -0.635       0.218\n",
      "app_Pav_slope12           0.0217      0.053      0.410      0.682      -0.082       0.126\n",
      "group                    -0.0444      0.051     -0.863      0.388      -0.145       0.056\n",
      "app_Pav_slope12:group    -0.0723      0.076     -0.950      0.342      -0.221       0.077\n",
      "site                      0.0090      0.024      0.375      0.708      -0.038       0.056\n",
      "cis                      -0.0327      0.034     -0.952      0.341      -0.100       0.035\n",
      "dep                       0.1282      0.055      2.332      0.020       0.020       0.236\n",
      "age                      -0.0020      0.002     -0.842      0.400      -0.007       0.003\n",
      "education                 0.0059      0.067      0.088      0.930      -0.126       0.138\n",
      "AD_past                  -0.0223      0.052     -0.430      0.667      -0.124       0.079\n",
      "marstat                  -0.0215      0.043     -0.499      0.618      -0.106       0.063\n",
      "=========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12495\n",
      "Method:                          IRLS   Log-Likelihood:                -70.528\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.741\n",
      "Time:                        15:06:40   Pearson chi2:                     23.7\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04248\n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -0.1940      0.215     -0.902      0.367      -0.616       0.228\n",
      "av_Pav_slope12           0.0148      0.040      0.365      0.715      -0.065       0.094\n",
      "group                   -0.0400      0.051     -0.785      0.433      -0.140       0.060\n",
      "av_Pav_slope12:group     0.0273      0.063      0.433      0.665      -0.096       0.151\n",
      "site                     0.0086      0.024      0.361      0.718      -0.038       0.056\n",
      "cis                     -0.0329      0.034     -0.963      0.336      -0.100       0.034\n",
      "dep                      0.1252      0.055      2.276      0.023       0.017       0.233\n",
      "age                     -0.0024      0.002     -0.984      0.325      -0.007       0.002\n",
      "education                0.0038      0.067      0.056      0.955      -0.128       0.136\n",
      "AD_past                 -0.0189      0.052     -0.363      0.716      -0.121       0.083\n",
      "marstat                 -0.0232      0.043     -0.538      0.591      -0.108       0.061\n",
      "========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12122\n",
      "Method:                          IRLS   Log-Likelihood:                -67.477\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.031\n",
      "Time:                        15:06:40   Pearson chi2:                     23.0\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.07118\n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept              -0.2411      0.213     -1.134      0.257      -0.658       0.176\n",
      "noise_slope12           0.0235      0.025      0.924      0.355      -0.026       0.073\n",
      "group                  -0.0350      0.050     -0.693      0.488      -0.134       0.064\n",
      "noise_slope12:group     0.0413      0.037      1.115      0.265      -0.031       0.114\n",
      "site                    0.0116      0.024      0.492      0.622      -0.035       0.058\n",
      "cis                    -0.0300      0.034     -0.894      0.372      -0.096       0.036\n",
      "dep                     0.1317      0.054      2.427      0.015       0.025       0.238\n",
      "age                    -0.0020      0.002     -0.855      0.393      -0.007       0.003\n",
      "education               0.0075      0.066      0.113      0.910      -0.122       0.137\n",
      "AD_past                -0.0219      0.051     -0.431      0.667      -0.122       0.078\n",
      "marstat                -0.0195      0.042     -0.459      0.646      -0.103       0.064\n",
      "=======================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           gadlog_slope   No. Observations:                  201\n",
      "Model:                            GLM   Df Residuals:                      190\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.12255\n",
      "Method:                          IRLS   Log-Likelihood:                -68.574\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       23.284\n",
      "Time:                        15:06:40   Pearson chi2:                     23.3\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.06086\n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "Intercept             -0.2051      0.213     -0.964      0.335      -0.622       0.212\n",
      "bias_slope12           0.0628      0.031      2.008      0.045       0.002       0.124\n",
      "group                 -0.0173      0.052     -0.336      0.737      -0.119       0.084\n",
      "bias_slope12:group    -0.0861      0.044     -1.968      0.049      -0.172      -0.000\n",
      "site                   0.0072      0.024      0.306      0.759      -0.039       0.054\n",
      "cis                   -0.0379      0.034     -1.125      0.260      -0.104       0.028\n",
      "dep                    0.1289      0.054      2.366      0.018       0.022       0.236\n",
      "age                   -0.0023      0.002     -0.960      0.337      -0.007       0.002\n",
      "education              0.0043      0.066      0.065      0.948      -0.126       0.135\n",
      "AD_past               -0.0230      0.051     -0.449      0.653      -0.123       0.077\n",
      "marstat               -0.0206      0.043     -0.481      0.630      -0.104       0.063\n",
      "======================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.11027\n",
      "Method:                          IRLS   Log-Likelihood:                -58.603\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       21.172\n",
      "Time:                        15:06:40   Pearson chi2:                     21.2\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.03864\n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -0.2239      0.202     -1.107      0.268      -0.620       0.173\n",
      "rew_se_slope12          -0.0124      0.023     -0.536      0.592      -0.058       0.033\n",
      "group                   -0.0117      0.048     -0.245      0.807      -0.105       0.082\n",
      "rew_se_slope12:group     0.0015      0.035      0.043      0.966      -0.067       0.070\n",
      "site                     0.0387      0.022      1.743      0.081      -0.005       0.082\n",
      "cis                     -0.0503      0.032     -1.578      0.115      -0.113       0.012\n",
      "dep                      0.0260      0.052      0.502      0.616      -0.076       0.127\n",
      "age                     -0.0005      0.002     -0.232      0.817      -0.005       0.004\n",
      "education                0.0147      0.063      0.232      0.817      -0.110       0.139\n",
      "AD_past                 -0.0044      0.048     -0.092      0.927      -0.099       0.090\n",
      "marstat                  0.0109      0.041      0.266      0.790      -0.069       0.091\n",
      "========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.10940\n",
      "Method:                          IRLS   Log-Likelihood:                -57.798\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       21.005\n",
      "Time:                        15:06:40   Pearson chi2:                     21.0\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04614\n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                -0.3000      0.206     -1.460      0.144      -0.703       0.103\n",
      "loss_se_slope12           0.0368      0.030      1.242      0.214      -0.021       0.095\n",
      "group                    -0.0058      0.048     -0.119      0.905      -0.100       0.089\n",
      "loss_se_slope12:group    -0.0140      0.044     -0.320      0.749      -0.100       0.072\n",
      "site                      0.0412      0.022      1.864      0.062      -0.002       0.085\n",
      "cis                      -0.0478      0.032     -1.504      0.133      -0.110       0.015\n",
      "dep                       0.0339      0.051      0.665      0.506      -0.066       0.134\n",
      "age                      -0.0002      0.002     -0.085      0.933      -0.005       0.004\n",
      "education                 0.0286      0.064      0.448      0.654      -0.097       0.154\n",
      "AD_past                  -0.0009      0.048     -0.018      0.985      -0.095       0.094\n",
      "marstat                   0.0174      0.041      0.428      0.668      -0.062       0.097\n",
      "=========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.10899\n",
      "Method:                          IRLS   Log-Likelihood:                -57.419\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       20.927\n",
      "Time:                        15:06:40   Pearson chi2:                     20.9\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04968\n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -0.2165      0.201     -1.078      0.281      -0.610       0.177\n",
      "rew_LR_slope12          -0.0182      0.018     -1.002      0.316      -0.054       0.017\n",
      "group                   -0.0135      0.048     -0.284      0.777      -0.107       0.080\n",
      "rew_LR_slope12:group    -0.0122      0.030     -0.414      0.679      -0.070       0.046\n",
      "site                     0.0404      0.022      1.830      0.067      -0.003       0.084\n",
      "cis                     -0.0509      0.032     -1.596      0.111      -0.113       0.012\n",
      "dep                      0.0359      0.051      0.704      0.482      -0.064       0.136\n",
      "age                     -0.0006      0.002     -0.280      0.779      -0.005       0.004\n",
      "education                0.0053      0.064      0.084      0.933      -0.119       0.130\n",
      "AD_past                 -0.0092      0.048     -0.193      0.847      -0.103       0.085\n",
      "marstat                  0.0133      0.040      0.331      0.741      -0.066       0.092\n",
      "========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.10933\n",
      "Method:                          IRLS   Log-Likelihood:                -57.728\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       20.991\n",
      "Time:                        15:06:40   Pearson chi2:                     21.0\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04680\n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                -0.2754      0.202     -1.365      0.172      -0.671       0.120\n",
      "loss_LR_slope12          -0.0112      0.016     -0.704      0.482      -0.043       0.020\n",
      "group                    -0.0048      0.048     -0.101      0.919      -0.098       0.089\n",
      "loss_LR_slope12:group    -0.0126      0.024     -0.515      0.606      -0.060       0.035\n",
      "site                      0.0414      0.022      1.866      0.062      -0.002       0.085\n",
      "cis                      -0.0481      0.032     -1.516      0.130      -0.110       0.014\n",
      "dep                       0.0349      0.051      0.683      0.494      -0.065       0.135\n",
      "age                      -0.0005      0.002     -0.203      0.839      -0.005       0.004\n",
      "education                 0.0278      0.063      0.439      0.660      -0.096       0.152\n",
      "AD_past                  -0.0020      0.048     -0.041      0.968      -0.097       0.093\n",
      "marstat                   0.0149      0.040      0.370      0.712      -0.064       0.094\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.10848\n",
      "Method:                          IRLS   Log-Likelihood:                -56.938\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       20.828\n",
      "Time:                        15:06:40   Pearson chi2:                     20.8\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.05416\n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                -0.3073      0.202     -1.520      0.128      -0.704       0.089\n",
      "app_Pav_slope12          -0.0174      0.049     -0.356      0.722      -0.113       0.078\n",
      "group                    -0.0120      0.048     -0.252      0.801      -0.106       0.081\n",
      "app_Pav_slope12:group    -0.0794      0.070     -1.126      0.260      -0.217       0.059\n",
      "site                      0.0438      0.022      1.981      0.048       0.000       0.087\n",
      "cis                      -0.0412      0.032     -1.291      0.197      -0.104       0.021\n",
      "dep                       0.0272      0.051      0.536      0.592      -0.072       0.127\n",
      "age                       0.0003      0.002      0.137      0.891      -0.004       0.005\n",
      "education                 0.0221      0.063      0.353      0.724      -0.101       0.145\n",
      "AD_past                   0.0002      0.048      0.005      0.996      -0.094       0.094\n",
      "marstat                   0.0133      0.040      0.331      0.741      -0.065       0.092\n",
      "=========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.10710\n",
      "Method:                          IRLS   Log-Likelihood:                -55.640\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       20.563\n",
      "Time:                        15:06:40   Pearson chi2:                     20.6\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.06625\n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               -0.2842      0.199     -1.431      0.152      -0.674       0.105\n",
      "av_Pav_slope12           0.0223      0.037      0.595      0.552      -0.051       0.096\n",
      "group                   -0.0111      0.047     -0.235      0.814      -0.103       0.081\n",
      "av_Pav_slope12:group     0.0866      0.058      1.485      0.138      -0.028       0.201\n",
      "site                     0.0438      0.022      1.997      0.046       0.001       0.087\n",
      "cis                     -0.0422      0.032     -1.339      0.181      -0.104       0.020\n",
      "dep                      0.0235      0.051      0.465      0.642      -0.076       0.123\n",
      "age                     -0.0004      0.002     -0.167      0.867      -0.005       0.004\n",
      "education                0.0247      0.062      0.397      0.691      -0.097       0.147\n",
      "AD_past                  0.0076      0.048      0.159      0.873      -0.086       0.101\n",
      "marstat                  0.0121      0.040      0.303      0.762      -0.066       0.090\n",
      "========================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.10631\n",
      "Method:                          IRLS   Log-Likelihood:                -54.888\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       20.411\n",
      "Time:                        15:06:40   Pearson chi2:                     20.4\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.07326\n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept              -0.2989      0.199     -1.506      0.132      -0.688       0.090\n",
      "noise_slope12           0.0426      0.024      1.800      0.072      -0.004       0.089\n",
      "group                  -0.0043      0.047     -0.091      0.928      -0.096       0.088\n",
      "noise_slope12:group     0.0098      0.035      0.283      0.778      -0.058       0.077\n",
      "site                    0.0433      0.022      1.986      0.047       0.001       0.086\n",
      "cis                    -0.0433      0.031     -1.382      0.167      -0.105       0.018\n",
      "dep                     0.0306      0.050      0.608      0.543      -0.068       0.129\n",
      "age                 -8.468e-05      0.002     -0.038      0.969      -0.004       0.004\n",
      "education               0.0192      0.062      0.309      0.758      -0.103       0.141\n",
      "AD_past                -0.0027      0.047     -0.058      0.954      -0.096       0.090\n",
      "marstat                 0.0170      0.040      0.427      0.670      -0.061       0.095\n",
      "=======================================================================================\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:           phqlog_slope   No. Observations:                  203\n",
      "Model:                            GLM   Df Residuals:                      192\n",
      "Model Family:                Gaussian   Df Model:                           10\n",
      "Link Function:               identity   Scale:                         0.10942\n",
      "Method:                          IRLS   Log-Likelihood:                -57.816\n",
      "Date:                Mon, 04 Jul 2022   Deviance:                       21.009\n",
      "Time:                        15:06:40   Pearson chi2:                     21.0\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.04598\n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "Intercept             -0.2560      0.200     -1.277      0.201      -0.649       0.137\n",
      "bias_slope12           0.0376      0.029      1.282      0.200      -0.020       0.095\n",
      "group                  0.0005      0.049      0.010      0.992      -0.095       0.096\n",
      "bias_slope12:group    -0.0540      0.041     -1.311      0.190      -0.135       0.027\n",
      "site                   0.0393      0.022      1.778      0.075      -0.004       0.083\n",
      "cis                   -0.0507      0.032     -1.599      0.110      -0.113       0.011\n",
      "dep                    0.0294      0.051      0.575      0.565      -0.071       0.129\n",
      "age                   -0.0003      0.002     -0.147      0.883      -0.005       0.004\n",
      "education              0.0191      0.063      0.304      0.761      -0.104       0.142\n",
      "AD_past               -0.0049      0.048     -0.102      0.919      -0.099       0.089\n",
      "marstat                0.0146      0.040      0.362      0.717      -0.065       0.094\n",
      "======================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('gadlog_slope ~' + i + '_slope12 * group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [])\n",
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('phqlog_slope ~' + i + '_slope12 * group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "ax = sns.regplot(x=\"av_Pav_slope12\", y=\"phq_slope\", data=df_panda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rew_se_slope12: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.8145\n",
      "loss_se_slope12: \tbeta: -0.0,\tCI: [-0.04,0.04],\tpvalue: 0.909\n",
      "rew_LR_slope12: \tbeta: -0.02,\tCI: [-0.04,0.01],\tpvalue: 0.1825\n",
      "loss_LR_slope12: \tbeta: 0.0,\tCI: [-0.02,0.02],\tpvalue: 0.9718\n",
      "app_Pav_slope12: \tbeta: -0.01,\tCI: [-0.07,0.06],\tpvalue: 0.7703\n",
      "av_Pav_slope12: \tbeta: 0.02,\tCI: [-0.04,0.07],\tpvalue: 0.5384\n",
      "noise_slope12: \tbeta: 0.01,\tCI: [-0.02,0.04],\tpvalue: 0.4764\n",
      "bias_slope12: \tbeta: 0.0,\tCI: [-0.03,0.04],\tpvalue: 0.816\n"
     ]
    }
   ],
   "source": [
    "# early changes in cognitive processing predicting treatment outcome?\n",
    "for i in parameter_labels[:-1]:\n",
    "    panda.glm('gad3log ~' + i + '_slope12 + gad1log + group + site + cis + dep + age + education + AD_past + marstat', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> bc4b59b2d8d46102f211f3aeb35950b647615087
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in psychiatric_questionnaires:\n",
    "    df_panda[i + 'log_slope'] = df_panda[i + '4log'] - df_panda[i + '1log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      "gad:\n",
      "rew_se_slope12:group: \tbeta: -0.01,\tCI: [-0.08,0.06],\tpvalue: 0.7916\n",
      "loss_se_slope12:group: \tbeta: -0.03,\tCI: [-0.13,0.06],\tpvalue: 0.4851\n",
      "rew_LR_slope12:group: \tbeta: -0.0,\tCI: [-0.07,0.06],\tpvalue: 0.9311\n",
      "loss_LR_slope12:group: \tbeta: 0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9463\n",
      "app_Pav_slope12:group: \tbeta: -0.06,\tCI: [-0.22,0.09],\tpvalue: 0.4381\n",
      "av_Pav_slope12:group: \tbeta: 0.05,\tCI: [-0.07,0.17],\tpvalue: 0.38\n",
      "noise_slope12:group: \tbeta: 0.02,\tCI: [-0.05,0.09],\tpvalue: 0.5579\n",
      "bias_slope12:group: \tbeta: -0.1,\tCI: [-0.2,-0.01],\tpvalue: 0.0255\n",
      "##############################################################################\n",
      "phq:\n",
      "rew_se_slope12:group: \tbeta: -0.0,\tCI: [-0.07,0.07],\tpvalue: 0.996\n",
      "loss_se_slope12:group: \tbeta: -0.03,\tCI: [-0.12,0.05],\tpvalue: 0.4654\n",
      "rew_LR_slope12:group: \tbeta: 0.01,\tCI: [-0.05,0.07],\tpvalue: 0.8469\n",
      "loss_LR_slope12:group: \tbeta: -0.0,\tCI: [-0.05,0.05],\tpvalue: 0.8989\n",
      "app_Pav_slope12:group: \tbeta: -0.09,\tCI: [-0.23,0.06],\tpvalue: 0.2294\n",
      "av_Pav_slope12:group: \tbeta: 0.09,\tCI: [-0.03,0.2],\tpvalue: 0.1328\n",
      "noise_slope12:group: \tbeta: 0.01,\tCI: [-0.06,0.07],\tpvalue: 0.8116\n",
      "bias_slope12:group: \tbeta: -0.06,\tCI: [-0.14,0.03],\tpvalue: 0.2053\n",
      "##############################################################################\n",
      "bdi:\n",
      "rew_se_slope12:group: \tbeta: -0.03,\tCI: [-0.1,0.04],\tpvalue: 0.4282\n",
      "loss_se_slope12:group: \tbeta: -0.04,\tCI: [-0.13,0.05],\tpvalue: 0.3906\n",
      "rew_LR_slope12:group: \tbeta: 0.0,\tCI: [-0.06,0.07],\tpvalue: 0.8913\n",
      "loss_LR_slope12:group: \tbeta: -0.01,\tCI: [-0.06,0.04],\tpvalue: 0.8098\n",
      "app_Pav_slope12:group: \tbeta: -0.1,\tCI: [-0.25,0.06],\tpvalue: 0.2125\n",
      "av_Pav_slope12:group: \tbeta: 0.14,\tCI: [0.03,0.26],\tpvalue: 0.0167\n",
      "noise_slope12:group: \tbeta: 0.02,\tCI: [-0.04,0.09],\tpvalue: 0.4901\n",
      "bias_slope12:group: \tbeta: -0.05,\tCI: [-0.14,0.04],\tpvalue: 0.3179\n"
     ]
    }
   ],
   "source": [
    "for j in psychiatric_questionnaires:\n",
    "    print('##############################################################################')\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.glm(j + 'log_slope ~' + i + '_slope12 * group + age + education + site', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12:group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placebo: av_Pav_slope12: \tbeta: 0.0,\tCI: [-0.08,0.08],\tpvalue: 0.9381\n",
      "sertraline: av_Pav_slope12: \tbeta: 0.15,\tCI: [0.06,0.24],\tpvalue: 0.0006\n"
     ]
    }
   ],
   "source": [
    "j = psychiatric_questionnaires[2]\n",
    "i = parameter_labels[5]\n",
    "for g in range(2):\n",
    "    print(group_label[g], end=': ')\n",
    "    tmp = df_panda['group'] == g\n",
    "    panda.glm(j + 'log_slope ~' + i + '_slope12 + age + education + site', \\\n",
    "                    df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)&tmp], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      "gad:\n",
      "rew_se_slope12:group: \tbeta: 0.02,\tCI: [-0.05,0.08],\tpvalue: 0.5884\n",
      "loss_se_slope12:group: \tbeta: 0.07,\tCI: [-0.01,0.15],\tpvalue: 0.1062\n",
      "rew_LR_slope12:group: \tbeta: -0.01,\tCI: [-0.07,0.04],\tpvalue: 0.6258\n",
      "loss_LR_slope12:group: \tbeta: -0.04,\tCI: [-0.08,0.01],\tpvalue: 0.1155\n",
      "app_Pav_slope12:group: \tbeta: 0.03,\tCI: [-0.1,0.16],\tpvalue: 0.7017\n",
      "av_Pav_slope12:group: \tbeta: 0.06,\tCI: [-0.05,0.16],\tpvalue: 0.2928\n",
      "noise_slope12:group: \tbeta: 0.02,\tCI: [-0.04,0.08],\tpvalue: 0.523\n",
      "bias_slope12:group: \tbeta: -0.02,\tCI: [-0.1,0.06],\tpvalue: 0.5684\n",
      "##############################################################################\n",
      "phq:\n",
      "rew_se_slope12:group: \tbeta: -0.02,\tCI: [-0.08,0.03],\tpvalue: 0.4493\n",
      "loss_se_slope12:group: \tbeta: 0.04,\tCI: [-0.03,0.11],\tpvalue: 0.2606\n",
      "rew_LR_slope12:group: \tbeta: 0.01,\tCI: [-0.03,0.06],\tpvalue: 0.5532\n",
      "loss_LR_slope12:group: \tbeta: -0.04,\tCI: [-0.07,-0.0],\tpvalue: 0.0484\n",
      "app_Pav_slope12:group: \tbeta: -0.06,\tCI: [-0.17,0.04],\tpvalue: 0.2471\n",
      "av_Pav_slope12:group: \tbeta: 0.12,\tCI: [0.03,0.2],\tpvalue: 0.0066\n",
      "noise_slope12:group: \tbeta: 0.02,\tCI: [-0.03,0.07],\tpvalue: 0.3775\n",
      "bias_slope12:group: \tbeta: 0.02,\tCI: [-0.04,0.09],\tpvalue: 0.4694\n",
      "##############################################################################\n",
      "bdi:\n",
      "rew_se_slope12:group: \tbeta: -0.02,\tCI: [-0.08,0.05],\tpvalue: 0.641\n",
      "loss_se_slope12:group: \tbeta: 0.01,\tCI: [-0.07,0.09],\tpvalue: 0.8382\n",
      "rew_LR_slope12:group: \tbeta: -0.01,\tCI: [-0.06,0.05],\tpvalue: 0.828\n",
      "loss_LR_slope12:group: \tbeta: -0.02,\tCI: [-0.06,0.03],\tpvalue: 0.4547\n",
      "app_Pav_slope12:group: \tbeta: -0.04,\tCI: [-0.17,0.09],\tpvalue: 0.5851\n",
      "av_Pav_slope12:group: \tbeta: 0.07,\tCI: [-0.03,0.17],\tpvalue: 0.1797\n",
      "noise_slope12:group: \tbeta: 0.02,\tCI: [-0.04,0.08],\tpvalue: 0.5598\n",
      "bias_slope12:group: \tbeta: 0.04,\tCI: [-0.04,0.12],\tpvalue: 0.3337\n"
     ]
    }
   ],
   "source": [
    "# early changes in cognitive processing predicting treatment outcome at week 6?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('##############################################################################')\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.glm(j + '3log ~' + i + '_slope12 * group + ' + j + '1log + age + education + site', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12:group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_Pav_slope12:group: \tbeta: 0.04,\tCI: [-0.03,0.1],\tpvalue: 0.2607\n",
      "placebo: av_Pav_slope12: \tbeta: 0.01,\tCI: [-0.04,0.05],\tpvalue: 0.709\n",
      "sertraline: av_Pav_slope12: \tbeta: 0.05,\tCI: [0.0,0.09],\tpvalue: 0.0352\n"
     ]
    }
   ],
   "source": [
    "i = parameter_labels[5]\n",
    "panda.glm('phq3log ~' + i + '_slope12 * group + phq1log + age + education + site', \\\n",
    "                df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12:group'])\n",
    "for g in range(2):\n",
    "    print(group_label[g], end=': ')\n",
    "    tmp = df_panda['group'] == g\n",
    "    panda.glm('phq3log ~' + i + '_slope12 + phq1log + age + education + site', \\\n",
    "                    df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)&tmp], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      "gad:\n",
      "rew_se_slope12:group: \tbeta: -0.01,\tCI: [-0.08,0.06],\tpvalue: 0.8025\n",
      "loss_se_slope12:group: \tbeta: -0.04,\tCI: [-0.13,0.05],\tpvalue: 0.3363\n",
      "rew_LR_slope12:group: \tbeta: -0.01,\tCI: [-0.08,0.05],\tpvalue: 0.6395\n",
      "loss_LR_slope12:group: \tbeta: 0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9791\n",
      "app_Pav_slope12:group: \tbeta: -0.05,\tCI: [-0.21,0.1],\tpvalue: 0.4839\n",
      "av_Pav_slope12:group: \tbeta: 0.06,\tCI: [-0.06,0.17],\tpvalue: 0.3511\n",
      "noise_slope12:group: \tbeta: 0.02,\tCI: [-0.05,0.09],\tpvalue: 0.5977\n",
      "bias_slope12:group: \tbeta: -0.09,\tCI: [-0.18,-0.01],\tpvalue: 0.0382\n",
      "##############################################################################\n",
      "phq:\n",
      "rew_se_slope12:group: \tbeta: 0.01,\tCI: [-0.06,0.08],\tpvalue: 0.7759\n",
      "loss_se_slope12:group: \tbeta: -0.04,\tCI: [-0.12,0.05],\tpvalue: 0.3667\n",
      "rew_LR_slope12:group: \tbeta: -0.01,\tCI: [-0.07,0.05],\tpvalue: 0.7273\n",
      "loss_LR_slope12:group: \tbeta: -0.0,\tCI: [-0.05,0.05],\tpvalue: 0.9677\n",
      "app_Pav_slope12:group: \tbeta: -0.08,\tCI: [-0.22,0.06],\tpvalue: 0.2631\n",
      "av_Pav_slope12:group: \tbeta: 0.08,\tCI: [-0.03,0.19],\tpvalue: 0.1582\n",
      "noise_slope12:group: \tbeta: 0.01,\tCI: [-0.05,0.07],\tpvalue: 0.7903\n",
      "bias_slope12:group: \tbeta: -0.06,\tCI: [-0.14,0.03],\tpvalue: 0.2006\n",
      "##############################################################################\n",
      "bdi:\n",
      "rew_se_slope12:group: \tbeta: -0.03,\tCI: [-0.1,0.04],\tpvalue: 0.4226\n",
      "loss_se_slope12:group: \tbeta: -0.04,\tCI: [-0.13,0.05],\tpvalue: 0.4028\n",
      "rew_LR_slope12:group: \tbeta: 0.01,\tCI: [-0.06,0.07],\tpvalue: 0.8465\n",
      "loss_LR_slope12:group: \tbeta: -0.01,\tCI: [-0.06,0.04],\tpvalue: 0.8014\n",
      "app_Pav_slope12:group: \tbeta: -0.1,\tCI: [-0.25,0.06],\tpvalue: 0.2077\n",
      "av_Pav_slope12:group: \tbeta: 0.15,\tCI: [0.03,0.26],\tpvalue: 0.0155\n",
      "noise_slope12:group: \tbeta: 0.02,\tCI: [-0.04,0.09],\tpvalue: 0.493\n",
      "bias_slope12:group: \tbeta: -0.05,\tCI: [-0.14,0.05],\tpvalue: 0.3237\n"
     ]
    }
   ],
   "source": [
    "# early changes in cognitive processing predicting treatment outcome at week 12?\n",
    "for j in psychiatric_questionnaires:\n",
    "    print('##############################################################################')\n",
    "    print(j + ':')\n",
    "    for i in parameter_labels[:-1]:\n",
    "        panda.glm(j + '4log ~' + i + '_slope12 * group + ' + j + '1log + age + education + site', \\\n",
    "                        df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12:group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_Pav_slope12:group: \tbeta: 0.15,\tCI: [0.03,0.26],\tpvalue: 0.0155\n",
      "placebo: av_Pav_slope12: \tbeta: 0.0,\tCI: [-0.08,0.08],\tpvalue: 0.9619\n",
      "sertraline: av_Pav_slope12: \tbeta: 0.14,\tCI: [0.06,0.23],\tpvalue: 0.0014\n"
     ]
    }
   ],
   "source": [
    "i = parameter_labels[5]\n",
    "panda.glm('bdi4log ~' + i + '_slope12 * group + bdi1log + age + education + site', \\\n",
    "                df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)], [i + '_slope12:group'])\n",
    "for g in range(2):\n",
    "    print(group_label[g], end=': ')\n",
    "    tmp = df_panda['group'] == g\n",
    "    panda.glm('bdi4log ~' + i + '_slope12 + bdi1log + age + education + site', \\\n",
    "                    df_panda[(df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0)&tmp], [i + '_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 213\n"
     ]
    }
   ],
   "source": [
    "print('N = ' + str(sum((df_panda['exclusiontot1']==0)&(df_panda['exclusiontot2']==0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main findings in whole sample\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group: \tbeta: 0.32,\tCI: [0.03,0.61],\tpvalue: 0.0294\n",
      "group: \tbeta: 0.33,\tCI: [-0.04,0.7],\tpvalue: 0.0807\n",
      "group: \tbeta: -0.34,\tCI: [-0.74,0.05],\tpvalue: 0.0866\n",
      "loss_LR: \tbeta: 0.0,\tCI: [-0.01,0.02],\tpvalue: 0.4536\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.0,0.02],\tpvalue: 0.2503\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.01,0.02],\tpvalue: 0.3205\n"
     ]
    }
   ],
   "source": [
    "# findings on loss LR\n",
    "panda.mle('loss_LR ~ group + time + cis + dep + site', \\\n",
    "                    mle_df[((mle_df['time']==1)|(mle_df['time']==2))], ['group'], [])\n",
    "panda.glm('loss_LR_slope12 ~ group + cis + dep + site', \\\n",
    "                    df_panda, ['group'])\n",
    "panda.glm('loss_LR_slope23 ~ group + cis + dep + site', \\\n",
    "                    df_panda, ['group'])\n",
    "panda.mle('gadlog ~ loss_LR + group + time + cis + dep + site', \\\n",
    "                    mle_df[((mle_df['time']==1)|(mle_df['time']==2))], ['loss_LR'], 'loss_LR')\n",
    "panda.mle('gadlog ~ loss_LR + group + time + cis + dep + site', \\\n",
    "                    mle_df[((mle_df['time']==1)|(mle_df['time']==3))], ['loss_LR'], 'loss_LR')\n",
    "panda.mle('gadlog ~ loss_LR + group + time + cis + dep + site', \\\n",
    "                    mle_df, ['loss_LR'], 'loss_LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_LR: \tbeta: 0.01,\tCI: [0.0,0.02],\tpvalue: 0.0063\n",
      "loss_LR: \tbeta: 0.01,\tCI: [-0.0,0.02],\tpvalue: 0.0786\n"
     ]
    }
   ],
   "source": [
    "panda.mle('bdilog ~ loss_LR + group + time + cis', \\\n",
    "                    mle_df, ['loss_LR'], 'loss_LR')\n",
    "panda.mle('bdilog ~ loss_LR + group + time + gad1log', \\\n",
    "                    mle_df, ['loss_LR'], 'loss_LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_LR</th>\n",
       "      <th>cis</th>\n",
       "      <th>bdilog</th>\n",
       "      <th>gad1log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss_LR</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037857</td>\n",
       "      <td>0.030782</td>\n",
       "      <td>0.020673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cis</th>\n",
       "      <td>-0.037857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468951</td>\n",
       "      <td>0.676980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bdilog</th>\n",
       "      <td>0.030782</td>\n",
       "      <td>0.468951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gad1log</th>\n",
       "      <td>0.020673</td>\n",
       "      <td>0.676980</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss_LR       cis    bdilog   gad1log\n",
       "loss_LR  1.000000 -0.037857  0.030782  0.020673\n",
       "cis     -0.037857  1.000000  0.468951  0.676980\n",
       "bdilog   0.030782  0.468951  1.000000  0.465390\n",
       "gad1log  0.020673  0.676980  0.465390  1.000000"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_df[['loss_LR','cis','bdilog','gad1log']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_Pav_slope12: \tbeta: 0.02,\tCI: [-0.01,0.05],\tpvalue: 0.2884\n",
      "av_Pav_slope12:group: \tbeta: 0.03,\tCI: [-0.03,0.09],\tpvalue: 0.3655\n",
      "av_Pav_slope12: \tbeta: 0.01,\tCI: [-0.03,0.05],\tpvalue: 0.6348\n",
      "av_Pav_slope12: \tbeta: 0.03,\tCI: [-0.01,0.07],\tpvalue: 0.1822\n"
     ]
    }
   ],
   "source": [
    "# findings on early change in av Pav prediciting BDI at week 12\n",
    "panda.glm('phq3log ~ av_Pav_slope12 + group + bdi1log', \\\n",
    "                    df_panda, ['av_Pav_slope12'])\n",
    "panda.glm('phq3log ~ av_Pav_slope12 * group + bdi1log', \\\n",
    "                    df_panda, ['av_Pav_slope12:group'])\n",
    "for i in range(2):\n",
    "    panda.glm('phq3log ~ av_Pav_slope12 + bdi1log + cis + dep + site', \\\n",
    "                        df_panda[df_panda['group']==i], ['av_Pav_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_Pav_slope12: \tbeta: 0.04,\tCI: [-0.0,0.08],\tpvalue: 0.0582\n",
      "av_Pav_slope12:group: \tbeta: 0.07,\tCI: [-0.01,0.15],\tpvalue: 0.0949\n",
      "av_Pav_slope12: \tbeta: 0.01,\tCI: [-0.05,0.07],\tpvalue: 0.7912\n",
      "av_Pav_slope12: \tbeta: 0.08,\tCI: [0.02,0.14],\tpvalue: 0.0083\n"
     ]
    }
   ],
   "source": [
    "# findings on early change in av Pav prediciting BDI at week 12\n",
    "panda.glm('bdi4log ~ av_Pav_slope12 + group + bdi1log', \\\n",
    "                    df_panda, ['av_Pav_slope12'])\n",
    "panda.glm('bdi4log ~ av_Pav_slope12 * group + bdi1log', \\\n",
    "                    df_panda, ['av_Pav_slope12:group'])\n",
    "for i in range(2):\n",
    "    panda.glm('bdi4log ~ av_Pav_slope12 + bdi1log + cis + dep + site', \\\n",
    "                        df_panda[df_panda['group']==i], ['av_Pav_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: \tbeta: 0.01,\tCI: [0.01,0.02],\tpvalue: 0.0\n",
      "time: \tbeta: -0.07,\tCI: [-0.11,-0.04],\tpvalue: 0.0001\n",
      "age: \tbeta: 0.01,\tCI: [0.0,0.01],\tpvalue: 0.0\n",
      "time: \tbeta: -0.05,\tCI: [-0.07,-0.02],\tpvalue: 0.0021\n",
      "age: \tbeta: -0.03,\tCI: [-0.04,-0.03],\tpvalue: 0.0\n",
      "time: \tbeta: 0.15,\tCI: [0.09,0.21],\tpvalue: 0.0\n",
      "age: \tbeta: -0.0,\tCI: [-0.0,-0.0],\tpvalue: 0.0\n",
      "time: \tbeta: 0.02,\tCI: [0.01,0.02],\tpvalue: 0.0\n"
     ]
    }
   ],
   "source": [
    "# findings of age and time\n",
    "panda.mle('av_Pav ~ age + time + cis + dep + site', mle_df, ['age', 'time'], [])\n",
    "panda.mle('app_Pav ~ age + time + cis + dep + site', mle_df, ['age', 'time'], [])\n",
    "panda.mle('bias ~ age + time + cis + dep + site', mle_df, ['age', 'time'], [])\n",
    "panda.mle('acctot ~ age + time + cis + dep + site', mle_df, ['age', 'time'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   5.,  15.,  55., 102., 143., 110.,  61.,  22.,   7.]),\n",
       " array([-3.09387293, -2.54825303, -2.00263313, -1.45701323, -0.91139332,\n",
       "        -0.36577342,  0.17984648,  0.72546639,  1.27108629,  1.81670619,\n",
       "         2.3623261 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAO20lEQVR4nO3df6xfdX3H8edrreLQGCG9YKXNLlsaIxIyzQ1zc1lcqrETQtkyTEncmknSmODUxUVbSeSPpUkXF6dZ5pJGGF3GwMYfoZFNYd0MWzJgF0QFCtJIhUqlV/HXZoIrvPfHPW5fr7fce7/n++2399Pn45/v93zOOd/zOmnzuqeffs+5qSokSW35hUkHkCSNnuUuSQ2y3CWpQZa7JDXIcpekBq2ddACAdevW1fT09KRjSNKqct99932nqqYWW3dalPv09DSzs7OTjiFJq0qSb55sndMyktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUoNPiDlXpdDa98/aJHPfInssmcly1Yckr9yQ3Jjme5MFF1v1pkkqybmBsV5LDSR5N8tZRB5YkLW050zI3AVsWDibZCLwFeGJg7CJgG/Dabp9PJFkzkqSSpGVbstyr6i7gmUVW/SXwAWDwl7BuBW6tqmer6nHgMHDpKIJKkpZvqP9QTXIF8K2q+sqCVRcATw4sH+3GFvuMHUlmk8zOzc0NE0OSdBIrLvckZwPXAR9ebPUiY7XIGFW1t6pmqmpmamrRxxFLkoY0zLdlfgW4EPhKEoANwP1JLmX+Sn3jwLYbgKf6hpQkrcyKr9yr6mtVdV5VTVfVNPOF/vqq+jZwANiW5KwkFwKbgHtHmliStKTlfBXyFuA/gFcnOZrkmpNtW1UPAfuBh4EvANdW1XOjCitJWp4lp2Wq6uol1k8vWN4N7O4XS5LUh48fkKQGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSg5Ys9yQ3Jjme5MGBsY8keSTJV5N8LskrBtbtSnI4yaNJ3jqm3JKkF7CcK/ebgC0Lxu4ELq6qS4CvA7sAklwEbANe2+3ziSRrRpZWkrQsS5Z7Vd0FPLNg7I6qOtEt3g1s6N5vBW6tqmer6nHgMHDpCPNKkpZhFHPu7wT+qXt/AfDkwLqj3djPSbIjyWyS2bm5uRHEkCT9VK9yT3IdcAK4+adDi2xWi+1bVXuraqaqZqampvrEkCQtsHbYHZNsBy4HNlfVTwv8KLBxYLMNwFPDx5MkDWOoK/ckW4APAldU1Y8HVh0AtiU5K8mFwCbg3v4xJUkrseSVe5JbgDcB65IcBa5n/tsxZwF3JgG4u6reVVUPJdkPPMz8dM21VfXcuMJLkha3ZLlX1dWLDN/wAtvvBnb3CSVJ6sc7VCWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUFDPzhM0nhN77x9Ysc+sueyiR1bo+GVuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGLfn4gSQ3ApcDx6vq4m7sXOBTwDRwBHh7VX2vW7cLuAZ4DnhPVX1xLMl1RpnkrfjSarScK/ebgC0LxnYCB6tqE3CwWybJRcA24LXdPp9IsmZkaSVJy7JkuVfVXcAzC4a3Avu69/uAKwfGb62qZ6vqceAwcOlookqSlmvYOffzq+oYQPd6Xjd+AfDkwHZHu7Gfk2RHktkks3Nzc0PGkCQtZtT/oZpFxmqxDatqb1XNVNXM1NTUiGNI0plt2HJ/Osl6gO71eDd+FNg4sN0G4Knh40mShjFsuR8AtnfvtwO3DYxvS3JWkguBTcC9/SJKklZqOV+FvAV4E7AuyVHgemAPsD/JNcATwFUAVfVQkv3Aw8AJ4Nqqem5M2SVJJ7FkuVfV1SdZtfkk2+8GdvcJJUnqxztUJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUoF7lnuRPkjyU5MEktyR5SZJzk9yZ5LHu9ZxRhZUkLc/Q5Z7kAuA9wExVXQysAbYBO4GDVbUJONgtS5JOob7TMmuBX0yyFjgbeArYCuzr1u8Drux5DEnSCg1d7lX1LeAvgCeAY8APquoO4PyqOtZtcww4b7H9k+xIMptkdm5ubtgYkqRF9JmWOYf5q/QLgVcBL03yjuXuX1V7q2qmqmampqaGjSFJWkSfaZk3A49X1VxV/Q/wWeA3gKeTrAfoXo/3jylJWok+5f4E8IYkZycJsBk4BBwAtnfbbAdu6xdRkrRSa4fdsaruSfJp4H7gBPBlYC/wMmB/kmuY/wFw1SiCSpKWb+hyB6iq64HrFww/y/xVvCRpQrxDVZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDepV7klekeTTSR5JcijJryc5N8mdSR7rXs8ZVVhJ0vKs7bn/x4EvVNXvJ3kxcDbwIeBgVe1JshPYCXyw53EknULTO2+fyHGP7LlsIsdt0dBX7kleDvwWcANAVf2kqr4PbAX2dZvtA67sF1GStFJ9pmV+GZgD/jbJl5N8MslLgfOr6hhA93reYjsn2ZFkNsns3NxcjxiSpIX6lPta4PXA31TV64D/Zn4KZlmqam9VzVTVzNTUVI8YkqSF+pT7UeBoVd3TLX+a+bJ/Osl6gO71eL+IkqSVGrrcq+rbwJNJXt0NbQYeBg4A27ux7cBtvRJKklas77dl/hi4ufumzDeAP2L+B8b+JNcATwBX9TyGJGmFepV7VT0AzCyyanOfz5Uk9eMdqpLUoL7TMjrDTOrmFkkr45W7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QG9S73JGuSfDnJ57vlc5PcmeSx7vWc/jElSSsxiiv39wKHBpZ3AgerahNwsFuWJJ1Cvco9yQbgMuCTA8NbgX3d+33AlX2OIUlaub5X7h8DPgA8PzB2flUdA+hez1tsxyQ7kswmmZ2bm+sZQ5I0aOhyT3I5cLyq7htm/6raW1UzVTUzNTU1bAxJ0iLW9tj3jcAVSd4GvAR4eZK/B55Osr6qjiVZDxwfRVBJ0vINfeVeVbuqakNVTQPbgH+pqncAB4Dt3Wbbgdt6p5Qkrcg4vue+B3hLkseAt3TLkqRTqM+0zP+pqi8BX+refxfYPIrPlSQNxztUJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUoKF/QXaSjcDfAa8Engf2VtXHk5wLfAqYBo4Ab6+q7/WPKql10ztvn8hxj+y5bCLHHac+V+4ngPdX1WuANwDXJrkI2AkcrKpNwMFuWZJ0Cg1d7lV1rKru797/CDgEXABsBfZ1m+0DruyZUZK0QiOZc08yDbwOuAc4v6qOwfwPAOC8k+yzI8lsktm5ublRxJAkdXqXe5KXAZ8B3ldVP1zuflW1t6pmqmpmamqqbwxJ0oBe5Z7kRcwX+81V9dlu+Okk67v164Hj/SJKklZq6HJPEuAG4FBVfXRg1QFge/d+O3Db8PEkScMY+quQwBuBPwC+luSBbuxDwB5gf5JrgCeAq3ollCSt2NDlXlX/DuQkqzcP+7mSpP68Q1WSGtRnWkYTMqm7+CStHl65S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDfLZMpLOeJN8XtORPZeN5XO9cpekBlnuktQgp2V68NG7kk5XXrlLUoMsd0lqkOUuSQ0a25x7ki3Ax4E1wCeras+4juXctyT9rLFcuSdZA/w18DvARcDVSS4ax7EkST9vXNMylwKHq+obVfUT4FZg65iOJUlaYFzTMhcATw4sHwV+bXCDJDuAHd3ifyV5dMQZ1gHfGfFnnk5aPr+Wzw08v9VupOeXP++1+y+dbMW4yj2LjNXPLFTtBfaO6fgkma2qmXF9/qS1fH4tnxt4fqvdajm/cU3LHAU2DixvAJ4a07EkSQuMq9z/E9iU5MIkLwa2AQfGdCxJ0gJjmZapqhNJ3g18kfmvQt5YVQ+N41gvYGxTPqeJls+v5XMDz2+1WxXnl6paeitJ0qriHaqS1CDLXZIa1HS5J/mzJF9N8kCSO5K8atKZRiXJR5I80p3f55K8YtKZRinJVUkeSvJ8ktP+a2fLlWRLkkeTHE6yc9J5RinJjUmOJ3lw0llGLcnGJP+a5FD39/K9k860lKbLHfhIVV1SVb8KfB748ITzjNKdwMVVdQnwdWDXhPOM2oPA7wF3TTrIqJwBj+W4Cdgy6RBjcgJ4f1W9BngDcO3p/mfXdLlX1Q8HFl/KghupVrOquqOqTnSLdzN/L0EzqupQVY36ruVJa/qxHFV1F/DMpHOMQ1Udq6r7u/c/Ag4xfyf+aav538SUZDfwh8APgN+ecJxxeSfwqUmH0JKWfCyHTn9JpoHXAfdMOMoLWvXlnuSfgVcusuq6qrqtqq4DrkuyC3g3cP0pDdjDUufWbXMd8/9kvPlUZhuF5ZxfY5Z8LIdOb0leBnwGeN+CmYHTzqov96p68zI3/QfgdlZRuS91bkm2A5cDm2sV3rCwgj+7VvhYjlUsyYuYL/abq+qzk86zlKbn3JNsGli8AnhkUllGrftlKB8ErqiqH086j5bFx3KsUkkC3AAcqqqPTjrPcjR9h2qSzwCvBp4Hvgm8q6q+NdlUo5HkMHAW8N1u6O6qetcEI41Ukt8F/gqYAr4PPFBVb51oqBFI8jbgY/z/Yzl2TzbR6CS5BXgT84/EfRq4vqpumGioEUnym8C/AV9jvk8APlRV/zi5VC+s6XKXpDNV09MyknSmstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSg/4XYbK7CsU53uoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df_panda['av_Pav_slope12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 28.,  93., 267., 396., 295., 203., 116., 165.,  51.,  16.]),\n",
       " array([-2.31112102, -1.91081521, -1.51050939, -1.11020357, -0.70989776,\n",
       "        -0.30959194,  0.09071388,  0.4910197 ,  0.89132551,  1.29163133,\n",
       "         1.69193715]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(mle_df['av_Pav'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
